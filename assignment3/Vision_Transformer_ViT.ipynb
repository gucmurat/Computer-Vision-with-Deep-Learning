{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c61d56cd",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b9e55",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dff2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72393c4",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986f774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "523cabc9",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=False, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724c974",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5b0c6ea",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520eba4",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b0e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        attention_logits =  torch.matmul(q, k.transpose(-2, -1)) * (1 / ((d//self.num_heads)**(1/2)))\n",
    "    \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "\n",
    "        attention_weights = torch.softmax(attention_logits, dim = -1)\n",
    "        \n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        \n",
    "        attn_out = torch.matmul(attention_weights, v).permute(0, 2, 1, 3).reshape(-1, n, self.proj_dims)\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ec7ddb",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2587af25",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1959fc4a",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a9a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.relu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e61cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        self.norm_layer = nn.LayerNorm(hidden_dims)\n",
    "        self.attention = SelfAttention(hidden_dims, hidden_dims//num_heads, num_heads, bias=bias)\n",
    "        self.norm_layer2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp= MLP(hidden_dims, hidden_dims, hidden_dims, bias=bias)\n",
    "        \n",
    "        \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        norm_layer = self.norm_layer(x)\n",
    "        attention  = self.attention(norm_layer)\n",
    "        norm_layer2       = self.norm_layer2(attention + x)\n",
    "        mlp                  = self.mlp(norm_layer2)\n",
    "        output               = mlp + attention + x\n",
    "        return output\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7befd09",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd48e252",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size()) \n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfe44d",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b83c0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c63c5",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac2c36c3",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size()) \n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c259c",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617e6e4",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121e11de",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d72424",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7073eb1",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e813d8",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c7ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019682b8",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1442668a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b8bb388",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 4.320 | Acc: 8.000% (2/25)\n",
      "Loss: 3.928 | Acc: 8.000% (4/50)\n",
      "Loss: 4.095 | Acc: 6.667% (5/75)\n",
      "Loss: 4.393 | Acc: 7.000% (7/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 7.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.856 | Acc: 20.000% (5/25)\n",
      "Loss: 3.321 | Acc: 16.000% (8/50)\n",
      "Loss: 3.180 | Acc: 16.000% (12/75)\n",
      "Loss: 3.317 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 4.164 | Acc: 12.000% (3/25)\n",
      "Loss: 3.692 | Acc: 16.000% (8/50)\n",
      "Loss: 3.571 | Acc: 14.667% (11/75)\n",
      "Loss: 3.351 | Acc: 14.000% (14/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 14.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.179 | Acc: 0.000% (0/25)\n",
      "Loss: 3.161 | Acc: 8.000% (4/50)\n",
      "Loss: 3.061 | Acc: 10.667% (8/75)\n",
      "Loss: 3.016 | Acc: 11.000% (11/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 11.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 2.103 | Acc: 20.000% (5/25)\n",
      "Loss: 2.362 | Acc: 16.000% (8/50)\n",
      "Loss: 2.432 | Acc: 12.000% (9/75)\n",
      "Loss: 2.521 | Acc: 11.000% (11/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 11.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.425 | Acc: 24.000% (6/25)\n",
      "Loss: 2.521 | Acc: 16.000% (8/50)\n",
      "Loss: 2.583 | Acc: 17.333% (13/75)\n",
      "Loss: 2.690 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 2.111 | Acc: 20.000% (5/25)\n",
      "Loss: 2.110 | Acc: 22.000% (11/50)\n",
      "Loss: 2.112 | Acc: 22.667% (17/75)\n",
      "Loss: 2.026 | Acc: 29.000% (29/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 29.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.423 | Acc: 16.000% (4/25)\n",
      "Loss: 2.236 | Acc: 24.000% (12/50)\n",
      "Loss: 2.319 | Acc: 24.000% (18/75)\n",
      "Loss: 2.375 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.640 | Acc: 36.000% (9/25)\n",
      "Loss: 1.644 | Acc: 38.000% (19/50)\n",
      "Loss: 1.695 | Acc: 37.333% (28/75)\n",
      "Loss: 1.685 | Acc: 37.000% (37/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 37.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.220 | Acc: 28.000% (7/25)\n",
      "Loss: 2.329 | Acc: 20.000% (10/50)\n",
      "Loss: 2.344 | Acc: 21.333% (16/75)\n",
      "Loss: 2.458 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.439 | Acc: 44.000% (11/25)\n",
      "Loss: 1.594 | Acc: 38.000% (19/50)\n",
      "Loss: 1.555 | Acc: 41.333% (31/75)\n",
      "Loss: 1.561 | Acc: 42.000% (42/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 42.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.293 | Acc: 28.000% (7/25)\n",
      "Loss: 2.396 | Acc: 24.000% (12/50)\n",
      "Loss: 2.384 | Acc: 21.333% (16/75)\n",
      "Loss: 2.522 | Acc: 17.000% (17/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 17.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.600 | Acc: 40.000% (10/25)\n",
      "Loss: 1.312 | Acc: 52.000% (26/50)\n",
      "Loss: 1.278 | Acc: 54.667% (41/75)\n",
      "Loss: 1.332 | Acc: 52.000% (52/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 52.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.230 | Acc: 16.000% (4/25)\n",
      "Loss: 2.438 | Acc: 20.000% (10/50)\n",
      "Loss: 2.400 | Acc: 22.667% (17/75)\n",
      "Loss: 2.562 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 0.922 | Acc: 80.000% (20/25)\n",
      "Loss: 0.957 | Acc: 74.000% (37/50)\n",
      "Loss: 0.966 | Acc: 72.000% (54/75)\n",
      "Loss: 1.005 | Acc: 72.000% (72/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 72.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.212 | Acc: 16.000% (4/25)\n",
      "Loss: 2.395 | Acc: 12.000% (6/50)\n",
      "Loss: 2.449 | Acc: 18.667% (14/75)\n",
      "Loss: 2.579 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.673 | Acc: 88.000% (22/25)\n",
      "Loss: 0.732 | Acc: 84.000% (42/50)\n",
      "Loss: 0.761 | Acc: 82.667% (62/75)\n",
      "Loss: 0.741 | Acc: 82.000% (82/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 82.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.510 | Acc: 36.000% (9/25)\n",
      "Loss: 2.646 | Acc: 30.000% (15/50)\n",
      "Loss: 2.614 | Acc: 28.000% (21/75)\n",
      "Loss: 2.708 | Acc: 23.000% (23/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 23.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.399 | Acc: 100.000% (25/25)\n",
      "Loss: 0.445 | Acc: 96.000% (48/50)\n",
      "Loss: 0.476 | Acc: 92.000% (69/75)\n",
      "Loss: 0.486 | Acc: 91.000% (91/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 91.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.671 | Acc: 12.000% (3/25)\n",
      "Loss: 2.822 | Acc: 14.000% (7/50)\n",
      "Loss: 2.851 | Acc: 17.333% (13/75)\n",
      "Loss: 3.004 | Acc: 15.000% (15/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 15.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.404 | Acc: 92.000% (23/25)\n",
      "Loss: 0.335 | Acc: 94.000% (47/50)\n",
      "Loss: 0.296 | Acc: 96.000% (72/75)\n",
      "Loss: 0.278 | Acc: 97.000% (97/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 97.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.714 | Acc: 12.000% (3/25)\n",
      "Loss: 2.911 | Acc: 10.000% (5/50)\n",
      "Loss: 2.910 | Acc: 13.333% (10/75)\n",
      "Loss: 3.069 | Acc: 12.000% (12/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 12.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.180 | Acc: 96.000% (24/25)\n",
      "Loss: 0.179 | Acc: 98.000% (49/50)\n",
      "Loss: 0.168 | Acc: 98.667% (74/75)\n",
      "Loss: 0.150 | Acc: 99.000% (99/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 99.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.957 | Acc: 32.000% (8/25)\n",
      "Loss: 3.293 | Acc: 22.000% (11/50)\n",
      "Loss: 3.229 | Acc: 22.667% (17/75)\n",
      "Loss: 3.392 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.068 | Acc: 100.000% (25/25)\n",
      "Loss: 0.094 | Acc: 100.000% (50/50)\n",
      "Loss: 0.090 | Acc: 100.000% (75/75)\n",
      "Loss: 0.083 | Acc: 100.000% (100/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.116 | Acc: 28.000% (7/25)\n",
      "Loss: 3.352 | Acc: 20.000% (10/50)\n",
      "Loss: 3.261 | Acc: 21.333% (16/75)\n",
      "Loss: 3.422 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.044 | Acc: 100.000% (25/25)\n",
      "Loss: 0.051 | Acc: 100.000% (50/50)\n",
      "Loss: 0.048 | Acc: 100.000% (75/75)\n",
      "Loss: 0.047 | Acc: 100.000% (100/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.300 | Acc: 24.000% (6/25)\n",
      "Loss: 3.451 | Acc: 16.000% (8/50)\n",
      "Loss: 3.341 | Acc: 20.000% (15/75)\n",
      "Loss: 3.518 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.035 | Acc: 100.000% (25/25)\n",
      "Loss: 0.034 | Acc: 100.000% (50/50)\n",
      "Loss: 0.032 | Acc: 100.000% (75/75)\n",
      "Loss: 0.029 | Acc: 100.000% (100/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.419 | Acc: 24.000% (6/25)\n",
      "Loss: 3.633 | Acc: 18.000% (9/50)\n",
      "Loss: 3.495 | Acc: 21.333% (16/75)\n",
      "Loss: 3.659 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Final train set accuracy is 100.0\n",
      "Final val set accuracy is 19.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, \n",
    "              input_dims, \n",
    "              output_dims, \n",
    "              num_trans_layers, \n",
    "              num_heads, \n",
    "              image_k, \n",
    "              patch_k)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793febb6",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22cece2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 2.748 | Acc: 15.625% (10/64)\n",
      "Loss: 3.407 | Acc: 16.406% (21/128)\n",
      "Loss: 3.894 | Acc: 15.104% (29/192)\n",
      "Loss: 4.345 | Acc: 14.453% (37/256)\n",
      "Loss: 4.359 | Acc: 15.312% (49/320)\n",
      "Loss: 4.221 | Acc: 15.625% (60/384)\n",
      "Loss: 4.091 | Acc: 15.625% (70/448)\n",
      "Loss: 3.982 | Acc: 15.430% (79/512)\n",
      "Loss: 3.905 | Acc: 14.931% (86/576)\n",
      "Loss: 3.798 | Acc: 15.000% (96/640)\n",
      "Loss: 3.690 | Acc: 15.199% (107/704)\n",
      "Loss: 3.579 | Acc: 15.885% (122/768)\n",
      "Loss: 3.550 | Acc: 15.865% (132/832)\n",
      "Loss: 3.509 | Acc: 16.071% (144/896)\n",
      "Loss: 3.466 | Acc: 15.833% (152/960)\n",
      "Loss: 3.400 | Acc: 16.309% (167/1024)\n",
      "Loss: 3.341 | Acc: 16.452% (179/1088)\n",
      "Loss: 3.284 | Acc: 16.580% (191/1152)\n",
      "Loss: 3.230 | Acc: 16.612% (202/1216)\n",
      "Loss: 3.186 | Acc: 16.641% (213/1280)\n",
      "Loss: 3.145 | Acc: 16.592% (223/1344)\n",
      "Loss: 3.116 | Acc: 16.690% (235/1408)\n",
      "Loss: 3.079 | Acc: 16.644% (245/1472)\n",
      "Loss: 3.041 | Acc: 16.797% (258/1536)\n",
      "Loss: 3.007 | Acc: 17.000% (272/1600)\n",
      "Loss: 2.978 | Acc: 17.188% (286/1664)\n",
      "Loss: 2.942 | Acc: 17.419% (301/1728)\n",
      "Loss: 2.919 | Acc: 17.578% (315/1792)\n",
      "Loss: 2.907 | Acc: 17.511% (325/1856)\n",
      "Loss: 2.880 | Acc: 17.708% (340/1920)\n",
      "Loss: 2.850 | Acc: 17.944% (356/1984)\n",
      "Loss: 2.820 | Acc: 18.408% (377/2048)\n",
      "Loss: 2.800 | Acc: 18.608% (393/2112)\n",
      "Loss: 2.783 | Acc: 18.750% (408/2176)\n",
      "Loss: 2.772 | Acc: 18.571% (416/2240)\n",
      "Loss: 2.747 | Acc: 19.054% (439/2304)\n",
      "Loss: 2.727 | Acc: 19.215% (455/2368)\n",
      "Loss: 2.707 | Acc: 19.408% (472/2432)\n",
      "Loss: 2.693 | Acc: 19.631% (490/2496)\n",
      "Loss: 2.681 | Acc: 19.727% (505/2560)\n",
      "Loss: 2.664 | Acc: 19.931% (523/2624)\n",
      "Loss: 2.651 | Acc: 19.903% (535/2688)\n",
      "Loss: 2.638 | Acc: 19.985% (550/2752)\n",
      "Loss: 2.630 | Acc: 19.993% (563/2816)\n",
      "Loss: 2.612 | Acc: 20.312% (585/2880)\n",
      "Loss: 2.599 | Acc: 20.380% (600/2944)\n",
      "Loss: 2.584 | Acc: 20.612% (620/3008)\n",
      "Loss: 2.569 | Acc: 20.801% (639/3072)\n",
      "Loss: 2.558 | Acc: 20.727% (650/3136)\n",
      "Loss: 2.545 | Acc: 20.875% (668/3200)\n",
      "Loss: 2.532 | Acc: 21.140% (690/3264)\n",
      "Loss: 2.517 | Acc: 21.304% (709/3328)\n",
      "Loss: 2.509 | Acc: 21.285% (722/3392)\n",
      "Loss: 2.499 | Acc: 21.325% (737/3456)\n",
      "Loss: 2.488 | Acc: 21.534% (758/3520)\n",
      "Loss: 2.479 | Acc: 21.791% (781/3584)\n",
      "Loss: 2.469 | Acc: 22.012% (803/3648)\n",
      "Loss: 2.459 | Acc: 22.144% (822/3712)\n",
      "Loss: 2.449 | Acc: 22.299% (842/3776)\n",
      "Loss: 2.443 | Acc: 22.240% (854/3840)\n",
      "Loss: 2.435 | Acc: 22.336% (872/3904)\n",
      "Loss: 2.428 | Acc: 22.505% (893/3968)\n",
      "Loss: 2.419 | Acc: 22.644% (913/4032)\n",
      "Loss: 2.414 | Acc: 22.607% (926/4096)\n",
      "Loss: 2.407 | Acc: 22.692% (944/4160)\n",
      "Loss: 2.397 | Acc: 22.893% (967/4224)\n",
      "Loss: 2.393 | Acc: 22.831% (979/4288)\n",
      "Loss: 2.387 | Acc: 22.863% (995/4352)\n",
      "Loss: 2.380 | Acc: 22.871% (1010/4416)\n",
      "Loss: 2.371 | Acc: 23.058% (1033/4480)\n",
      "Loss: 2.369 | Acc: 23.041% (1047/4544)\n",
      "Loss: 2.364 | Acc: 23.069% (1063/4608)\n",
      "Loss: 2.358 | Acc: 23.116% (1080/4672)\n",
      "Loss: 2.352 | Acc: 23.205% (1099/4736)\n",
      "Loss: 2.345 | Acc: 23.396% (1123/4800)\n",
      "Loss: 2.340 | Acc: 23.458% (1141/4864)\n",
      "Loss: 2.333 | Acc: 23.539% (1160/4928)\n",
      "Loss: 2.326 | Acc: 23.638% (1180/4992)\n",
      "Loss: 2.318 | Acc: 23.873% (1207/5056)\n",
      "Loss: 2.313 | Acc: 23.926% (1225/5120)\n",
      "Loss: 2.309 | Acc: 24.016% (1245/5184)\n",
      "Loss: 2.302 | Acc: 24.085% (1264/5248)\n",
      "Loss: 2.296 | Acc: 24.247% (1288/5312)\n",
      "Loss: 2.293 | Acc: 24.312% (1307/5376)\n",
      "Loss: 2.289 | Acc: 24.338% (1324/5440)\n",
      "Loss: 2.284 | Acc: 24.473% (1347/5504)\n",
      "Loss: 2.283 | Acc: 24.479% (1363/5568)\n",
      "Loss: 2.276 | Acc: 24.627% (1387/5632)\n",
      "Loss: 2.271 | Acc: 24.737% (1409/5696)\n",
      "Loss: 2.267 | Acc: 24.896% (1434/5760)\n",
      "Loss: 2.262 | Acc: 24.914% (1451/5824)\n",
      "Loss: 2.258 | Acc: 24.966% (1470/5888)\n",
      "Loss: 2.253 | Acc: 25.084% (1493/5952)\n",
      "Loss: 2.248 | Acc: 25.150% (1513/6016)\n",
      "Loss: 2.246 | Acc: 25.132% (1528/6080)\n",
      "Loss: 2.243 | Acc: 25.212% (1549/6144)\n",
      "Loss: 2.236 | Acc: 25.370% (1575/6208)\n",
      "Loss: 2.235 | Acc: 25.303% (1587/6272)\n",
      "Loss: 2.230 | Acc: 25.442% (1612/6336)\n",
      "Loss: 2.227 | Acc: 25.484% (1631/6400)\n",
      "Loss: 2.225 | Acc: 25.526% (1650/6464)\n",
      "Loss: 2.221 | Acc: 25.628% (1673/6528)\n",
      "Loss: 2.222 | Acc: 25.576% (1686/6592)\n",
      "Loss: 2.218 | Acc: 25.601% (1704/6656)\n",
      "Loss: 2.217 | Acc: 25.640% (1723/6720)\n",
      "Loss: 2.215 | Acc: 25.619% (1738/6784)\n",
      "Loss: 2.213 | Acc: 25.643% (1756/6848)\n",
      "Loss: 2.208 | Acc: 25.666% (1774/6912)\n",
      "Loss: 2.204 | Acc: 25.717% (1794/6976)\n",
      "Loss: 2.202 | Acc: 25.795% (1816/7040)\n",
      "Loss: 2.201 | Acc: 25.816% (1834/7104)\n",
      "Loss: 2.197 | Acc: 25.921% (1858/7168)\n",
      "Loss: 2.194 | Acc: 25.954% (1877/7232)\n",
      "Loss: 2.193 | Acc: 25.863% (1887/7296)\n",
      "Loss: 2.190 | Acc: 25.992% (1913/7360)\n",
      "Loss: 2.187 | Acc: 25.983% (1929/7424)\n",
      "Loss: 2.184 | Acc: 26.015% (1948/7488)\n",
      "Loss: 2.182 | Acc: 26.020% (1965/7552)\n",
      "Loss: 2.180 | Acc: 26.011% (1981/7616)\n",
      "Loss: 2.176 | Acc: 26.081% (2003/7680)\n",
      "Loss: 2.173 | Acc: 26.149% (2025/7744)\n",
      "Loss: 2.171 | Acc: 26.165% (2043/7808)\n",
      "Loss: 2.169 | Acc: 26.156% (2059/7872)\n",
      "Loss: 2.167 | Acc: 26.235% (2082/7936)\n",
      "Loss: 2.166 | Acc: 26.225% (2098/8000)\n",
      "Loss: 2.163 | Acc: 26.314% (2122/8064)\n",
      "Loss: 2.160 | Acc: 26.403% (2146/8128)\n",
      "Loss: 2.157 | Acc: 26.526% (2173/8192)\n",
      "Loss: 2.153 | Acc: 26.587% (2195/8256)\n",
      "Loss: 2.151 | Acc: 26.659% (2218/8320)\n",
      "Loss: 2.147 | Acc: 26.741% (2242/8384)\n",
      "Loss: 2.146 | Acc: 26.764% (2261/8448)\n",
      "Loss: 2.144 | Acc: 26.715% (2274/8512)\n",
      "Loss: 2.141 | Acc: 26.784% (2297/8576)\n",
      "Loss: 2.138 | Acc: 26.898% (2324/8640)\n",
      "Loss: 2.134 | Acc: 27.057% (2355/8704)\n",
      "Loss: 2.134 | Acc: 27.087% (2375/8768)\n",
      "Loss: 2.131 | Acc: 27.117% (2395/8832)\n",
      "Loss: 2.130 | Acc: 27.181% (2418/8896)\n",
      "Loss: 2.127 | Acc: 27.210% (2438/8960)\n",
      "Loss: 2.124 | Acc: 27.294% (2463/9024)\n",
      "Loss: 2.123 | Acc: 27.300% (2481/9088)\n",
      "Loss: 2.123 | Acc: 27.371% (2505/9152)\n",
      "Loss: 2.120 | Acc: 27.463% (2531/9216)\n",
      "Loss: 2.118 | Acc: 27.478% (2550/9280)\n",
      "Loss: 2.117 | Acc: 27.494% (2569/9344)\n",
      "Loss: 2.113 | Acc: 27.551% (2592/9408)\n",
      "Loss: 2.111 | Acc: 27.587% (2613/9472)\n",
      "Loss: 2.110 | Acc: 27.622% (2634/9536)\n",
      "Loss: 2.109 | Acc: 27.594% (2649/9600)\n",
      "Loss: 2.108 | Acc: 27.618% (2669/9664)\n",
      "Loss: 2.106 | Acc: 27.621% (2687/9728)\n",
      "Loss: 2.106 | Acc: 27.614% (2704/9792)\n",
      "Loss: 2.104 | Acc: 27.648% (2725/9856)\n",
      "Loss: 2.102 | Acc: 27.671% (2745/9920)\n",
      "Loss: 2.100 | Acc: 27.734% (2769/9984)\n",
      "Loss: 2.099 | Acc: 27.717% (2785/10048)\n",
      "Loss: 2.097 | Acc: 27.729% (2804/10112)\n",
      "Loss: 2.094 | Acc: 27.791% (2828/10176)\n",
      "Loss: 2.093 | Acc: 27.793% (2846/10240)\n",
      "Loss: 2.090 | Acc: 27.844% (2869/10304)\n",
      "Loss: 2.089 | Acc: 27.922% (2895/10368)\n",
      "Loss: 2.087 | Acc: 27.972% (2918/10432)\n",
      "Loss: 2.085 | Acc: 28.011% (2940/10496)\n",
      "Loss: 2.082 | Acc: 28.116% (2969/10560)\n",
      "Loss: 2.080 | Acc: 28.181% (2994/10624)\n",
      "Loss: 2.079 | Acc: 28.134% (3007/10688)\n",
      "Loss: 2.077 | Acc: 28.172% (3029/10752)\n",
      "Loss: 2.075 | Acc: 28.199% (3050/10816)\n",
      "Loss: 2.074 | Acc: 28.254% (3074/10880)\n",
      "Loss: 2.073 | Acc: 28.253% (3092/10944)\n",
      "Loss: 2.071 | Acc: 28.243% (3109/11008)\n",
      "Loss: 2.069 | Acc: 28.288% (3132/11072)\n",
      "Loss: 2.067 | Acc: 28.305% (3152/11136)\n",
      "Loss: 2.066 | Acc: 28.339% (3174/11200)\n",
      "Loss: 2.064 | Acc: 28.391% (3198/11264)\n",
      "Loss: 2.062 | Acc: 28.496% (3228/11328)\n",
      "Loss: 2.061 | Acc: 28.485% (3245/11392)\n",
      "Loss: 2.061 | Acc: 28.500% (3265/11456)\n",
      "Loss: 2.060 | Acc: 28.533% (3287/11520)\n",
      "Loss: 2.057 | Acc: 28.608% (3314/11584)\n",
      "Loss: 2.056 | Acc: 28.674% (3340/11648)\n",
      "Loss: 2.054 | Acc: 28.723% (3364/11712)\n",
      "Loss: 2.051 | Acc: 28.745% (3385/11776)\n",
      "Loss: 2.049 | Acc: 28.809% (3411/11840)\n",
      "Loss: 2.048 | Acc: 28.814% (3430/11904)\n",
      "Loss: 2.046 | Acc: 28.885% (3457/11968)\n",
      "Loss: 2.044 | Acc: 28.915% (3479/12032)\n",
      "Loss: 2.042 | Acc: 28.968% (3504/12096)\n",
      "Loss: 2.041 | Acc: 28.988% (3525/12160)\n",
      "Loss: 2.040 | Acc: 29.049% (3551/12224)\n",
      "Loss: 2.038 | Acc: 29.093% (3575/12288)\n",
      "Loss: 2.036 | Acc: 29.186% (3605/12352)\n",
      "Loss: 2.033 | Acc: 29.277% (3635/12416)\n",
      "Loss: 2.031 | Acc: 29.383% (3667/12480)\n",
      "Loss: 2.028 | Acc: 29.504% (3701/12544)\n",
      "Loss: 2.027 | Acc: 29.545% (3725/12608)\n",
      "Loss: 2.025 | Acc: 29.624% (3754/12672)\n",
      "Loss: 2.024 | Acc: 29.656% (3777/12736)\n",
      "Loss: 2.023 | Acc: 29.680% (3799/12800)\n",
      "Loss: 2.021 | Acc: 29.765% (3829/12864)\n",
      "Loss: 2.020 | Acc: 29.804% (3853/12928)\n",
      "Loss: 2.018 | Acc: 29.872% (3881/12992)\n",
      "Loss: 2.016 | Acc: 29.948% (3910/13056)\n",
      "Loss: 2.014 | Acc: 29.992% (3935/13120)\n",
      "Loss: 2.013 | Acc: 29.976% (3952/13184)\n",
      "Loss: 2.011 | Acc: 30.072% (3984/13248)\n",
      "Loss: 2.009 | Acc: 30.086% (4005/13312)\n",
      "Loss: 2.007 | Acc: 30.106% (4027/13376)\n",
      "Loss: 2.007 | Acc: 30.179% (4056/13440)\n",
      "Loss: 2.005 | Acc: 30.206% (4079/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.004 | Acc: 30.262% (4106/13568)\n",
      "Loss: 2.004 | Acc: 30.238% (4122/13632)\n",
      "Loss: 2.002 | Acc: 30.250% (4143/13696)\n",
      "Loss: 2.001 | Acc: 30.269% (4165/13760)\n",
      "Loss: 2.001 | Acc: 30.302% (4189/13824)\n",
      "Loss: 1.999 | Acc: 30.343% (4214/13888)\n",
      "Loss: 1.997 | Acc: 30.368% (4237/13952)\n",
      "Loss: 1.996 | Acc: 30.387% (4259/14016)\n",
      "Loss: 1.994 | Acc: 30.433% (4285/14080)\n",
      "Loss: 1.993 | Acc: 30.444% (4306/14144)\n",
      "Loss: 1.992 | Acc: 30.469% (4329/14208)\n",
      "Loss: 1.990 | Acc: 30.521% (4356/14272)\n",
      "Loss: 1.989 | Acc: 30.552% (4380/14336)\n",
      "Loss: 1.987 | Acc: 30.618% (4409/14400)\n",
      "Loss: 1.984 | Acc: 30.704% (4441/14464)\n",
      "Loss: 1.983 | Acc: 30.775% (4471/14528)\n",
      "Loss: 1.981 | Acc: 30.853% (4502/14592)\n",
      "Loss: 1.979 | Acc: 30.888% (4527/14656)\n",
      "Loss: 1.979 | Acc: 30.897% (4548/14720)\n",
      "Loss: 1.977 | Acc: 30.979% (4580/14784)\n",
      "Loss: 1.975 | Acc: 31.041% (4609/14848)\n",
      "Loss: 1.974 | Acc: 31.029% (4627/14912)\n",
      "Loss: 1.973 | Acc: 31.076% (4654/14976)\n",
      "Loss: 1.970 | Acc: 31.203% (4693/15040)\n",
      "Loss: 1.969 | Acc: 31.230% (4717/15104)\n",
      "Loss: 1.969 | Acc: 31.243% (4739/15168)\n",
      "Loss: 1.968 | Acc: 31.237% (4758/15232)\n",
      "Loss: 1.966 | Acc: 31.296% (4787/15296)\n",
      "Loss: 1.966 | Acc: 31.296% (4807/15360)\n",
      "Loss: 1.965 | Acc: 31.334% (4833/15424)\n",
      "Loss: 1.963 | Acc: 31.373% (4859/15488)\n",
      "Loss: 1.962 | Acc: 31.404% (4884/15552)\n",
      "Loss: 1.960 | Acc: 31.442% (4910/15616)\n",
      "Loss: 1.959 | Acc: 31.499% (4939/15680)\n",
      "Loss: 1.957 | Acc: 31.555% (4968/15744)\n",
      "Loss: 1.956 | Acc: 31.566% (4990/15808)\n",
      "Loss: 1.954 | Acc: 31.641% (5022/15872)\n",
      "Loss: 1.954 | Acc: 31.645% (5043/15936)\n",
      "Loss: 1.953 | Acc: 31.675% (5068/16000)\n",
      "Loss: 1.952 | Acc: 31.692% (5091/16064)\n",
      "Loss: 1.951 | Acc: 31.715% (5115/16128)\n",
      "Loss: 1.950 | Acc: 31.732% (5138/16192)\n",
      "Loss: 1.949 | Acc: 31.748% (5161/16256)\n",
      "Loss: 1.948 | Acc: 31.789% (5188/16320)\n",
      "Loss: 1.946 | Acc: 31.824% (5214/16384)\n",
      "Loss: 1.946 | Acc: 31.876% (5243/16448)\n",
      "Loss: 1.945 | Acc: 31.916% (5270/16512)\n",
      "Loss: 1.944 | Acc: 31.908% (5289/16576)\n",
      "Loss: 1.943 | Acc: 31.941% (5315/16640)\n",
      "Loss: 1.942 | Acc: 31.992% (5344/16704)\n",
      "Loss: 1.941 | Acc: 32.025% (5370/16768)\n",
      "Loss: 1.940 | Acc: 32.040% (5393/16832)\n",
      "Loss: 1.939 | Acc: 32.108% (5425/16896)\n",
      "Loss: 1.938 | Acc: 32.123% (5448/16960)\n",
      "Loss: 1.937 | Acc: 32.137% (5471/17024)\n",
      "Loss: 1.936 | Acc: 32.140% (5492/17088)\n",
      "Loss: 1.935 | Acc: 32.165% (5517/17152)\n",
      "Loss: 1.933 | Acc: 32.226% (5548/17216)\n",
      "Loss: 1.932 | Acc: 32.280% (5578/17280)\n",
      "Loss: 1.931 | Acc: 32.294% (5601/17344)\n",
      "Loss: 1.930 | Acc: 32.330% (5628/17408)\n",
      "Loss: 1.930 | Acc: 32.320% (5647/17472)\n",
      "Loss: 1.930 | Acc: 32.339% (5671/17536)\n",
      "Loss: 1.929 | Acc: 32.386% (5700/17600)\n",
      "Loss: 1.928 | Acc: 32.428% (5728/17664)\n",
      "Loss: 1.927 | Acc: 32.451% (5753/17728)\n",
      "Loss: 1.927 | Acc: 32.470% (5777/17792)\n",
      "Loss: 1.925 | Acc: 32.499% (5803/17856)\n",
      "Loss: 1.925 | Acc: 32.489% (5822/17920)\n",
      "Loss: 1.923 | Acc: 32.507% (5846/17984)\n",
      "Loss: 1.922 | Acc: 32.513% (5868/18048)\n",
      "Loss: 1.921 | Acc: 32.531% (5892/18112)\n",
      "Loss: 1.920 | Acc: 32.565% (5919/18176)\n",
      "Loss: 1.919 | Acc: 32.626% (5951/18240)\n",
      "Loss: 1.919 | Acc: 32.621% (5971/18304)\n",
      "Loss: 1.918 | Acc: 32.633% (5994/18368)\n",
      "Loss: 1.917 | Acc: 32.612% (6011/18432)\n",
      "Loss: 1.917 | Acc: 32.634% (6036/18496)\n",
      "Loss: 1.916 | Acc: 32.635% (6057/18560)\n",
      "Loss: 1.915 | Acc: 32.662% (6083/18624)\n",
      "Loss: 1.914 | Acc: 32.673% (6106/18688)\n",
      "Loss: 1.914 | Acc: 32.711% (6134/18752)\n",
      "Loss: 1.913 | Acc: 32.712% (6155/18816)\n",
      "Loss: 1.913 | Acc: 32.722% (6178/18880)\n",
      "Loss: 1.911 | Acc: 32.797% (6213/18944)\n",
      "Loss: 1.910 | Acc: 32.818% (6238/19008)\n",
      "Loss: 1.909 | Acc: 32.849% (6265/19072)\n",
      "Loss: 1.907 | Acc: 32.896% (6295/19136)\n",
      "Loss: 1.907 | Acc: 32.901% (6317/19200)\n",
      "Loss: 1.907 | Acc: 32.901% (6338/19264)\n",
      "Loss: 1.906 | Acc: 32.963% (6371/19328)\n",
      "Loss: 1.904 | Acc: 33.024% (6404/19392)\n",
      "Loss: 1.903 | Acc: 33.039% (6428/19456)\n",
      "Loss: 1.903 | Acc: 33.064% (6454/19520)\n",
      "Loss: 1.902 | Acc: 33.114% (6485/19584)\n",
      "Loss: 1.901 | Acc: 33.123% (6508/19648)\n",
      "Loss: 1.900 | Acc: 33.173% (6539/19712)\n",
      "Loss: 1.899 | Acc: 33.202% (6566/19776)\n",
      "Loss: 1.898 | Acc: 33.241% (6595/19840)\n",
      "Loss: 1.897 | Acc: 33.265% (6621/19904)\n",
      "Loss: 1.896 | Acc: 33.283% (6646/19968)\n",
      "Loss: 1.895 | Acc: 33.302% (6671/20032)\n",
      "Loss: 1.895 | Acc: 33.330% (6698/20096)\n",
      "Loss: 1.893 | Acc: 33.373% (6728/20160)\n",
      "Loss: 1.892 | Acc: 33.411% (6757/20224)\n",
      "Loss: 1.892 | Acc: 33.404% (6777/20288)\n",
      "Loss: 1.892 | Acc: 33.402% (6798/20352)\n",
      "Loss: 1.892 | Acc: 33.395% (6818/20416)\n",
      "Loss: 1.891 | Acc: 33.418% (6844/20480)\n",
      "Loss: 1.891 | Acc: 33.450% (6872/20544)\n",
      "Loss: 1.889 | Acc: 33.502% (6904/20608)\n",
      "Loss: 1.889 | Acc: 33.519% (6929/20672)\n",
      "Loss: 1.889 | Acc: 33.550% (6957/20736)\n",
      "Loss: 1.887 | Acc: 33.591% (6987/20800)\n",
      "Loss: 1.886 | Acc: 33.603% (7011/20864)\n",
      "Loss: 1.886 | Acc: 33.587% (7029/20928)\n",
      "Loss: 1.885 | Acc: 33.632% (7060/20992)\n",
      "Loss: 1.885 | Acc: 33.620% (7079/21056)\n",
      "Loss: 1.885 | Acc: 33.641% (7105/21120)\n",
      "Loss: 1.884 | Acc: 33.648% (7128/21184)\n",
      "Loss: 1.883 | Acc: 33.697% (7160/21248)\n",
      "Loss: 1.881 | Acc: 33.737% (7190/21312)\n",
      "Loss: 1.881 | Acc: 33.753% (7215/21376)\n",
      "Loss: 1.880 | Acc: 33.797% (7246/21440)\n",
      "Loss: 1.878 | Acc: 33.850% (7279/21504)\n",
      "Loss: 1.877 | Acc: 33.884% (7308/21568)\n",
      "Loss: 1.876 | Acc: 33.899% (7333/21632)\n",
      "Loss: 1.875 | Acc: 33.942% (7364/21696)\n",
      "Loss: 1.874 | Acc: 33.994% (7397/21760)\n",
      "Loss: 1.873 | Acc: 34.004% (7421/21824)\n",
      "Loss: 1.873 | Acc: 34.032% (7449/21888)\n",
      "Loss: 1.872 | Acc: 34.052% (7475/21952)\n",
      "Loss: 1.872 | Acc: 34.048% (7496/22016)\n",
      "Loss: 1.871 | Acc: 34.067% (7522/22080)\n",
      "Loss: 1.870 | Acc: 34.090% (7549/22144)\n",
      "Loss: 1.869 | Acc: 34.127% (7579/22208)\n",
      "Loss: 1.867 | Acc: 34.177% (7612/22272)\n",
      "Loss: 1.866 | Acc: 34.227% (7645/22336)\n",
      "Loss: 1.865 | Acc: 34.254% (7673/22400)\n",
      "Loss: 1.864 | Acc: 34.295% (7704/22464)\n",
      "Loss: 1.864 | Acc: 34.339% (7736/22528)\n",
      "Loss: 1.863 | Acc: 34.371% (7765/22592)\n",
      "Loss: 1.862 | Acc: 34.375% (7788/22656)\n",
      "Loss: 1.862 | Acc: 34.384% (7812/22720)\n",
      "Loss: 1.861 | Acc: 34.406% (7839/22784)\n",
      "Loss: 1.860 | Acc: 34.423% (7865/22848)\n",
      "Loss: 1.860 | Acc: 34.432% (7889/22912)\n",
      "Loss: 1.859 | Acc: 34.449% (7915/22976)\n",
      "Loss: 1.859 | Acc: 34.484% (7945/23040)\n",
      "Loss: 1.858 | Acc: 34.496% (7970/23104)\n",
      "Loss: 1.857 | Acc: 34.504% (7994/23168)\n",
      "Loss: 1.856 | Acc: 34.539% (8024/23232)\n",
      "Loss: 1.855 | Acc: 34.598% (8060/23296)\n",
      "Loss: 1.854 | Acc: 34.619% (8087/23360)\n",
      "Loss: 1.853 | Acc: 34.631% (8112/23424)\n",
      "Loss: 1.853 | Acc: 34.626% (8133/23488)\n",
      "Loss: 1.852 | Acc: 34.672% (8166/23552)\n",
      "Loss: 1.851 | Acc: 34.705% (8196/23616)\n",
      "Loss: 1.850 | Acc: 34.692% (8215/23680)\n",
      "Loss: 1.850 | Acc: 34.712% (8242/23744)\n",
      "Loss: 1.849 | Acc: 34.732% (8269/23808)\n",
      "Loss: 1.847 | Acc: 34.773% (8301/23872)\n",
      "Loss: 1.846 | Acc: 34.793% (8328/23936)\n",
      "Loss: 1.845 | Acc: 34.842% (8362/24000)\n",
      "Loss: 1.844 | Acc: 34.853% (8387/24064)\n",
      "Loss: 1.844 | Acc: 34.856% (8410/24128)\n",
      "Loss: 1.842 | Acc: 34.875% (8437/24192)\n",
      "Loss: 1.841 | Acc: 34.878% (8460/24256)\n",
      "Loss: 1.840 | Acc: 34.914% (8491/24320)\n",
      "Loss: 1.840 | Acc: 34.912% (8513/24384)\n",
      "Loss: 1.839 | Acc: 34.927% (8539/24448)\n",
      "Loss: 1.838 | Acc: 34.942% (8565/24512)\n",
      "Loss: 1.837 | Acc: 34.981% (8597/24576)\n",
      "Loss: 1.837 | Acc: 34.984% (8620/24640)\n",
      "Loss: 1.836 | Acc: 34.982% (8642/24704)\n",
      "Loss: 1.836 | Acc: 34.997% (8668/24768)\n",
      "Loss: 1.835 | Acc: 35.048% (8703/24832)\n",
      "Loss: 1.834 | Acc: 35.074% (8732/24896)\n",
      "Loss: 1.834 | Acc: 35.088% (8758/24960)\n",
      "Loss: 1.833 | Acc: 35.122% (8789/25024)\n",
      "Loss: 1.832 | Acc: 35.132% (8814/25088)\n",
      "Loss: 1.832 | Acc: 35.158% (8843/25152)\n",
      "Loss: 1.831 | Acc: 35.172% (8869/25216)\n",
      "Loss: 1.830 | Acc: 35.198% (8898/25280)\n",
      "Loss: 1.830 | Acc: 35.204% (8922/25344)\n",
      "Loss: 1.829 | Acc: 35.241% (8954/25408)\n",
      "Loss: 1.828 | Acc: 35.282% (8987/25472)\n",
      "Loss: 1.828 | Acc: 35.291% (9012/25536)\n",
      "Loss: 1.827 | Acc: 35.320% (9042/25600)\n",
      "Loss: 1.827 | Acc: 35.326% (9066/25664)\n",
      "Loss: 1.826 | Acc: 35.343% (9093/25728)\n",
      "Loss: 1.825 | Acc: 35.391% (9128/25792)\n",
      "Loss: 1.824 | Acc: 35.412% (9156/25856)\n",
      "Loss: 1.824 | Acc: 35.471% (9194/25920)\n",
      "Loss: 1.822 | Acc: 35.518% (9229/25984)\n",
      "Loss: 1.822 | Acc: 35.546% (9259/26048)\n",
      "Loss: 1.822 | Acc: 35.535% (9279/26112)\n",
      "Loss: 1.822 | Acc: 35.529% (9300/26176)\n",
      "Loss: 1.821 | Acc: 35.549% (9328/26240)\n",
      "Loss: 1.821 | Acc: 35.554% (9352/26304)\n",
      "Loss: 1.820 | Acc: 35.585% (9383/26368)\n",
      "Loss: 1.820 | Acc: 35.578% (9404/26432)\n",
      "Loss: 1.820 | Acc: 35.583% (9428/26496)\n",
      "Loss: 1.820 | Acc: 35.561% (9445/26560)\n",
      "Loss: 1.819 | Acc: 35.581% (9473/26624)\n",
      "Loss: 1.819 | Acc: 35.600% (9501/26688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.819 | Acc: 35.624% (9530/26752)\n",
      "Loss: 1.818 | Acc: 35.643% (9558/26816)\n",
      "Loss: 1.817 | Acc: 35.636% (9579/26880)\n",
      "Loss: 1.817 | Acc: 35.641% (9603/26944)\n",
      "Loss: 1.816 | Acc: 35.682% (9637/27008)\n",
      "Loss: 1.816 | Acc: 35.694% (9663/27072)\n",
      "Loss: 1.815 | Acc: 35.724% (9694/27136)\n",
      "Loss: 1.814 | Acc: 35.746% (9723/27200)\n",
      "Loss: 1.813 | Acc: 35.780% (9755/27264)\n",
      "Loss: 1.812 | Acc: 35.817% (9788/27328)\n",
      "Loss: 1.812 | Acc: 35.832% (9815/27392)\n",
      "Loss: 1.811 | Acc: 35.857% (9845/27456)\n",
      "Loss: 1.810 | Acc: 35.883% (9875/27520)\n",
      "Loss: 1.809 | Acc: 35.927% (9910/27584)\n",
      "Loss: 1.808 | Acc: 35.952% (9940/27648)\n",
      "Loss: 1.808 | Acc: 35.974% (9969/27712)\n",
      "Loss: 1.807 | Acc: 35.992% (9997/27776)\n",
      "Loss: 1.806 | Acc: 36.027% (10030/27840)\n",
      "Loss: 1.805 | Acc: 36.070% (10065/27904)\n",
      "Loss: 1.805 | Acc: 36.073% (10089/27968)\n",
      "Loss: 1.804 | Acc: 36.077% (10113/28032)\n",
      "Loss: 1.804 | Acc: 36.094% (10141/28096)\n",
      "Loss: 1.804 | Acc: 36.094% (10164/28160)\n",
      "Loss: 1.803 | Acc: 36.115% (10193/28224)\n",
      "Loss: 1.803 | Acc: 36.139% (10223/28288)\n",
      "Loss: 1.803 | Acc: 36.135% (10245/28352)\n",
      "Loss: 1.803 | Acc: 36.128% (10266/28416)\n",
      "Loss: 1.802 | Acc: 36.148% (10295/28480)\n",
      "Loss: 1.802 | Acc: 36.169% (10324/28544)\n",
      "Loss: 1.801 | Acc: 36.172% (10348/28608)\n",
      "Loss: 1.801 | Acc: 36.168% (10370/28672)\n",
      "Loss: 1.801 | Acc: 36.188% (10399/28736)\n",
      "Loss: 1.800 | Acc: 36.201% (10426/28800)\n",
      "Loss: 1.800 | Acc: 36.235% (10459/28864)\n",
      "Loss: 1.799 | Acc: 36.266% (10491/28928)\n",
      "Loss: 1.798 | Acc: 36.276% (10517/28992)\n",
      "Loss: 1.798 | Acc: 36.261% (10536/29056)\n",
      "Loss: 1.798 | Acc: 36.253% (10557/29120)\n",
      "Loss: 1.798 | Acc: 36.277% (10587/29184)\n",
      "Loss: 1.798 | Acc: 36.290% (10614/29248)\n",
      "Loss: 1.797 | Acc: 36.303% (10641/29312)\n",
      "Loss: 1.797 | Acc: 36.312% (10667/29376)\n",
      "Loss: 1.797 | Acc: 36.304% (10688/29440)\n",
      "Loss: 1.797 | Acc: 36.310% (10713/29504)\n",
      "Loss: 1.796 | Acc: 36.320% (10739/29568)\n",
      "Loss: 1.796 | Acc: 36.322% (10763/29632)\n",
      "Loss: 1.795 | Acc: 36.355% (10796/29696)\n",
      "Loss: 1.796 | Acc: 36.341% (10815/29760)\n",
      "Loss: 1.796 | Acc: 36.333% (10836/29824)\n",
      "Loss: 1.795 | Acc: 36.346% (10863/29888)\n",
      "Loss: 1.794 | Acc: 36.382% (10897/29952)\n",
      "Loss: 1.794 | Acc: 36.404% (10927/30016)\n",
      "Loss: 1.793 | Acc: 36.430% (10958/30080)\n",
      "Loss: 1.793 | Acc: 36.425% (10980/30144)\n",
      "Loss: 1.792 | Acc: 36.461% (11014/30208)\n",
      "Loss: 1.791 | Acc: 36.486% (11045/30272)\n",
      "Loss: 1.791 | Acc: 36.495% (11071/30336)\n",
      "Loss: 1.790 | Acc: 36.526% (11104/30400)\n",
      "Loss: 1.789 | Acc: 36.545% (11133/30464)\n",
      "Loss: 1.788 | Acc: 36.563% (11162/30528)\n",
      "Loss: 1.788 | Acc: 36.565% (11186/30592)\n",
      "Loss: 1.788 | Acc: 36.554% (11206/30656)\n",
      "Loss: 1.788 | Acc: 36.549% (11228/30720)\n",
      "Loss: 1.788 | Acc: 36.555% (11253/30784)\n",
      "Loss: 1.787 | Acc: 36.566% (11280/30848)\n",
      "Loss: 1.787 | Acc: 36.584% (11309/30912)\n",
      "Loss: 1.787 | Acc: 36.586% (11333/30976)\n",
      "Loss: 1.786 | Acc: 36.614% (11365/31040)\n",
      "Loss: 1.786 | Acc: 36.642% (11397/31104)\n",
      "Loss: 1.785 | Acc: 36.634% (11418/31168)\n",
      "Loss: 1.785 | Acc: 36.645% (11445/31232)\n",
      "Loss: 1.784 | Acc: 36.663% (11474/31296)\n",
      "Loss: 1.784 | Acc: 36.687% (11505/31360)\n",
      "Loss: 1.784 | Acc: 36.689% (11529/31424)\n",
      "Loss: 1.783 | Acc: 36.719% (11562/31488)\n",
      "Loss: 1.782 | Acc: 36.743% (11593/31552)\n",
      "Loss: 1.781 | Acc: 36.747% (11618/31616)\n",
      "Loss: 1.780 | Acc: 36.777% (11651/31680)\n",
      "Loss: 1.780 | Acc: 36.791% (11679/31744)\n",
      "Loss: 1.780 | Acc: 36.805% (11707/31808)\n",
      "Loss: 1.779 | Acc: 36.816% (11734/31872)\n",
      "Loss: 1.779 | Acc: 36.833% (11763/31936)\n",
      "Loss: 1.778 | Acc: 36.847% (11791/32000)\n",
      "Loss: 1.779 | Acc: 36.829% (11809/32064)\n",
      "Loss: 1.778 | Acc: 36.840% (11836/32128)\n",
      "Loss: 1.777 | Acc: 36.866% (11868/32192)\n",
      "Loss: 1.777 | Acc: 36.868% (11892/32256)\n",
      "Loss: 1.777 | Acc: 36.869% (11916/32320)\n",
      "Loss: 1.777 | Acc: 36.864% (11938/32384)\n",
      "Loss: 1.776 | Acc: 36.881% (11967/32448)\n",
      "Loss: 1.776 | Acc: 36.900% (11997/32512)\n",
      "Loss: 1.775 | Acc: 36.941% (12034/32576)\n",
      "Loss: 1.774 | Acc: 36.955% (12062/32640)\n",
      "Loss: 1.774 | Acc: 36.968% (12090/32704)\n",
      "Loss: 1.774 | Acc: 36.981% (12118/32768)\n",
      "Loss: 1.774 | Acc: 36.988% (12144/32832)\n",
      "Loss: 1.773 | Acc: 37.004% (12173/32896)\n",
      "Loss: 1.773 | Acc: 37.002% (12196/32960)\n",
      "Loss: 1.773 | Acc: 37.009% (12222/33024)\n",
      "Loss: 1.772 | Acc: 37.026% (12251/33088)\n",
      "Loss: 1.771 | Acc: 37.051% (12283/33152)\n",
      "Loss: 1.770 | Acc: 37.085% (12318/33216)\n",
      "Loss: 1.770 | Acc: 37.097% (12346/33280)\n",
      "Loss: 1.769 | Acc: 37.110% (12374/33344)\n",
      "Loss: 1.769 | Acc: 37.111% (12398/33408)\n",
      "Loss: 1.769 | Acc: 37.115% (12423/33472)\n",
      "Loss: 1.768 | Acc: 37.127% (12451/33536)\n",
      "Loss: 1.768 | Acc: 37.155% (12484/33600)\n",
      "Loss: 1.768 | Acc: 37.167% (12512/33664)\n",
      "Loss: 1.767 | Acc: 37.201% (12547/33728)\n",
      "Loss: 1.767 | Acc: 37.219% (12577/33792)\n",
      "Loss: 1.767 | Acc: 37.243% (12609/33856)\n",
      "Loss: 1.767 | Acc: 37.258% (12638/33920)\n",
      "Loss: 1.766 | Acc: 37.273% (12667/33984)\n",
      "Loss: 1.766 | Acc: 37.280% (12693/34048)\n",
      "Loss: 1.765 | Acc: 37.318% (12730/34112)\n",
      "Loss: 1.764 | Acc: 37.345% (12763/34176)\n",
      "Loss: 1.764 | Acc: 37.357% (12791/34240)\n",
      "Loss: 1.763 | Acc: 37.380% (12823/34304)\n",
      "Loss: 1.763 | Acc: 37.410% (12857/34368)\n",
      "Loss: 1.762 | Acc: 37.436% (12890/34432)\n",
      "Loss: 1.761 | Acc: 37.465% (12924/34496)\n",
      "Loss: 1.761 | Acc: 37.459% (12946/34560)\n",
      "Loss: 1.760 | Acc: 37.465% (12972/34624)\n",
      "Loss: 1.760 | Acc: 37.483% (13002/34688)\n",
      "Loss: 1.759 | Acc: 37.503% (13033/34752)\n",
      "Loss: 1.758 | Acc: 37.529% (13066/34816)\n",
      "Loss: 1.758 | Acc: 37.549% (13097/34880)\n",
      "Loss: 1.757 | Acc: 37.589% (13135/34944)\n",
      "Loss: 1.757 | Acc: 37.611% (13167/35008)\n",
      "Loss: 1.756 | Acc: 37.625% (13196/35072)\n",
      "Loss: 1.755 | Acc: 37.642% (13226/35136)\n",
      "Loss: 1.755 | Acc: 37.665% (13258/35200)\n",
      "Loss: 1.754 | Acc: 37.684% (13289/35264)\n",
      "Loss: 1.753 | Acc: 37.735% (13331/35328)\n",
      "Loss: 1.753 | Acc: 37.746% (13359/35392)\n",
      "Loss: 1.752 | Acc: 37.751% (13385/35456)\n",
      "Loss: 1.751 | Acc: 37.784% (13421/35520)\n",
      "Loss: 1.751 | Acc: 37.804% (13452/35584)\n",
      "Loss: 1.750 | Acc: 37.825% (13484/35648)\n",
      "Loss: 1.750 | Acc: 37.830% (13510/35712)\n",
      "Loss: 1.750 | Acc: 37.844% (13539/35776)\n",
      "Loss: 1.749 | Acc: 37.857% (13568/35840)\n",
      "Loss: 1.749 | Acc: 37.865% (13595/35904)\n",
      "Loss: 1.749 | Acc: 37.889% (13628/35968)\n",
      "Loss: 1.749 | Acc: 37.880% (13649/36032)\n",
      "Loss: 1.748 | Acc: 37.896% (13679/36096)\n",
      "Loss: 1.748 | Acc: 37.907% (13707/36160)\n",
      "Loss: 1.747 | Acc: 37.936% (13742/36224)\n",
      "Loss: 1.747 | Acc: 37.930% (13764/36288)\n",
      "Loss: 1.747 | Acc: 37.937% (13791/36352)\n",
      "Loss: 1.746 | Acc: 37.959% (13823/36416)\n",
      "Loss: 1.746 | Acc: 37.982% (13856/36480)\n",
      "Loss: 1.745 | Acc: 37.998% (13886/36544)\n",
      "Loss: 1.745 | Acc: 38.016% (13917/36608)\n",
      "Loss: 1.744 | Acc: 38.051% (13954/36672)\n",
      "Loss: 1.743 | Acc: 38.066% (13984/36736)\n",
      "Loss: 1.743 | Acc: 38.060% (14006/36800)\n",
      "Loss: 1.743 | Acc: 38.070% (14034/36864)\n",
      "Loss: 1.742 | Acc: 38.093% (14067/36928)\n",
      "Loss: 1.742 | Acc: 38.097% (14093/36992)\n",
      "Loss: 1.741 | Acc: 38.118% (14125/37056)\n",
      "Loss: 1.740 | Acc: 38.152% (14162/37120)\n",
      "Loss: 1.741 | Acc: 38.151% (14186/37184)\n",
      "Loss: 1.740 | Acc: 38.182% (14222/37248)\n",
      "Loss: 1.739 | Acc: 38.205% (14255/37312)\n",
      "Loss: 1.739 | Acc: 38.230% (14289/37376)\n",
      "Loss: 1.738 | Acc: 38.240% (14317/37440)\n",
      "Loss: 1.738 | Acc: 38.244% (14343/37504)\n",
      "Loss: 1.738 | Acc: 38.235% (14364/37568)\n",
      "Loss: 1.738 | Acc: 38.239% (14390/37632)\n",
      "Loss: 1.738 | Acc: 38.248% (14418/37696)\n",
      "Loss: 1.738 | Acc: 38.247% (14442/37760)\n",
      "Loss: 1.737 | Acc: 38.269% (14475/37824)\n",
      "Loss: 1.737 | Acc: 38.273% (14501/37888)\n",
      "Loss: 1.736 | Acc: 38.293% (14533/37952)\n",
      "Loss: 1.736 | Acc: 38.294% (14558/38016)\n",
      "Loss: 1.736 | Acc: 38.311% (14589/38080)\n",
      "Loss: 1.735 | Acc: 38.344% (14626/38144)\n",
      "Loss: 1.735 | Acc: 38.361% (14657/38208)\n",
      "Loss: 1.735 | Acc: 38.367% (14684/38272)\n",
      "Loss: 1.735 | Acc: 38.363% (14707/38336)\n",
      "Loss: 1.735 | Acc: 38.378% (14737/38400)\n",
      "Loss: 1.734 | Acc: 38.389% (14766/38464)\n",
      "Loss: 1.734 | Acc: 38.395% (14793/38528)\n",
      "Loss: 1.733 | Acc: 38.412% (14824/38592)\n",
      "Loss: 1.733 | Acc: 38.413% (14849/38656)\n",
      "Loss: 1.732 | Acc: 38.427% (14879/38720)\n",
      "Loss: 1.733 | Acc: 38.415% (14899/38784)\n",
      "Loss: 1.733 | Acc: 38.424% (14927/38848)\n",
      "Loss: 1.732 | Acc: 38.443% (14959/38912)\n",
      "Loss: 1.731 | Acc: 38.460% (14990/38976)\n",
      "Loss: 1.731 | Acc: 38.466% (15017/39040)\n",
      "Loss: 1.730 | Acc: 38.467% (15042/39104)\n",
      "Loss: 1.730 | Acc: 38.493% (15077/39168)\n",
      "Loss: 1.729 | Acc: 38.489% (15100/39232)\n",
      "Loss: 1.729 | Acc: 38.498% (15128/39296)\n",
      "Loss: 1.728 | Acc: 38.526% (15164/39360)\n",
      "Loss: 1.728 | Acc: 38.550% (15198/39424)\n",
      "Loss: 1.727 | Acc: 38.566% (15229/39488)\n",
      "Loss: 1.727 | Acc: 38.575% (15257/39552)\n",
      "Loss: 1.726 | Acc: 38.596% (15290/39616)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.726 | Acc: 38.616% (15323/39680)\n",
      "Loss: 1.725 | Acc: 38.632% (15354/39744)\n",
      "Loss: 1.726 | Acc: 38.630% (15378/39808)\n",
      "Loss: 1.725 | Acc: 38.654% (15412/39872)\n",
      "Loss: 1.725 | Acc: 38.664% (15441/39936)\n",
      "Loss: 1.724 | Acc: 38.688% (15475/40000)\n",
      "Loss: 1.724 | Acc: 38.696% (15503/40064)\n",
      "Loss: 1.723 | Acc: 38.719% (15537/40128)\n",
      "Loss: 1.723 | Acc: 38.717% (15561/40192)\n",
      "Loss: 1.722 | Acc: 38.735% (15593/40256)\n",
      "Loss: 1.722 | Acc: 38.725% (15614/40320)\n",
      "Loss: 1.722 | Acc: 38.760% (15653/40384)\n",
      "Loss: 1.721 | Acc: 38.771% (15682/40448)\n",
      "Loss: 1.721 | Acc: 38.789% (15714/40512)\n",
      "Loss: 1.720 | Acc: 38.811% (15748/40576)\n",
      "Loss: 1.720 | Acc: 38.826% (15779/40640)\n",
      "Loss: 1.719 | Acc: 38.841% (15810/40704)\n",
      "Loss: 1.719 | Acc: 38.859% (15842/40768)\n",
      "Loss: 1.719 | Acc: 38.871% (15872/40832)\n",
      "Loss: 1.719 | Acc: 38.882% (15901/40896)\n",
      "Loss: 1.718 | Acc: 38.921% (15942/40960)\n",
      "Loss: 1.717 | Acc: 38.931% (15971/41024)\n",
      "Loss: 1.717 | Acc: 38.948% (16003/41088)\n",
      "Loss: 1.717 | Acc: 38.963% (16034/41152)\n",
      "Loss: 1.716 | Acc: 38.961% (16058/41216)\n",
      "Loss: 1.716 | Acc: 38.978% (16090/41280)\n",
      "Loss: 1.715 | Acc: 39.000% (16124/41344)\n",
      "Loss: 1.715 | Acc: 39.012% (16154/41408)\n",
      "Loss: 1.714 | Acc: 39.029% (16186/41472)\n",
      "Loss: 1.714 | Acc: 39.038% (16215/41536)\n",
      "Loss: 1.714 | Acc: 39.058% (16248/41600)\n",
      "Loss: 1.714 | Acc: 39.043% (16267/41664)\n",
      "Loss: 1.713 | Acc: 39.072% (16304/41728)\n",
      "Loss: 1.713 | Acc: 39.084% (16334/41792)\n",
      "Loss: 1.712 | Acc: 39.103% (16367/41856)\n",
      "Loss: 1.712 | Acc: 39.117% (16398/41920)\n",
      "Loss: 1.712 | Acc: 39.103% (16417/41984)\n",
      "Loss: 1.712 | Acc: 39.115% (16447/42048)\n",
      "Loss: 1.711 | Acc: 39.138% (16482/42112)\n",
      "Loss: 1.711 | Acc: 39.145% (16510/42176)\n",
      "Loss: 1.710 | Acc: 39.171% (16546/42240)\n",
      "Loss: 1.710 | Acc: 39.181% (16575/42304)\n",
      "Loss: 1.710 | Acc: 39.188% (16603/42368)\n",
      "Loss: 1.710 | Acc: 39.206% (16636/42432)\n",
      "Loss: 1.710 | Acc: 39.206% (16661/42496)\n",
      "Loss: 1.709 | Acc: 39.213% (16689/42560)\n",
      "Loss: 1.709 | Acc: 39.220% (16717/42624)\n",
      "Loss: 1.708 | Acc: 39.229% (16746/42688)\n",
      "Loss: 1.708 | Acc: 39.243% (16777/42752)\n",
      "Loss: 1.708 | Acc: 39.256% (16808/42816)\n",
      "Loss: 1.707 | Acc: 39.275% (16841/42880)\n",
      "Loss: 1.707 | Acc: 39.281% (16869/42944)\n",
      "Loss: 1.707 | Acc: 39.293% (16899/43008)\n",
      "Loss: 1.706 | Acc: 39.311% (16932/43072)\n",
      "Loss: 1.706 | Acc: 39.315% (16959/43136)\n",
      "Loss: 1.706 | Acc: 39.312% (16983/43200)\n",
      "Loss: 1.705 | Acc: 39.324% (17013/43264)\n",
      "Loss: 1.704 | Acc: 39.353% (17051/43328)\n",
      "Loss: 1.704 | Acc: 39.362% (17080/43392)\n",
      "Loss: 1.704 | Acc: 39.362% (17105/43456)\n",
      "Loss: 1.703 | Acc: 39.377% (17137/43520)\n",
      "Loss: 1.703 | Acc: 39.384% (17165/43584)\n",
      "Loss: 1.703 | Acc: 39.397% (17196/43648)\n",
      "Loss: 1.702 | Acc: 39.410% (17227/43712)\n",
      "Loss: 1.702 | Acc: 39.421% (17257/43776)\n",
      "Loss: 1.702 | Acc: 39.434% (17288/43840)\n",
      "Loss: 1.702 | Acc: 39.429% (17311/43904)\n",
      "Loss: 1.702 | Acc: 39.433% (17338/43968)\n",
      "Loss: 1.701 | Acc: 39.446% (17369/44032)\n",
      "Loss: 1.701 | Acc: 39.468% (17404/44096)\n",
      "Loss: 1.700 | Acc: 39.470% (17430/44160)\n",
      "Loss: 1.700 | Acc: 39.474% (17457/44224)\n",
      "Loss: 1.700 | Acc: 39.476% (17483/44288)\n",
      "Loss: 1.699 | Acc: 39.495% (17517/44352)\n",
      "Loss: 1.699 | Acc: 39.508% (17548/44416)\n",
      "Loss: 1.699 | Acc: 39.519% (17578/44480)\n",
      "Loss: 1.698 | Acc: 39.541% (17613/44544)\n",
      "Loss: 1.698 | Acc: 39.544% (17640/44608)\n",
      "Loss: 1.698 | Acc: 39.559% (17672/44672)\n",
      "Loss: 1.697 | Acc: 39.574% (17704/44736)\n",
      "Loss: 1.697 | Acc: 39.587% (17735/44800)\n",
      "Loss: 1.697 | Acc: 39.586% (17760/44864)\n",
      "Loss: 1.697 | Acc: 39.592% (17788/44928)\n",
      "Loss: 1.696 | Acc: 39.607% (17820/44992)\n",
      "Loss: 1.696 | Acc: 39.626% (17854/45056)\n",
      "Loss: 1.696 | Acc: 39.612% (17873/45120)\n",
      "Loss: 1.696 | Acc: 39.618% (17901/45184)\n",
      "Loss: 1.696 | Acc: 39.613% (17924/45248)\n",
      "Loss: 1.695 | Acc: 39.627% (17956/45312)\n",
      "Loss: 1.694 | Acc: 39.642% (17988/45376)\n",
      "Loss: 1.694 | Acc: 39.652% (18018/45440)\n",
      "Loss: 1.694 | Acc: 39.654% (18044/45504)\n",
      "Loss: 1.694 | Acc: 39.664% (18074/45568)\n",
      "Loss: 1.693 | Acc: 39.678% (18106/45632)\n",
      "Loss: 1.693 | Acc: 39.677% (18131/45696)\n",
      "Loss: 1.693 | Acc: 39.696% (18165/45760)\n",
      "Loss: 1.692 | Acc: 39.713% (18198/45824)\n",
      "Loss: 1.692 | Acc: 39.718% (18226/45888)\n",
      "Loss: 1.692 | Acc: 39.715% (18250/45952)\n",
      "Loss: 1.691 | Acc: 39.730% (18282/46016)\n",
      "Loss: 1.691 | Acc: 39.746% (18315/46080)\n",
      "Loss: 1.691 | Acc: 39.756% (18345/46144)\n",
      "Loss: 1.690 | Acc: 39.783% (18383/46208)\n",
      "Loss: 1.690 | Acc: 39.804% (18418/46272)\n",
      "Loss: 1.689 | Acc: 39.809% (18446/46336)\n",
      "Loss: 1.689 | Acc: 39.812% (18473/46400)\n",
      "Loss: 1.688 | Acc: 39.829% (18506/46464)\n",
      "Loss: 1.688 | Acc: 39.832% (18533/46528)\n",
      "Loss: 1.688 | Acc: 39.822% (18554/46592)\n",
      "Loss: 1.687 | Acc: 39.832% (18584/46656)\n",
      "Loss: 1.687 | Acc: 39.842% (18614/46720)\n",
      "Loss: 1.686 | Acc: 39.868% (18652/46784)\n",
      "Loss: 1.686 | Acc: 39.871% (18679/46848)\n",
      "Loss: 1.686 | Acc: 39.885% (18711/46912)\n",
      "Loss: 1.685 | Acc: 39.897% (18742/46976)\n",
      "Loss: 1.685 | Acc: 39.896% (18767/47040)\n",
      "Loss: 1.685 | Acc: 39.893% (18791/47104)\n",
      "Loss: 1.684 | Acc: 39.902% (18821/47168)\n",
      "Loss: 1.684 | Acc: 39.914% (18852/47232)\n",
      "Loss: 1.684 | Acc: 39.921% (18881/47296)\n",
      "Loss: 1.684 | Acc: 39.922% (18907/47360)\n",
      "Loss: 1.683 | Acc: 39.916% (18930/47424)\n",
      "Loss: 1.683 | Acc: 39.928% (18961/47488)\n",
      "Loss: 1.683 | Acc: 39.939% (18992/47552)\n",
      "Loss: 1.682 | Acc: 39.961% (19028/47616)\n",
      "Loss: 1.682 | Acc: 39.977% (19061/47680)\n",
      "Loss: 1.681 | Acc: 39.992% (19094/47744)\n",
      "Loss: 1.681 | Acc: 39.993% (19120/47808)\n",
      "Loss: 1.681 | Acc: 40.003% (19150/47872)\n",
      "Loss: 1.680 | Acc: 40.005% (19177/47936)\n",
      "Loss: 1.680 | Acc: 40.004% (19202/48000)\n",
      "Loss: 1.680 | Acc: 40.011% (19231/48064)\n",
      "Loss: 1.680 | Acc: 40.002% (19252/48128)\n",
      "Loss: 1.679 | Acc: 40.009% (19281/48192)\n",
      "Loss: 1.679 | Acc: 40.012% (19308/48256)\n",
      "Loss: 1.679 | Acc: 40.014% (19335/48320)\n",
      "Loss: 1.679 | Acc: 40.017% (19362/48384)\n",
      "Loss: 1.678 | Acc: 40.033% (19395/48448)\n",
      "Loss: 1.678 | Acc: 40.035% (19422/48512)\n",
      "Loss: 1.678 | Acc: 40.036% (19448/48576)\n",
      "Loss: 1.678 | Acc: 40.039% (19475/48640)\n",
      "Loss: 1.678 | Acc: 40.040% (19501/48704)\n",
      "Loss: 1.677 | Acc: 40.049% (19531/48768)\n",
      "Loss: 1.677 | Acc: 40.056% (19560/48832)\n",
      "Loss: 1.677 | Acc: 40.075% (19595/48896)\n",
      "Loss: 1.677 | Acc: 40.086% (19626/48960)\n",
      "Loss: 1.677 | Acc: 40.088% (19643/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 40.087755102040816\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.229 | Acc: 54.688% (35/64)\n",
      "Loss: 1.319 | Acc: 53.125% (68/128)\n",
      "Loss: 1.385 | Acc: 48.438% (93/192)\n",
      "Loss: 1.472 | Acc: 46.484% (119/256)\n",
      "Loss: 1.427 | Acc: 48.438% (155/320)\n",
      "Loss: 1.421 | Acc: 48.177% (185/384)\n",
      "Loss: 1.459 | Acc: 47.321% (212/448)\n",
      "Loss: 1.464 | Acc: 46.875% (240/512)\n",
      "Loss: 1.439 | Acc: 47.396% (273/576)\n",
      "Loss: 1.423 | Acc: 47.500% (304/640)\n",
      "Loss: 1.436 | Acc: 47.727% (336/704)\n",
      "Loss: 1.447 | Acc: 47.656% (366/768)\n",
      "Loss: 1.446 | Acc: 47.957% (399/832)\n",
      "Loss: 1.454 | Acc: 47.321% (424/896)\n",
      "Loss: 1.443 | Acc: 47.812% (459/960)\n",
      "Loss: 1.421 | Acc: 48.633% (498/1024)\n",
      "Loss: 1.424 | Acc: 48.438% (527/1088)\n",
      "Loss: 1.423 | Acc: 48.264% (556/1152)\n",
      "Loss: 1.419 | Acc: 48.684% (592/1216)\n",
      "Loss: 1.428 | Acc: 48.125% (616/1280)\n",
      "Loss: 1.428 | Acc: 48.214% (648/1344)\n",
      "Loss: 1.430 | Acc: 48.224% (679/1408)\n",
      "Loss: 1.428 | Acc: 48.098% (708/1472)\n",
      "Loss: 1.430 | Acc: 48.307% (742/1536)\n",
      "Loss: 1.430 | Acc: 48.188% (771/1600)\n",
      "Loss: 1.431 | Acc: 48.317% (804/1664)\n",
      "Loss: 1.432 | Acc: 48.495% (838/1728)\n",
      "Loss: 1.435 | Acc: 48.270% (865/1792)\n",
      "Loss: 1.433 | Acc: 48.491% (900/1856)\n",
      "Loss: 1.429 | Acc: 48.542% (932/1920)\n",
      "Loss: 1.435 | Acc: 48.438% (961/1984)\n",
      "Loss: 1.437 | Acc: 48.633% (996/2048)\n",
      "Loss: 1.434 | Acc: 48.769% (1030/2112)\n",
      "Loss: 1.439 | Acc: 48.529% (1056/2176)\n",
      "Loss: 1.437 | Acc: 48.750% (1092/2240)\n",
      "Loss: 1.436 | Acc: 48.741% (1123/2304)\n",
      "Loss: 1.436 | Acc: 48.818% (1156/2368)\n",
      "Loss: 1.436 | Acc: 48.890% (1189/2432)\n",
      "Loss: 1.432 | Acc: 49.038% (1224/2496)\n",
      "Loss: 1.441 | Acc: 48.789% (1249/2560)\n",
      "Loss: 1.440 | Acc: 48.704% (1278/2624)\n",
      "Loss: 1.441 | Acc: 48.661% (1308/2688)\n",
      "Loss: 1.438 | Acc: 48.619% (1338/2752)\n",
      "Loss: 1.440 | Acc: 48.651% (1370/2816)\n",
      "Loss: 1.439 | Acc: 48.681% (1402/2880)\n",
      "Loss: 1.437 | Acc: 48.811% (1437/2944)\n",
      "Loss: 1.433 | Acc: 48.836% (1469/3008)\n",
      "Loss: 1.434 | Acc: 48.796% (1499/3072)\n",
      "Loss: 1.436 | Acc: 48.756% (1529/3136)\n",
      "Loss: 1.433 | Acc: 48.906% (1565/3200)\n",
      "Loss: 1.434 | Acc: 49.020% (1600/3264)\n",
      "Loss: 1.437 | Acc: 48.888% (1627/3328)\n",
      "Loss: 1.436 | Acc: 48.939% (1660/3392)\n",
      "Loss: 1.437 | Acc: 48.900% (1690/3456)\n",
      "Loss: 1.438 | Acc: 48.949% (1723/3520)\n",
      "Loss: 1.435 | Acc: 48.996% (1756/3584)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.438 | Acc: 48.794% (1780/3648)\n",
      "Loss: 1.437 | Acc: 48.680% (1807/3712)\n",
      "Loss: 1.437 | Acc: 48.570% (1834/3776)\n",
      "Loss: 1.433 | Acc: 48.724% (1871/3840)\n",
      "Loss: 1.433 | Acc: 48.668% (1900/3904)\n",
      "Loss: 1.431 | Acc: 48.715% (1933/3968)\n",
      "Loss: 1.431 | Acc: 48.735% (1965/4032)\n",
      "Loss: 1.434 | Acc: 48.657% (1993/4096)\n",
      "Loss: 1.437 | Acc: 48.582% (2021/4160)\n",
      "Loss: 1.436 | Acc: 48.698% (2057/4224)\n",
      "Loss: 1.437 | Acc: 48.671% (2087/4288)\n",
      "Loss: 1.436 | Acc: 48.782% (2123/4352)\n",
      "Loss: 1.434 | Acc: 48.958% (2162/4416)\n",
      "Loss: 1.430 | Acc: 49.107% (2200/4480)\n",
      "Loss: 1.428 | Acc: 49.186% (2235/4544)\n",
      "Loss: 1.429 | Acc: 49.089% (2262/4608)\n",
      "Loss: 1.429 | Acc: 49.101% (2294/4672)\n",
      "Loss: 1.428 | Acc: 49.050% (2323/4736)\n",
      "Loss: 1.430 | Acc: 49.042% (2354/4800)\n",
      "Loss: 1.429 | Acc: 49.075% (2387/4864)\n",
      "Loss: 1.428 | Acc: 49.168% (2423/4928)\n",
      "Loss: 1.428 | Acc: 49.159% (2454/4992)\n",
      "Loss: 1.428 | Acc: 49.110% (2483/5056)\n",
      "Loss: 1.430 | Acc: 48.984% (2508/5120)\n",
      "Loss: 1.429 | Acc: 49.055% (2543/5184)\n",
      "Loss: 1.431 | Acc: 49.009% (2572/5248)\n",
      "Loss: 1.431 | Acc: 48.965% (2601/5312)\n",
      "Loss: 1.435 | Acc: 48.884% (2628/5376)\n",
      "Loss: 1.435 | Acc: 48.787% (2654/5440)\n",
      "Loss: 1.436 | Acc: 48.765% (2684/5504)\n",
      "Loss: 1.439 | Acc: 48.617% (2707/5568)\n",
      "Loss: 1.441 | Acc: 48.668% (2741/5632)\n",
      "Loss: 1.443 | Acc: 48.631% (2770/5696)\n",
      "Loss: 1.441 | Acc: 48.681% (2804/5760)\n",
      "Loss: 1.441 | Acc: 48.575% (2829/5824)\n",
      "Loss: 1.444 | Acc: 48.488% (2855/5888)\n",
      "Loss: 1.445 | Acc: 48.505% (2887/5952)\n",
      "Loss: 1.444 | Acc: 48.471% (2916/6016)\n",
      "Loss: 1.445 | Acc: 48.372% (2941/6080)\n",
      "Loss: 1.444 | Acc: 48.372% (2972/6144)\n",
      "Loss: 1.445 | Acc: 48.276% (2997/6208)\n",
      "Loss: 1.447 | Acc: 48.246% (3026/6272)\n",
      "Loss: 1.447 | Acc: 48.264% (3058/6336)\n",
      "Loss: 1.446 | Acc: 48.266% (3089/6400)\n",
      "Loss: 1.448 | Acc: 48.221% (3117/6464)\n",
      "Loss: 1.447 | Acc: 48.315% (3154/6528)\n",
      "Loss: 1.449 | Acc: 48.195% (3177/6592)\n",
      "Loss: 1.449 | Acc: 48.197% (3208/6656)\n",
      "Loss: 1.449 | Acc: 48.125% (3234/6720)\n",
      "Loss: 1.448 | Acc: 48.187% (3269/6784)\n",
      "Loss: 1.446 | Acc: 48.233% (3303/6848)\n",
      "Loss: 1.449 | Acc: 48.148% (3328/6912)\n",
      "Loss: 1.449 | Acc: 48.050% (3352/6976)\n",
      "Loss: 1.451 | Acc: 47.983% (3378/7040)\n",
      "Loss: 1.450 | Acc: 48.029% (3412/7104)\n",
      "Loss: 1.451 | Acc: 48.033% (3443/7168)\n",
      "Loss: 1.452 | Acc: 47.981% (3470/7232)\n",
      "Loss: 1.451 | Acc: 47.999% (3502/7296)\n",
      "Loss: 1.449 | Acc: 48.030% (3535/7360)\n",
      "Loss: 1.450 | Acc: 47.953% (3560/7424)\n",
      "Loss: 1.449 | Acc: 48.010% (3595/7488)\n",
      "Loss: 1.448 | Acc: 47.974% (3623/7552)\n",
      "Loss: 1.448 | Acc: 47.899% (3648/7616)\n",
      "Loss: 1.447 | Acc: 47.904% (3679/7680)\n",
      "Loss: 1.447 | Acc: 47.934% (3712/7744)\n",
      "Loss: 1.447 | Acc: 47.989% (3747/7808)\n",
      "Loss: 1.449 | Acc: 47.967% (3776/7872)\n",
      "Loss: 1.449 | Acc: 47.946% (3805/7936)\n",
      "Loss: 1.449 | Acc: 47.913% (3833/8000)\n",
      "Loss: 1.448 | Acc: 47.917% (3864/8064)\n",
      "Loss: 1.448 | Acc: 47.970% (3899/8128)\n",
      "Loss: 1.448 | Acc: 47.986% (3931/8192)\n",
      "Loss: 1.449 | Acc: 47.892% (3954/8256)\n",
      "Loss: 1.450 | Acc: 47.837% (3980/8320)\n",
      "Loss: 1.450 | Acc: 47.793% (4007/8384)\n",
      "Loss: 1.450 | Acc: 47.775% (4036/8448)\n",
      "Loss: 1.450 | Acc: 47.756% (4065/8512)\n",
      "Loss: 1.450 | Acc: 47.761% (4096/8576)\n",
      "Loss: 1.450 | Acc: 47.778% (4128/8640)\n",
      "Loss: 1.451 | Acc: 47.771% (4158/8704)\n",
      "Loss: 1.451 | Acc: 47.753% (4187/8768)\n",
      "Loss: 1.451 | Acc: 47.769% (4219/8832)\n",
      "Loss: 1.449 | Acc: 47.864% (4258/8896)\n",
      "Loss: 1.449 | Acc: 47.913% (4293/8960)\n",
      "Loss: 1.449 | Acc: 47.917% (4324/9024)\n",
      "Loss: 1.450 | Acc: 47.876% (4351/9088)\n",
      "Loss: 1.450 | Acc: 47.869% (4381/9152)\n",
      "Loss: 1.448 | Acc: 47.982% (4422/9216)\n",
      "Loss: 1.447 | Acc: 48.006% (4455/9280)\n",
      "Loss: 1.448 | Acc: 47.935% (4479/9344)\n",
      "Loss: 1.448 | Acc: 47.959% (4512/9408)\n",
      "Loss: 1.449 | Acc: 47.962% (4543/9472)\n",
      "Loss: 1.449 | Acc: 47.987% (4576/9536)\n",
      "Loss: 1.447 | Acc: 48.083% (4616/9600)\n",
      "Loss: 1.447 | Acc: 48.075% (4646/9664)\n",
      "Loss: 1.446 | Acc: 48.098% (4679/9728)\n",
      "Loss: 1.446 | Acc: 48.070% (4707/9792)\n",
      "Loss: 1.446 | Acc: 48.011% (4732/9856)\n",
      "Loss: 1.447 | Acc: 47.944% (4756/9920)\n",
      "Loss: 1.448 | Acc: 47.887% (4781/9984)\n",
      "Loss: 1.447 | Acc: 47.910% (4791/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 47.91\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.316 | Acc: 50.000% (32/64)\n",
      "Loss: 1.311 | Acc: 52.344% (67/128)\n",
      "Loss: 1.449 | Acc: 52.604% (101/192)\n",
      "Loss: 1.421 | Acc: 52.344% (134/256)\n",
      "Loss: 1.400 | Acc: 52.188% (167/320)\n",
      "Loss: 1.387 | Acc: 51.823% (199/384)\n",
      "Loss: 1.391 | Acc: 51.562% (231/448)\n",
      "Loss: 1.377 | Acc: 51.758% (265/512)\n",
      "Loss: 1.351 | Acc: 52.778% (304/576)\n",
      "Loss: 1.342 | Acc: 52.188% (334/640)\n",
      "Loss: 1.354 | Acc: 52.131% (367/704)\n",
      "Loss: 1.354 | Acc: 51.823% (398/768)\n",
      "Loss: 1.333 | Acc: 52.644% (438/832)\n",
      "Loss: 1.326 | Acc: 52.679% (472/896)\n",
      "Loss: 1.326 | Acc: 53.229% (511/960)\n",
      "Loss: 1.320 | Acc: 53.418% (547/1024)\n",
      "Loss: 1.325 | Acc: 52.574% (572/1088)\n",
      "Loss: 1.328 | Acc: 52.604% (606/1152)\n",
      "Loss: 1.330 | Acc: 52.796% (642/1216)\n",
      "Loss: 1.347 | Acc: 52.031% (666/1280)\n",
      "Loss: 1.363 | Acc: 51.786% (696/1344)\n",
      "Loss: 1.358 | Acc: 51.776% (729/1408)\n",
      "Loss: 1.366 | Acc: 51.630% (760/1472)\n",
      "Loss: 1.377 | Acc: 51.042% (784/1536)\n",
      "Loss: 1.379 | Acc: 51.062% (817/1600)\n",
      "Loss: 1.371 | Acc: 51.442% (856/1664)\n",
      "Loss: 1.374 | Acc: 50.984% (881/1728)\n",
      "Loss: 1.375 | Acc: 51.116% (916/1792)\n",
      "Loss: 1.381 | Acc: 51.024% (947/1856)\n",
      "Loss: 1.377 | Acc: 51.042% (980/1920)\n",
      "Loss: 1.379 | Acc: 51.260% (1017/1984)\n",
      "Loss: 1.383 | Acc: 51.123% (1047/2048)\n",
      "Loss: 1.387 | Acc: 50.852% (1074/2112)\n",
      "Loss: 1.391 | Acc: 50.827% (1106/2176)\n",
      "Loss: 1.391 | Acc: 50.848% (1139/2240)\n",
      "Loss: 1.392 | Acc: 50.564% (1165/2304)\n",
      "Loss: 1.395 | Acc: 50.253% (1190/2368)\n",
      "Loss: 1.401 | Acc: 50.082% (1218/2432)\n",
      "Loss: 1.408 | Acc: 49.960% (1247/2496)\n",
      "Loss: 1.412 | Acc: 49.844% (1276/2560)\n",
      "Loss: 1.415 | Acc: 49.771% (1306/2624)\n",
      "Loss: 1.413 | Acc: 49.926% (1342/2688)\n",
      "Loss: 1.414 | Acc: 49.782% (1370/2752)\n",
      "Loss: 1.415 | Acc: 49.680% (1399/2816)\n",
      "Loss: 1.416 | Acc: 49.583% (1428/2880)\n",
      "Loss: 1.413 | Acc: 49.728% (1464/2944)\n",
      "Loss: 1.408 | Acc: 49.934% (1502/3008)\n",
      "Loss: 1.405 | Acc: 49.967% (1535/3072)\n",
      "Loss: 1.401 | Acc: 50.032% (1569/3136)\n",
      "Loss: 1.398 | Acc: 50.125% (1604/3200)\n",
      "Loss: 1.399 | Acc: 50.245% (1640/3264)\n",
      "Loss: 1.399 | Acc: 50.331% (1675/3328)\n",
      "Loss: 1.399 | Acc: 50.265% (1705/3392)\n",
      "Loss: 1.397 | Acc: 50.347% (1740/3456)\n",
      "Loss: 1.395 | Acc: 50.398% (1774/3520)\n",
      "Loss: 1.394 | Acc: 50.502% (1810/3584)\n",
      "Loss: 1.393 | Acc: 50.576% (1845/3648)\n",
      "Loss: 1.396 | Acc: 50.458% (1873/3712)\n",
      "Loss: 1.394 | Acc: 50.477% (1906/3776)\n",
      "Loss: 1.391 | Acc: 50.573% (1942/3840)\n",
      "Loss: 1.392 | Acc: 50.435% (1969/3904)\n",
      "Loss: 1.393 | Acc: 50.378% (1999/3968)\n",
      "Loss: 1.390 | Acc: 50.422% (2033/4032)\n",
      "Loss: 1.388 | Acc: 50.342% (2062/4096)\n",
      "Loss: 1.386 | Acc: 50.385% (2096/4160)\n",
      "Loss: 1.386 | Acc: 50.379% (2128/4224)\n",
      "Loss: 1.387 | Acc: 50.326% (2158/4288)\n",
      "Loss: 1.388 | Acc: 50.230% (2186/4352)\n",
      "Loss: 1.387 | Acc: 50.340% (2223/4416)\n",
      "Loss: 1.386 | Acc: 50.290% (2253/4480)\n",
      "Loss: 1.388 | Acc: 50.220% (2282/4544)\n",
      "Loss: 1.387 | Acc: 50.152% (2311/4608)\n",
      "Loss: 1.388 | Acc: 50.064% (2339/4672)\n",
      "Loss: 1.388 | Acc: 50.063% (2371/4736)\n",
      "Loss: 1.390 | Acc: 50.000% (2400/4800)\n",
      "Loss: 1.395 | Acc: 49.897% (2427/4864)\n",
      "Loss: 1.397 | Acc: 49.797% (2454/4928)\n",
      "Loss: 1.396 | Acc: 49.800% (2486/4992)\n",
      "Loss: 1.396 | Acc: 49.802% (2518/5056)\n",
      "Loss: 1.398 | Acc: 49.609% (2540/5120)\n",
      "Loss: 1.398 | Acc: 49.633% (2573/5184)\n",
      "Loss: 1.399 | Acc: 49.562% (2601/5248)\n",
      "Loss: 1.402 | Acc: 49.379% (2623/5312)\n",
      "Loss: 1.407 | Acc: 49.256% (2648/5376)\n",
      "Loss: 1.404 | Acc: 49.338% (2684/5440)\n",
      "Loss: 1.404 | Acc: 49.364% (2717/5504)\n",
      "Loss: 1.405 | Acc: 49.335% (2747/5568)\n",
      "Loss: 1.408 | Acc: 49.201% (2771/5632)\n",
      "Loss: 1.407 | Acc: 49.315% (2809/5696)\n",
      "Loss: 1.408 | Acc: 49.323% (2841/5760)\n",
      "Loss: 1.409 | Acc: 49.296% (2871/5824)\n",
      "Loss: 1.412 | Acc: 49.134% (2893/5888)\n",
      "Loss: 1.410 | Acc: 49.093% (2922/5952)\n",
      "Loss: 1.410 | Acc: 49.119% (2955/6016)\n",
      "Loss: 1.410 | Acc: 49.095% (2985/6080)\n",
      "Loss: 1.409 | Acc: 49.137% (3019/6144)\n",
      "Loss: 1.409 | Acc: 49.146% (3051/6208)\n",
      "Loss: 1.409 | Acc: 49.123% (3081/6272)\n",
      "Loss: 1.410 | Acc: 49.085% (3110/6336)\n",
      "Loss: 1.413 | Acc: 49.016% (3137/6400)\n",
      "Loss: 1.413 | Acc: 49.041% (3170/6464)\n",
      "Loss: 1.412 | Acc: 49.050% (3202/6528)\n",
      "Loss: 1.412 | Acc: 49.029% (3232/6592)\n",
      "Loss: 1.411 | Acc: 49.144% (3271/6656)\n",
      "Loss: 1.409 | Acc: 49.256% (3310/6720)\n",
      "Loss: 1.408 | Acc: 49.307% (3345/6784)\n",
      "Loss: 1.409 | Acc: 49.255% (3373/6848)\n",
      "Loss: 1.409 | Acc: 49.146% (3397/6912)\n",
      "Loss: 1.411 | Acc: 49.083% (3424/6976)\n",
      "Loss: 1.411 | Acc: 49.048% (3453/7040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.411 | Acc: 49.113% (3489/7104)\n",
      "Loss: 1.412 | Acc: 49.037% (3515/7168)\n",
      "Loss: 1.411 | Acc: 49.087% (3550/7232)\n",
      "Loss: 1.410 | Acc: 49.095% (3582/7296)\n",
      "Loss: 1.411 | Acc: 49.062% (3611/7360)\n",
      "Loss: 1.411 | Acc: 48.976% (3636/7424)\n",
      "Loss: 1.410 | Acc: 49.025% (3671/7488)\n",
      "Loss: 1.409 | Acc: 49.086% (3707/7552)\n",
      "Loss: 1.409 | Acc: 49.055% (3736/7616)\n",
      "Loss: 1.408 | Acc: 49.102% (3771/7680)\n",
      "Loss: 1.409 | Acc: 49.109% (3803/7744)\n",
      "Loss: 1.408 | Acc: 49.155% (3838/7808)\n",
      "Loss: 1.406 | Acc: 49.200% (3873/7872)\n",
      "Loss: 1.405 | Acc: 49.231% (3907/7936)\n",
      "Loss: 1.406 | Acc: 49.212% (3937/8000)\n",
      "Loss: 1.406 | Acc: 49.206% (3968/8064)\n",
      "Loss: 1.404 | Acc: 49.311% (4008/8128)\n",
      "Loss: 1.403 | Acc: 49.341% (4042/8192)\n",
      "Loss: 1.404 | Acc: 49.249% (4066/8256)\n",
      "Loss: 1.405 | Acc: 49.219% (4095/8320)\n",
      "Loss: 1.406 | Acc: 49.237% (4128/8384)\n",
      "Loss: 1.405 | Acc: 49.290% (4164/8448)\n",
      "Loss: 1.407 | Acc: 49.248% (4192/8512)\n",
      "Loss: 1.407 | Acc: 49.207% (4220/8576)\n",
      "Loss: 1.408 | Acc: 49.236% (4254/8640)\n",
      "Loss: 1.408 | Acc: 49.230% (4285/8704)\n",
      "Loss: 1.408 | Acc: 49.190% (4313/8768)\n",
      "Loss: 1.408 | Acc: 49.241% (4349/8832)\n",
      "Loss: 1.407 | Acc: 49.269% (4383/8896)\n",
      "Loss: 1.406 | Acc: 49.263% (4414/8960)\n",
      "Loss: 1.407 | Acc: 49.291% (4448/9024)\n",
      "Loss: 1.408 | Acc: 49.219% (4473/9088)\n",
      "Loss: 1.408 | Acc: 49.202% (4503/9152)\n",
      "Loss: 1.407 | Acc: 49.164% (4531/9216)\n",
      "Loss: 1.408 | Acc: 49.116% (4558/9280)\n",
      "Loss: 1.408 | Acc: 49.090% (4587/9344)\n",
      "Loss: 1.409 | Acc: 48.980% (4608/9408)\n",
      "Loss: 1.410 | Acc: 48.976% (4639/9472)\n",
      "Loss: 1.408 | Acc: 49.025% (4675/9536)\n",
      "Loss: 1.409 | Acc: 49.000% (4704/9600)\n",
      "Loss: 1.408 | Acc: 48.996% (4735/9664)\n",
      "Loss: 1.408 | Acc: 48.993% (4766/9728)\n",
      "Loss: 1.408 | Acc: 48.999% (4798/9792)\n",
      "Loss: 1.410 | Acc: 49.006% (4830/9856)\n",
      "Loss: 1.411 | Acc: 48.972% (4858/9920)\n",
      "Loss: 1.410 | Acc: 48.958% (4888/9984)\n",
      "Loss: 1.410 | Acc: 48.945% (4918/10048)\n",
      "Loss: 1.410 | Acc: 48.932% (4948/10112)\n",
      "Loss: 1.410 | Acc: 48.988% (4985/10176)\n",
      "Loss: 1.410 | Acc: 48.945% (5012/10240)\n",
      "Loss: 1.409 | Acc: 49.020% (5051/10304)\n",
      "Loss: 1.408 | Acc: 49.007% (5081/10368)\n",
      "Loss: 1.409 | Acc: 49.013% (5113/10432)\n",
      "Loss: 1.409 | Acc: 48.981% (5141/10496)\n",
      "Loss: 1.409 | Acc: 48.949% (5169/10560)\n",
      "Loss: 1.409 | Acc: 48.936% (5199/10624)\n",
      "Loss: 1.408 | Acc: 48.990% (5236/10688)\n",
      "Loss: 1.409 | Acc: 48.940% (5262/10752)\n",
      "Loss: 1.411 | Acc: 48.844% (5283/10816)\n",
      "Loss: 1.412 | Acc: 48.778% (5307/10880)\n",
      "Loss: 1.412 | Acc: 48.803% (5341/10944)\n",
      "Loss: 1.413 | Acc: 48.765% (5368/11008)\n",
      "Loss: 1.412 | Acc: 48.772% (5400/11072)\n",
      "Loss: 1.412 | Acc: 48.752% (5429/11136)\n",
      "Loss: 1.412 | Acc: 48.768% (5462/11200)\n",
      "Loss: 1.413 | Acc: 48.722% (5488/11264)\n",
      "Loss: 1.414 | Acc: 48.720% (5519/11328)\n",
      "Loss: 1.413 | Acc: 48.727% (5551/11392)\n",
      "Loss: 1.413 | Acc: 48.760% (5586/11456)\n",
      "Loss: 1.411 | Acc: 48.819% (5624/11520)\n",
      "Loss: 1.410 | Acc: 48.886% (5663/11584)\n",
      "Loss: 1.411 | Acc: 48.867% (5692/11648)\n",
      "Loss: 1.411 | Acc: 48.890% (5726/11712)\n",
      "Loss: 1.409 | Acc: 48.956% (5765/11776)\n",
      "Loss: 1.410 | Acc: 48.927% (5793/11840)\n",
      "Loss: 1.409 | Acc: 48.908% (5822/11904)\n",
      "Loss: 1.409 | Acc: 48.947% (5858/11968)\n",
      "Loss: 1.409 | Acc: 48.895% (5883/12032)\n",
      "Loss: 1.410 | Acc: 48.851% (5909/12096)\n",
      "Loss: 1.412 | Acc: 48.791% (5933/12160)\n",
      "Loss: 1.413 | Acc: 48.757% (5960/12224)\n",
      "Loss: 1.413 | Acc: 48.796% (5996/12288)\n",
      "Loss: 1.413 | Acc: 48.794% (6027/12352)\n",
      "Loss: 1.415 | Acc: 48.760% (6054/12416)\n",
      "Loss: 1.416 | Acc: 48.694% (6077/12480)\n",
      "Loss: 1.416 | Acc: 48.693% (6108/12544)\n",
      "Loss: 1.417 | Acc: 48.699% (6140/12608)\n",
      "Loss: 1.415 | Acc: 48.761% (6179/12672)\n",
      "Loss: 1.416 | Acc: 48.697% (6202/12736)\n",
      "Loss: 1.416 | Acc: 48.680% (6231/12800)\n",
      "Loss: 1.417 | Acc: 48.678% (6262/12864)\n",
      "Loss: 1.416 | Acc: 48.670% (6292/12928)\n",
      "Loss: 1.416 | Acc: 48.676% (6324/12992)\n",
      "Loss: 1.416 | Acc: 48.698% (6358/13056)\n",
      "Loss: 1.416 | Acc: 48.704% (6390/13120)\n",
      "Loss: 1.416 | Acc: 48.642% (6413/13184)\n",
      "Loss: 1.416 | Acc: 48.656% (6446/13248)\n",
      "Loss: 1.415 | Acc: 48.678% (6480/13312)\n",
      "Loss: 1.415 | Acc: 48.684% (6512/13376)\n",
      "Loss: 1.415 | Acc: 48.705% (6546/13440)\n",
      "Loss: 1.414 | Acc: 48.719% (6579/13504)\n",
      "Loss: 1.414 | Acc: 48.754% (6615/13568)\n",
      "Loss: 1.414 | Acc: 48.753% (6646/13632)\n",
      "Loss: 1.414 | Acc: 48.751% (6677/13696)\n",
      "Loss: 1.414 | Acc: 48.743% (6707/13760)\n",
      "Loss: 1.415 | Acc: 48.691% (6731/13824)\n",
      "Loss: 1.413 | Acc: 48.747% (6770/13888)\n",
      "Loss: 1.414 | Acc: 48.724% (6798/13952)\n",
      "Loss: 1.413 | Acc: 48.744% (6832/14016)\n",
      "Loss: 1.414 | Acc: 48.714% (6859/14080)\n",
      "Loss: 1.413 | Acc: 48.742% (6894/14144)\n",
      "Loss: 1.412 | Acc: 48.768% (6929/14208)\n",
      "Loss: 1.411 | Acc: 48.816% (6967/14272)\n",
      "Loss: 1.410 | Acc: 48.814% (6998/14336)\n",
      "Loss: 1.409 | Acc: 48.854% (7035/14400)\n",
      "Loss: 1.408 | Acc: 48.894% (7072/14464)\n",
      "Loss: 1.407 | Acc: 48.912% (7106/14528)\n",
      "Loss: 1.407 | Acc: 48.945% (7142/14592)\n",
      "Loss: 1.408 | Acc: 48.908% (7168/14656)\n",
      "Loss: 1.409 | Acc: 48.893% (7197/14720)\n",
      "Loss: 1.408 | Acc: 48.918% (7232/14784)\n",
      "Loss: 1.408 | Acc: 48.929% (7265/14848)\n",
      "Loss: 1.409 | Acc: 48.934% (7297/14912)\n",
      "Loss: 1.409 | Acc: 48.918% (7326/14976)\n",
      "Loss: 1.409 | Acc: 48.923% (7358/15040)\n",
      "Loss: 1.409 | Acc: 48.927% (7390/15104)\n",
      "Loss: 1.409 | Acc: 48.952% (7425/15168)\n",
      "Loss: 1.409 | Acc: 48.950% (7456/15232)\n",
      "Loss: 1.409 | Acc: 48.941% (7486/15296)\n",
      "Loss: 1.410 | Acc: 48.919% (7514/15360)\n",
      "Loss: 1.409 | Acc: 48.963% (7552/15424)\n",
      "Loss: 1.409 | Acc: 48.973% (7585/15488)\n",
      "Loss: 1.408 | Acc: 48.990% (7619/15552)\n",
      "Loss: 1.409 | Acc: 48.995% (7651/15616)\n",
      "Loss: 1.408 | Acc: 49.005% (7684/15680)\n",
      "Loss: 1.407 | Acc: 49.041% (7721/15744)\n",
      "Loss: 1.407 | Acc: 49.064% (7756/15808)\n",
      "Loss: 1.406 | Acc: 49.093% (7792/15872)\n",
      "Loss: 1.406 | Acc: 49.128% (7829/15936)\n",
      "Loss: 1.405 | Acc: 49.188% (7870/16000)\n",
      "Loss: 1.405 | Acc: 49.147% (7895/16064)\n",
      "Loss: 1.405 | Acc: 49.157% (7928/16128)\n",
      "Loss: 1.406 | Acc: 49.160% (7960/16192)\n",
      "Loss: 1.406 | Acc: 49.139% (7988/16256)\n",
      "Loss: 1.405 | Acc: 49.142% (8020/16320)\n",
      "Loss: 1.405 | Acc: 49.146% (8052/16384)\n",
      "Loss: 1.404 | Acc: 49.185% (8090/16448)\n",
      "Loss: 1.403 | Acc: 49.225% (8128/16512)\n",
      "Loss: 1.403 | Acc: 49.264% (8166/16576)\n",
      "Loss: 1.404 | Acc: 49.249% (8195/16640)\n",
      "Loss: 1.404 | Acc: 49.258% (8228/16704)\n",
      "Loss: 1.405 | Acc: 49.237% (8256/16768)\n",
      "Loss: 1.405 | Acc: 49.257% (8291/16832)\n",
      "Loss: 1.405 | Acc: 49.254% (8322/16896)\n",
      "Loss: 1.405 | Acc: 49.263% (8355/16960)\n",
      "Loss: 1.405 | Acc: 49.307% (8394/17024)\n",
      "Loss: 1.404 | Acc: 49.333% (8430/17088)\n",
      "Loss: 1.405 | Acc: 49.318% (8459/17152)\n",
      "Loss: 1.404 | Acc: 49.367% (8499/17216)\n",
      "Loss: 1.404 | Acc: 49.381% (8533/17280)\n",
      "Loss: 1.403 | Acc: 49.383% (8565/17344)\n",
      "Loss: 1.403 | Acc: 49.380% (8596/17408)\n",
      "Loss: 1.403 | Acc: 49.399% (8631/17472)\n",
      "Loss: 1.403 | Acc: 49.430% (8668/17536)\n",
      "Loss: 1.403 | Acc: 49.420% (8698/17600)\n",
      "Loss: 1.404 | Acc: 49.428% (8731/17664)\n",
      "Loss: 1.404 | Acc: 49.419% (8761/17728)\n",
      "Loss: 1.403 | Acc: 49.404% (8790/17792)\n",
      "Loss: 1.404 | Acc: 49.406% (8822/17856)\n",
      "Loss: 1.403 | Acc: 49.459% (8863/17920)\n",
      "Loss: 1.403 | Acc: 49.466% (8896/17984)\n",
      "Loss: 1.403 | Acc: 49.468% (8928/18048)\n",
      "Loss: 1.402 | Acc: 49.487% (8963/18112)\n",
      "Loss: 1.402 | Acc: 49.483% (8994/18176)\n",
      "Loss: 1.403 | Acc: 49.457% (9021/18240)\n",
      "Loss: 1.403 | Acc: 49.426% (9047/18304)\n",
      "Loss: 1.404 | Acc: 49.428% (9079/18368)\n",
      "Loss: 1.402 | Acc: 49.457% (9116/18432)\n",
      "Loss: 1.402 | Acc: 49.459% (9148/18496)\n",
      "Loss: 1.402 | Acc: 49.483% (9184/18560)\n",
      "Loss: 1.401 | Acc: 49.501% (9219/18624)\n",
      "Loss: 1.401 | Acc: 49.518% (9254/18688)\n",
      "Loss: 1.400 | Acc: 49.563% (9294/18752)\n",
      "Loss: 1.401 | Acc: 49.548% (9323/18816)\n",
      "Loss: 1.401 | Acc: 49.571% (9359/18880)\n",
      "Loss: 1.401 | Acc: 49.567% (9390/18944)\n",
      "Loss: 1.401 | Acc: 49.574% (9423/19008)\n",
      "Loss: 1.401 | Acc: 49.575% (9455/19072)\n",
      "Loss: 1.401 | Acc: 49.566% (9485/19136)\n",
      "Loss: 1.401 | Acc: 49.609% (9525/19200)\n",
      "Loss: 1.401 | Acc: 49.590% (9553/19264)\n",
      "Loss: 1.401 | Acc: 49.555% (9578/19328)\n",
      "Loss: 1.401 | Acc: 49.562% (9611/19392)\n",
      "Loss: 1.401 | Acc: 49.563% (9643/19456)\n",
      "Loss: 1.400 | Acc: 49.570% (9676/19520)\n",
      "Loss: 1.400 | Acc: 49.581% (9710/19584)\n",
      "Loss: 1.401 | Acc: 49.562% (9738/19648)\n",
      "Loss: 1.401 | Acc: 49.564% (9770/19712)\n",
      "Loss: 1.400 | Acc: 49.590% (9807/19776)\n",
      "Loss: 1.400 | Acc: 49.607% (9842/19840)\n",
      "Loss: 1.400 | Acc: 49.618% (9876/19904)\n",
      "Loss: 1.400 | Acc: 49.624% (9909/19968)\n",
      "Loss: 1.399 | Acc: 49.656% (9947/20032)\n",
      "Loss: 1.399 | Acc: 49.662% (9980/20096)\n",
      "Loss: 1.399 | Acc: 49.673% (10014/20160)\n",
      "Loss: 1.399 | Acc: 49.693% (10050/20224)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.399 | Acc: 49.675% (10078/20288)\n",
      "Loss: 1.398 | Acc: 49.686% (10112/20352)\n",
      "Loss: 1.398 | Acc: 49.662% (10139/20416)\n",
      "Loss: 1.398 | Acc: 49.644% (10167/20480)\n",
      "Loss: 1.398 | Acc: 49.625% (10195/20544)\n",
      "Loss: 1.398 | Acc: 49.641% (10230/20608)\n",
      "Loss: 1.398 | Acc: 49.632% (10260/20672)\n",
      "Loss: 1.398 | Acc: 49.638% (10293/20736)\n",
      "Loss: 1.397 | Acc: 49.644% (10326/20800)\n",
      "Loss: 1.397 | Acc: 49.645% (10358/20864)\n",
      "Loss: 1.397 | Acc: 49.642% (10389/20928)\n",
      "Loss: 1.397 | Acc: 49.652% (10423/20992)\n",
      "Loss: 1.397 | Acc: 49.649% (10454/21056)\n",
      "Loss: 1.398 | Acc: 49.645% (10485/21120)\n",
      "Loss: 1.397 | Acc: 49.646% (10517/21184)\n",
      "Loss: 1.397 | Acc: 49.642% (10548/21248)\n",
      "Loss: 1.397 | Acc: 49.667% (10585/21312)\n",
      "Loss: 1.397 | Acc: 49.701% (10624/21376)\n",
      "Loss: 1.397 | Acc: 49.683% (10652/21440)\n",
      "Loss: 1.397 | Acc: 49.688% (10685/21504)\n",
      "Loss: 1.397 | Acc: 49.680% (10715/21568)\n",
      "Loss: 1.397 | Acc: 49.695% (10750/21632)\n",
      "Loss: 1.397 | Acc: 49.714% (10786/21696)\n",
      "Loss: 1.396 | Acc: 49.733% (10822/21760)\n",
      "Loss: 1.396 | Acc: 49.739% (10855/21824)\n",
      "Loss: 1.396 | Acc: 49.735% (10886/21888)\n",
      "Loss: 1.396 | Acc: 49.708% (10912/21952)\n",
      "Loss: 1.396 | Acc: 49.723% (10947/22016)\n",
      "Loss: 1.396 | Acc: 49.746% (10984/22080)\n",
      "Loss: 1.396 | Acc: 49.752% (11017/22144)\n",
      "Loss: 1.396 | Acc: 49.739% (11046/22208)\n",
      "Loss: 1.395 | Acc: 49.776% (11086/22272)\n",
      "Loss: 1.395 | Acc: 49.754% (11113/22336)\n",
      "Loss: 1.394 | Acc: 49.781% (11151/22400)\n",
      "Loss: 1.394 | Acc: 49.800% (11187/22464)\n",
      "Loss: 1.394 | Acc: 49.809% (11221/22528)\n",
      "Loss: 1.393 | Acc: 49.823% (11256/22592)\n",
      "Loss: 1.393 | Acc: 49.823% (11288/22656)\n",
      "Loss: 1.393 | Acc: 49.833% (11322/22720)\n",
      "Loss: 1.393 | Acc: 49.842% (11356/22784)\n",
      "Loss: 1.393 | Acc: 49.834% (11386/22848)\n",
      "Loss: 1.394 | Acc: 49.821% (11415/22912)\n",
      "Loss: 1.393 | Acc: 49.835% (11450/22976)\n",
      "Loss: 1.393 | Acc: 49.852% (11486/23040)\n",
      "Loss: 1.392 | Acc: 49.892% (11527/23104)\n",
      "Loss: 1.392 | Acc: 49.909% (11563/23168)\n",
      "Loss: 1.392 | Acc: 49.914% (11596/23232)\n",
      "Loss: 1.391 | Acc: 49.948% (11636/23296)\n",
      "Loss: 1.390 | Acc: 49.961% (11671/23360)\n",
      "Loss: 1.391 | Acc: 49.932% (11696/23424)\n",
      "Loss: 1.390 | Acc: 49.936% (11729/23488)\n",
      "Loss: 1.390 | Acc: 49.945% (11763/23552)\n",
      "Loss: 1.390 | Acc: 49.941% (11794/23616)\n",
      "Loss: 1.390 | Acc: 49.920% (11821/23680)\n",
      "Loss: 1.390 | Acc: 49.912% (11851/23744)\n",
      "Loss: 1.390 | Acc: 49.929% (11887/23808)\n",
      "Loss: 1.389 | Acc: 49.933% (11920/23872)\n",
      "Loss: 1.389 | Acc: 49.962% (11959/23936)\n",
      "Loss: 1.390 | Acc: 49.938% (11985/24000)\n",
      "Loss: 1.390 | Acc: 49.946% (12019/24064)\n",
      "Loss: 1.390 | Acc: 49.963% (12055/24128)\n",
      "Loss: 1.389 | Acc: 49.992% (12094/24192)\n",
      "Loss: 1.389 | Acc: 50.008% (12130/24256)\n",
      "Loss: 1.389 | Acc: 50.004% (12161/24320)\n",
      "Loss: 1.389 | Acc: 50.004% (12193/24384)\n",
      "Loss: 1.389 | Acc: 50.004% (12225/24448)\n",
      "Loss: 1.389 | Acc: 49.996% (12255/24512)\n",
      "Loss: 1.389 | Acc: 49.996% (12287/24576)\n",
      "Loss: 1.389 | Acc: 49.984% (12316/24640)\n",
      "Loss: 1.389 | Acc: 49.984% (12348/24704)\n",
      "Loss: 1.389 | Acc: 49.992% (12382/24768)\n",
      "Loss: 1.389 | Acc: 50.004% (12417/24832)\n",
      "Loss: 1.388 | Acc: 49.992% (12446/24896)\n",
      "Loss: 1.389 | Acc: 49.996% (12479/24960)\n",
      "Loss: 1.389 | Acc: 49.988% (12509/25024)\n",
      "Loss: 1.389 | Acc: 49.976% (12538/25088)\n",
      "Loss: 1.389 | Acc: 49.976% (12570/25152)\n",
      "Loss: 1.389 | Acc: 49.960% (12598/25216)\n",
      "Loss: 1.389 | Acc: 49.968% (12632/25280)\n",
      "Loss: 1.389 | Acc: 49.964% (12663/25344)\n",
      "Loss: 1.389 | Acc: 49.969% (12696/25408)\n",
      "Loss: 1.389 | Acc: 49.973% (12729/25472)\n",
      "Loss: 1.389 | Acc: 49.996% (12767/25536)\n",
      "Loss: 1.389 | Acc: 49.996% (12799/25600)\n",
      "Loss: 1.388 | Acc: 50.016% (12836/25664)\n",
      "Loss: 1.388 | Acc: 50.031% (12872/25728)\n",
      "Loss: 1.389 | Acc: 50.019% (12901/25792)\n",
      "Loss: 1.389 | Acc: 50.012% (12931/25856)\n",
      "Loss: 1.389 | Acc: 50.023% (12966/25920)\n",
      "Loss: 1.389 | Acc: 50.015% (12996/25984)\n",
      "Loss: 1.389 | Acc: 50.015% (13028/26048)\n",
      "Loss: 1.389 | Acc: 50.011% (13059/26112)\n",
      "Loss: 1.389 | Acc: 49.981% (13083/26176)\n",
      "Loss: 1.390 | Acc: 49.958% (13109/26240)\n",
      "Loss: 1.390 | Acc: 49.947% (13138/26304)\n",
      "Loss: 1.390 | Acc: 49.962% (13174/26368)\n",
      "Loss: 1.390 | Acc: 49.958% (13205/26432)\n",
      "Loss: 1.390 | Acc: 49.981% (13243/26496)\n",
      "Loss: 1.390 | Acc: 49.981% (13275/26560)\n",
      "Loss: 1.390 | Acc: 49.970% (13304/26624)\n",
      "Loss: 1.390 | Acc: 49.974% (13337/26688)\n",
      "Loss: 1.390 | Acc: 49.993% (13374/26752)\n",
      "Loss: 1.390 | Acc: 49.974% (13401/26816)\n",
      "Loss: 1.390 | Acc: 49.981% (13435/26880)\n",
      "Loss: 1.389 | Acc: 49.985% (13468/26944)\n",
      "Loss: 1.389 | Acc: 50.011% (13507/27008)\n",
      "Loss: 1.389 | Acc: 50.011% (13539/27072)\n",
      "Loss: 1.389 | Acc: 50.018% (13573/27136)\n",
      "Loss: 1.389 | Acc: 50.044% (13612/27200)\n",
      "Loss: 1.389 | Acc: 50.044% (13644/27264)\n",
      "Loss: 1.390 | Acc: 50.026% (13671/27328)\n",
      "Loss: 1.389 | Acc: 50.047% (13709/27392)\n",
      "Loss: 1.390 | Acc: 50.051% (13742/27456)\n",
      "Loss: 1.389 | Acc: 50.058% (13776/27520)\n",
      "Loss: 1.389 | Acc: 50.080% (13814/27584)\n",
      "Loss: 1.389 | Acc: 50.083% (13847/27648)\n",
      "Loss: 1.388 | Acc: 50.101% (13884/27712)\n",
      "Loss: 1.388 | Acc: 50.112% (13919/27776)\n",
      "Loss: 1.388 | Acc: 50.115% (13952/27840)\n",
      "Loss: 1.388 | Acc: 50.122% (13986/27904)\n",
      "Loss: 1.388 | Acc: 50.129% (14020/27968)\n",
      "Loss: 1.388 | Acc: 50.143% (14056/28032)\n",
      "Loss: 1.387 | Acc: 50.153% (14091/28096)\n",
      "Loss: 1.387 | Acc: 50.153% (14123/28160)\n",
      "Loss: 1.387 | Acc: 50.135% (14150/28224)\n",
      "Loss: 1.387 | Acc: 50.152% (14187/28288)\n",
      "Loss: 1.387 | Acc: 50.148% (14218/28352)\n",
      "Loss: 1.387 | Acc: 50.141% (14248/28416)\n",
      "Loss: 1.387 | Acc: 50.112% (14272/28480)\n",
      "Loss: 1.387 | Acc: 50.119% (14306/28544)\n",
      "Loss: 1.386 | Acc: 50.119% (14338/28608)\n",
      "Loss: 1.387 | Acc: 50.126% (14372/28672)\n",
      "Loss: 1.386 | Acc: 50.146% (14410/28736)\n",
      "Loss: 1.387 | Acc: 50.128% (14437/28800)\n",
      "Loss: 1.387 | Acc: 50.121% (14467/28864)\n",
      "Loss: 1.387 | Acc: 50.131% (14502/28928)\n",
      "Loss: 1.386 | Acc: 50.152% (14540/28992)\n",
      "Loss: 1.386 | Acc: 50.151% (14572/29056)\n",
      "Loss: 1.386 | Acc: 50.158% (14606/29120)\n",
      "Loss: 1.386 | Acc: 50.154% (14637/29184)\n",
      "Loss: 1.386 | Acc: 50.157% (14670/29248)\n",
      "Loss: 1.386 | Acc: 50.157% (14702/29312)\n",
      "Loss: 1.385 | Acc: 50.167% (14737/29376)\n",
      "Loss: 1.385 | Acc: 50.166% (14769/29440)\n",
      "Loss: 1.386 | Acc: 50.176% (14804/29504)\n",
      "Loss: 1.386 | Acc: 50.162% (14832/29568)\n",
      "Loss: 1.386 | Acc: 50.162% (14864/29632)\n",
      "Loss: 1.387 | Acc: 50.155% (14894/29696)\n",
      "Loss: 1.387 | Acc: 50.158% (14927/29760)\n",
      "Loss: 1.387 | Acc: 50.168% (14962/29824)\n",
      "Loss: 1.386 | Acc: 50.161% (14992/29888)\n",
      "Loss: 1.386 | Acc: 50.180% (15030/29952)\n",
      "Loss: 1.386 | Acc: 50.190% (15065/30016)\n",
      "Loss: 1.386 | Acc: 50.186% (15096/30080)\n",
      "Loss: 1.386 | Acc: 50.176% (15125/30144)\n",
      "Loss: 1.386 | Acc: 50.166% (15154/30208)\n",
      "Loss: 1.386 | Acc: 50.182% (15191/30272)\n",
      "Loss: 1.386 | Acc: 50.191% (15226/30336)\n",
      "Loss: 1.386 | Acc: 50.181% (15255/30400)\n",
      "Loss: 1.386 | Acc: 50.164% (15282/30464)\n",
      "Loss: 1.386 | Acc: 50.174% (15317/30528)\n",
      "Loss: 1.386 | Acc: 50.154% (15343/30592)\n",
      "Loss: 1.386 | Acc: 50.170% (15380/30656)\n",
      "Loss: 1.386 | Acc: 50.150% (15406/30720)\n",
      "Loss: 1.386 | Acc: 50.146% (15437/30784)\n",
      "Loss: 1.386 | Acc: 50.130% (15464/30848)\n",
      "Loss: 1.386 | Acc: 50.110% (15490/30912)\n",
      "Loss: 1.386 | Acc: 50.132% (15529/30976)\n",
      "Loss: 1.386 | Acc: 50.135% (15562/31040)\n",
      "Loss: 1.386 | Acc: 50.119% (15589/31104)\n",
      "Loss: 1.387 | Acc: 50.122% (15622/31168)\n",
      "Loss: 1.386 | Acc: 50.147% (15662/31232)\n",
      "Loss: 1.386 | Acc: 50.160% (15698/31296)\n",
      "Loss: 1.386 | Acc: 50.156% (15729/31360)\n",
      "Loss: 1.386 | Acc: 50.153% (15760/31424)\n",
      "Loss: 1.386 | Acc: 50.156% (15793/31488)\n",
      "Loss: 1.386 | Acc: 50.149% (15823/31552)\n",
      "Loss: 1.386 | Acc: 50.152% (15856/31616)\n",
      "Loss: 1.386 | Acc: 50.161% (15891/31680)\n",
      "Loss: 1.386 | Acc: 50.154% (15921/31744)\n",
      "Loss: 1.385 | Acc: 50.170% (15958/31808)\n",
      "Loss: 1.386 | Acc: 50.173% (15991/31872)\n",
      "Loss: 1.386 | Acc: 50.157% (16018/31936)\n",
      "Loss: 1.386 | Acc: 50.156% (16050/32000)\n",
      "Loss: 1.385 | Acc: 50.168% (16086/32064)\n",
      "Loss: 1.385 | Acc: 50.168% (16118/32128)\n",
      "Loss: 1.385 | Acc: 50.171% (16151/32192)\n",
      "Loss: 1.385 | Acc: 50.164% (16181/32256)\n",
      "Loss: 1.385 | Acc: 50.155% (16210/32320)\n",
      "Loss: 1.385 | Acc: 50.170% (16247/32384)\n",
      "Loss: 1.385 | Acc: 50.176% (16281/32448)\n",
      "Loss: 1.384 | Acc: 50.191% (16318/32512)\n",
      "Loss: 1.384 | Acc: 50.193% (16351/32576)\n",
      "Loss: 1.384 | Acc: 50.196% (16384/32640)\n",
      "Loss: 1.383 | Acc: 50.223% (16425/32704)\n",
      "Loss: 1.383 | Acc: 50.226% (16458/32768)\n",
      "Loss: 1.382 | Acc: 50.238% (16494/32832)\n",
      "Loss: 1.383 | Acc: 50.243% (16528/32896)\n",
      "Loss: 1.382 | Acc: 50.237% (16558/32960)\n",
      "Loss: 1.383 | Acc: 50.239% (16591/33024)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.383 | Acc: 50.230% (16620/33088)\n",
      "Loss: 1.383 | Acc: 50.241% (16656/33152)\n",
      "Loss: 1.383 | Acc: 50.247% (16690/33216)\n",
      "Loss: 1.382 | Acc: 50.255% (16725/33280)\n",
      "Loss: 1.382 | Acc: 50.276% (16764/33344)\n",
      "Loss: 1.382 | Acc: 50.290% (16801/33408)\n",
      "Loss: 1.381 | Acc: 50.308% (16839/33472)\n",
      "Loss: 1.381 | Acc: 50.310% (16872/33536)\n",
      "Loss: 1.381 | Acc: 50.330% (16911/33600)\n",
      "Loss: 1.380 | Acc: 50.345% (16948/33664)\n",
      "Loss: 1.380 | Acc: 50.362% (16986/33728)\n",
      "Loss: 1.380 | Acc: 50.358% (17017/33792)\n",
      "Loss: 1.380 | Acc: 50.366% (17052/33856)\n",
      "Loss: 1.380 | Acc: 50.360% (17082/33920)\n",
      "Loss: 1.380 | Acc: 50.365% (17116/33984)\n",
      "Loss: 1.380 | Acc: 50.376% (17152/34048)\n",
      "Loss: 1.379 | Acc: 50.393% (17190/34112)\n",
      "Loss: 1.379 | Acc: 50.395% (17223/34176)\n",
      "Loss: 1.379 | Acc: 50.409% (17260/34240)\n",
      "Loss: 1.379 | Acc: 50.394% (17287/34304)\n",
      "Loss: 1.379 | Acc: 50.384% (17316/34368)\n",
      "Loss: 1.379 | Acc: 50.395% (17352/34432)\n",
      "Loss: 1.379 | Acc: 50.406% (17388/34496)\n",
      "Loss: 1.379 | Acc: 50.405% (17420/34560)\n",
      "Loss: 1.378 | Acc: 50.419% (17457/34624)\n",
      "Loss: 1.378 | Acc: 50.430% (17493/34688)\n",
      "Loss: 1.378 | Acc: 50.429% (17525/34752)\n",
      "Loss: 1.378 | Acc: 50.425% (17556/34816)\n",
      "Loss: 1.378 | Acc: 50.444% (17595/34880)\n",
      "Loss: 1.378 | Acc: 50.432% (17623/34944)\n",
      "Loss: 1.378 | Acc: 50.431% (17655/35008)\n",
      "Loss: 1.379 | Acc: 50.419% (17683/35072)\n",
      "Loss: 1.378 | Acc: 50.418% (17715/35136)\n",
      "Loss: 1.378 | Acc: 50.406% (17743/35200)\n",
      "Loss: 1.378 | Acc: 50.423% (17781/35264)\n",
      "Loss: 1.378 | Acc: 50.419% (17812/35328)\n",
      "Loss: 1.377 | Acc: 50.418% (17844/35392)\n",
      "Loss: 1.378 | Acc: 50.415% (17875/35456)\n",
      "Loss: 1.378 | Acc: 50.428% (17912/35520)\n",
      "Loss: 1.377 | Acc: 50.438% (17948/35584)\n",
      "Loss: 1.378 | Acc: 50.426% (17976/35648)\n",
      "Loss: 1.378 | Acc: 50.426% (18008/35712)\n",
      "Loss: 1.378 | Acc: 50.442% (18046/35776)\n",
      "Loss: 1.378 | Acc: 50.452% (18082/35840)\n",
      "Loss: 1.378 | Acc: 50.423% (18104/35904)\n",
      "Loss: 1.378 | Acc: 50.420% (18135/35968)\n",
      "Loss: 1.378 | Acc: 50.425% (18169/36032)\n",
      "Loss: 1.378 | Acc: 50.432% (18204/36096)\n",
      "Loss: 1.378 | Acc: 50.442% (18240/36160)\n",
      "Loss: 1.377 | Acc: 50.469% (18282/36224)\n",
      "Loss: 1.377 | Acc: 50.474% (18316/36288)\n",
      "Loss: 1.377 | Acc: 50.490% (18354/36352)\n",
      "Loss: 1.376 | Acc: 50.500% (18390/36416)\n",
      "Loss: 1.377 | Acc: 50.482% (18416/36480)\n",
      "Loss: 1.377 | Acc: 50.471% (18444/36544)\n",
      "Loss: 1.377 | Acc: 50.453% (18470/36608)\n",
      "Loss: 1.377 | Acc: 50.455% (18503/36672)\n",
      "Loss: 1.377 | Acc: 50.463% (18538/36736)\n",
      "Loss: 1.377 | Acc: 50.476% (18575/36800)\n",
      "Loss: 1.377 | Acc: 50.488% (18612/36864)\n",
      "Loss: 1.377 | Acc: 50.493% (18646/36928)\n",
      "Loss: 1.377 | Acc: 50.503% (18682/36992)\n",
      "Loss: 1.377 | Acc: 50.499% (18713/37056)\n",
      "Loss: 1.377 | Acc: 50.496% (18744/37120)\n",
      "Loss: 1.377 | Acc: 50.489% (18774/37184)\n",
      "Loss: 1.377 | Acc: 50.478% (18802/37248)\n",
      "Loss: 1.377 | Acc: 50.480% (18835/37312)\n",
      "Loss: 1.377 | Acc: 50.490% (18871/37376)\n",
      "Loss: 1.377 | Acc: 50.491% (18904/37440)\n",
      "Loss: 1.376 | Acc: 50.499% (18939/37504)\n",
      "Loss: 1.376 | Acc: 50.511% (18976/37568)\n",
      "Loss: 1.376 | Acc: 50.516% (19010/37632)\n",
      "Loss: 1.376 | Acc: 50.528% (19047/37696)\n",
      "Loss: 1.376 | Acc: 50.522% (19077/37760)\n",
      "Loss: 1.375 | Acc: 50.542% (19117/37824)\n",
      "Loss: 1.375 | Acc: 50.554% (19154/37888)\n",
      "Loss: 1.375 | Acc: 50.582% (19197/37952)\n",
      "Loss: 1.375 | Acc: 50.547% (19216/38016)\n",
      "Loss: 1.375 | Acc: 50.546% (19248/38080)\n",
      "Loss: 1.374 | Acc: 50.561% (19286/38144)\n",
      "Loss: 1.374 | Acc: 50.565% (19320/38208)\n",
      "Loss: 1.373 | Acc: 50.585% (19360/38272)\n",
      "Loss: 1.373 | Acc: 50.587% (19393/38336)\n",
      "Loss: 1.373 | Acc: 50.602% (19431/38400)\n",
      "Loss: 1.373 | Acc: 50.616% (19469/38464)\n",
      "Loss: 1.373 | Acc: 50.607% (19498/38528)\n",
      "Loss: 1.373 | Acc: 50.612% (19532/38592)\n",
      "Loss: 1.373 | Acc: 50.618% (19567/38656)\n",
      "Loss: 1.373 | Acc: 50.610% (19596/38720)\n",
      "Loss: 1.372 | Acc: 50.611% (19629/38784)\n",
      "Loss: 1.372 | Acc: 50.615% (19663/38848)\n",
      "Loss: 1.372 | Acc: 50.609% (19693/38912)\n",
      "Loss: 1.372 | Acc: 50.616% (19728/38976)\n",
      "Loss: 1.372 | Acc: 50.628% (19765/39040)\n",
      "Loss: 1.372 | Acc: 50.629% (19798/39104)\n",
      "Loss: 1.372 | Acc: 50.628% (19830/39168)\n",
      "Loss: 1.372 | Acc: 50.630% (19863/39232)\n",
      "Loss: 1.372 | Acc: 50.639% (19899/39296)\n",
      "Loss: 1.372 | Acc: 50.658% (19939/39360)\n",
      "Loss: 1.371 | Acc: 50.672% (19977/39424)\n",
      "Loss: 1.371 | Acc: 50.681% (20013/39488)\n",
      "Loss: 1.371 | Acc: 50.675% (20043/39552)\n",
      "Loss: 1.371 | Acc: 50.689% (20081/39616)\n",
      "Loss: 1.371 | Acc: 50.685% (20112/39680)\n",
      "Loss: 1.371 | Acc: 50.697% (20149/39744)\n",
      "Loss: 1.371 | Acc: 50.703% (20184/39808)\n",
      "Loss: 1.371 | Acc: 50.707% (20218/39872)\n",
      "Loss: 1.371 | Acc: 50.699% (20247/39936)\n",
      "Loss: 1.370 | Acc: 50.712% (20285/40000)\n",
      "Loss: 1.370 | Acc: 50.719% (20320/40064)\n",
      "Loss: 1.371 | Acc: 50.705% (20347/40128)\n",
      "Loss: 1.371 | Acc: 50.704% (20379/40192)\n",
      "Loss: 1.371 | Acc: 50.696% (20408/40256)\n",
      "Loss: 1.371 | Acc: 50.685% (20436/40320)\n",
      "Loss: 1.371 | Acc: 50.688% (20470/40384)\n",
      "Loss: 1.371 | Acc: 50.687% (20502/40448)\n",
      "Loss: 1.371 | Acc: 50.694% (20537/40512)\n",
      "Loss: 1.371 | Acc: 50.680% (20564/40576)\n",
      "Loss: 1.371 | Acc: 50.664% (20590/40640)\n",
      "Loss: 1.371 | Acc: 50.673% (20626/40704)\n",
      "Loss: 1.371 | Acc: 50.677% (20660/40768)\n",
      "Loss: 1.371 | Acc: 50.671% (20690/40832)\n",
      "Loss: 1.371 | Acc: 50.672% (20723/40896)\n",
      "Loss: 1.371 | Acc: 50.671% (20755/40960)\n",
      "Loss: 1.371 | Acc: 50.673% (20788/41024)\n",
      "Loss: 1.370 | Acc: 50.677% (20822/41088)\n",
      "Loss: 1.370 | Acc: 50.668% (20851/41152)\n",
      "Loss: 1.370 | Acc: 50.677% (20887/41216)\n",
      "Loss: 1.370 | Acc: 50.673% (20918/41280)\n",
      "Loss: 1.370 | Acc: 50.675% (20951/41344)\n",
      "Loss: 1.370 | Acc: 50.662% (20978/41408)\n",
      "Loss: 1.370 | Acc: 50.663% (21011/41472)\n",
      "Loss: 1.370 | Acc: 50.686% (21053/41536)\n",
      "Loss: 1.370 | Acc: 50.680% (21083/41600)\n",
      "Loss: 1.369 | Acc: 50.691% (21120/41664)\n",
      "Loss: 1.369 | Acc: 50.678% (21147/41728)\n",
      "Loss: 1.369 | Acc: 50.672% (21177/41792)\n",
      "Loss: 1.370 | Acc: 50.659% (21204/41856)\n",
      "Loss: 1.370 | Acc: 50.661% (21237/41920)\n",
      "Loss: 1.370 | Acc: 50.655% (21267/41984)\n",
      "Loss: 1.370 | Acc: 50.666% (21304/42048)\n",
      "Loss: 1.369 | Acc: 50.674% (21340/42112)\n",
      "Loss: 1.369 | Acc: 50.666% (21369/42176)\n",
      "Loss: 1.369 | Acc: 50.675% (21405/42240)\n",
      "Loss: 1.369 | Acc: 50.676% (21438/42304)\n",
      "Loss: 1.369 | Acc: 50.666% (21466/42368)\n",
      "Loss: 1.369 | Acc: 50.672% (21501/42432)\n",
      "Loss: 1.369 | Acc: 50.678% (21536/42496)\n",
      "Loss: 1.368 | Acc: 50.703% (21579/42560)\n",
      "Loss: 1.368 | Acc: 50.711% (21615/42624)\n",
      "Loss: 1.368 | Acc: 50.703% (21644/42688)\n",
      "Loss: 1.368 | Acc: 50.706% (21678/42752)\n",
      "Loss: 1.369 | Acc: 50.694% (21705/42816)\n",
      "Loss: 1.369 | Acc: 50.688% (21735/42880)\n",
      "Loss: 1.368 | Acc: 50.703% (21774/42944)\n",
      "Loss: 1.368 | Acc: 50.705% (21807/43008)\n",
      "Loss: 1.368 | Acc: 50.722% (21847/43072)\n",
      "Loss: 1.368 | Acc: 50.723% (21880/43136)\n",
      "Loss: 1.368 | Acc: 50.727% (21914/43200)\n",
      "Loss: 1.368 | Acc: 50.735% (21950/43264)\n",
      "Loss: 1.368 | Acc: 50.725% (21978/43328)\n",
      "Loss: 1.369 | Acc: 50.726% (22011/43392)\n",
      "Loss: 1.369 | Acc: 50.729% (22045/43456)\n",
      "Loss: 1.369 | Acc: 50.724% (22075/43520)\n",
      "Loss: 1.369 | Acc: 50.718% (22105/43584)\n",
      "Loss: 1.369 | Acc: 50.701% (22130/43648)\n",
      "Loss: 1.369 | Acc: 50.693% (22159/43712)\n",
      "Loss: 1.369 | Acc: 50.701% (22195/43776)\n",
      "Loss: 1.369 | Acc: 50.716% (22234/43840)\n",
      "Loss: 1.369 | Acc: 50.711% (22264/43904)\n",
      "Loss: 1.369 | Acc: 50.716% (22299/43968)\n",
      "Loss: 1.369 | Acc: 50.706% (22327/44032)\n",
      "Loss: 1.369 | Acc: 50.683% (22349/44096)\n",
      "Loss: 1.369 | Acc: 50.686% (22383/44160)\n",
      "Loss: 1.369 | Acc: 50.683% (22414/44224)\n",
      "Loss: 1.369 | Acc: 50.682% (22446/44288)\n",
      "Loss: 1.369 | Acc: 50.679% (22477/44352)\n",
      "Loss: 1.369 | Acc: 50.687% (22513/44416)\n",
      "Loss: 1.369 | Acc: 50.695% (22549/44480)\n",
      "Loss: 1.369 | Acc: 50.698% (22583/44544)\n",
      "Loss: 1.369 | Acc: 50.697% (22615/44608)\n",
      "Loss: 1.369 | Acc: 50.705% (22651/44672)\n",
      "Loss: 1.369 | Acc: 50.709% (22685/44736)\n",
      "Loss: 1.369 | Acc: 50.725% (22725/44800)\n",
      "Loss: 1.368 | Acc: 50.740% (22764/44864)\n",
      "Loss: 1.368 | Acc: 50.730% (22792/44928)\n",
      "Loss: 1.368 | Acc: 50.742% (22830/44992)\n",
      "Loss: 1.368 | Acc: 50.741% (22862/45056)\n",
      "Loss: 1.368 | Acc: 50.742% (22895/45120)\n",
      "Loss: 1.368 | Acc: 50.752% (22932/45184)\n",
      "Loss: 1.368 | Acc: 50.743% (22960/45248)\n",
      "Loss: 1.367 | Acc: 50.761% (23001/45312)\n",
      "Loss: 1.367 | Acc: 50.763% (23034/45376)\n",
      "Loss: 1.367 | Acc: 50.757% (23064/45440)\n",
      "Loss: 1.367 | Acc: 50.758% (23097/45504)\n",
      "Loss: 1.367 | Acc: 50.770% (23135/45568)\n",
      "Loss: 1.367 | Acc: 50.767% (23166/45632)\n",
      "Loss: 1.367 | Acc: 50.755% (23193/45696)\n",
      "Loss: 1.367 | Acc: 50.752% (23224/45760)\n",
      "Loss: 1.368 | Acc: 50.735% (23249/45824)\n",
      "Loss: 1.368 | Acc: 50.745% (23286/45888)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.368 | Acc: 50.740% (23316/45952)\n",
      "Loss: 1.367 | Acc: 50.767% (23361/46016)\n",
      "Loss: 1.367 | Acc: 50.770% (23395/46080)\n",
      "Loss: 1.368 | Acc: 50.763% (23424/46144)\n",
      "Loss: 1.367 | Acc: 50.779% (23464/46208)\n",
      "Loss: 1.367 | Acc: 50.782% (23498/46272)\n",
      "Loss: 1.367 | Acc: 50.768% (23524/46336)\n",
      "Loss: 1.367 | Acc: 50.780% (23562/46400)\n",
      "Loss: 1.367 | Acc: 50.783% (23596/46464)\n",
      "Loss: 1.367 | Acc: 50.800% (23636/46528)\n",
      "Loss: 1.367 | Acc: 50.818% (23677/46592)\n",
      "Loss: 1.367 | Acc: 50.819% (23710/46656)\n",
      "Loss: 1.367 | Acc: 50.826% (23746/46720)\n",
      "Loss: 1.367 | Acc: 50.829% (23780/46784)\n",
      "Loss: 1.367 | Acc: 50.822% (23809/46848)\n",
      "Loss: 1.367 | Acc: 50.827% (23844/46912)\n",
      "Loss: 1.367 | Acc: 50.826% (23876/46976)\n",
      "Loss: 1.367 | Acc: 50.825% (23908/47040)\n",
      "Loss: 1.367 | Acc: 50.819% (23938/47104)\n",
      "Loss: 1.367 | Acc: 50.823% (23972/47168)\n",
      "Loss: 1.367 | Acc: 50.821% (24004/47232)\n",
      "Loss: 1.367 | Acc: 50.827% (24039/47296)\n",
      "Loss: 1.367 | Acc: 50.830% (24073/47360)\n",
      "Loss: 1.367 | Acc: 50.839% (24110/47424)\n",
      "Loss: 1.367 | Acc: 50.828% (24137/47488)\n",
      "Loss: 1.367 | Acc: 50.816% (24164/47552)\n",
      "Loss: 1.367 | Acc: 50.815% (24196/47616)\n",
      "Loss: 1.367 | Acc: 50.826% (24234/47680)\n",
      "Loss: 1.367 | Acc: 50.815% (24261/47744)\n",
      "Loss: 1.366 | Acc: 50.828% (24300/47808)\n",
      "Loss: 1.366 | Acc: 50.838% (24337/47872)\n",
      "Loss: 1.366 | Acc: 50.843% (24372/47936)\n",
      "Loss: 1.366 | Acc: 50.844% (24405/48000)\n",
      "Loss: 1.366 | Acc: 50.834% (24433/48064)\n",
      "Loss: 1.366 | Acc: 50.844% (24470/48128)\n",
      "Loss: 1.366 | Acc: 50.859% (24510/48192)\n",
      "Loss: 1.366 | Acc: 50.854% (24540/48256)\n",
      "Loss: 1.366 | Acc: 50.846% (24569/48320)\n",
      "Loss: 1.366 | Acc: 50.854% (24605/48384)\n",
      "Loss: 1.366 | Acc: 50.865% (24643/48448)\n",
      "Loss: 1.366 | Acc: 50.866% (24676/48512)\n",
      "Loss: 1.366 | Acc: 50.854% (24703/48576)\n",
      "Loss: 1.366 | Acc: 50.855% (24736/48640)\n",
      "Loss: 1.366 | Acc: 50.860% (24771/48704)\n",
      "Loss: 1.366 | Acc: 50.863% (24805/48768)\n",
      "Loss: 1.366 | Acc: 50.862% (24837/48832)\n",
      "Loss: 1.366 | Acc: 50.871% (24874/48896)\n",
      "Loss: 1.366 | Acc: 50.864% (24903/48960)\n",
      "Loss: 1.366 | Acc: 50.871% (24927/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 50.871428571428574\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.295 | Acc: 54.688% (35/64)\n",
      "Loss: 1.273 | Acc: 50.000% (64/128)\n",
      "Loss: 1.354 | Acc: 48.438% (93/192)\n",
      "Loss: 1.412 | Acc: 46.094% (118/256)\n",
      "Loss: 1.380 | Acc: 47.500% (152/320)\n",
      "Loss: 1.400 | Acc: 48.177% (185/384)\n",
      "Loss: 1.432 | Acc: 45.982% (206/448)\n",
      "Loss: 1.426 | Acc: 45.898% (235/512)\n",
      "Loss: 1.387 | Acc: 48.438% (279/576)\n",
      "Loss: 1.371 | Acc: 48.906% (313/640)\n",
      "Loss: 1.375 | Acc: 48.722% (343/704)\n",
      "Loss: 1.360 | Acc: 48.958% (376/768)\n",
      "Loss: 1.364 | Acc: 48.678% (405/832)\n",
      "Loss: 1.375 | Acc: 48.661% (436/896)\n",
      "Loss: 1.358 | Acc: 49.167% (472/960)\n",
      "Loss: 1.340 | Acc: 50.195% (514/1024)\n",
      "Loss: 1.330 | Acc: 50.551% (550/1088)\n",
      "Loss: 1.329 | Acc: 50.521% (582/1152)\n",
      "Loss: 1.321 | Acc: 51.316% (624/1216)\n",
      "Loss: 1.340 | Acc: 50.625% (648/1280)\n",
      "Loss: 1.335 | Acc: 50.893% (684/1344)\n",
      "Loss: 1.332 | Acc: 50.994% (718/1408)\n",
      "Loss: 1.326 | Acc: 51.155% (753/1472)\n",
      "Loss: 1.337 | Acc: 50.846% (781/1536)\n",
      "Loss: 1.342 | Acc: 50.438% (807/1600)\n",
      "Loss: 1.344 | Acc: 50.541% (841/1664)\n",
      "Loss: 1.348 | Acc: 50.579% (874/1728)\n",
      "Loss: 1.350 | Acc: 50.502% (905/1792)\n",
      "Loss: 1.350 | Acc: 50.539% (938/1856)\n",
      "Loss: 1.354 | Acc: 50.573% (971/1920)\n",
      "Loss: 1.360 | Acc: 50.454% (1001/1984)\n",
      "Loss: 1.358 | Acc: 50.586% (1036/2048)\n",
      "Loss: 1.354 | Acc: 50.710% (1071/2112)\n",
      "Loss: 1.356 | Acc: 50.597% (1101/2176)\n",
      "Loss: 1.359 | Acc: 50.312% (1127/2240)\n",
      "Loss: 1.356 | Acc: 50.304% (1159/2304)\n",
      "Loss: 1.359 | Acc: 50.084% (1186/2368)\n",
      "Loss: 1.357 | Acc: 50.247% (1222/2432)\n",
      "Loss: 1.355 | Acc: 50.521% (1261/2496)\n",
      "Loss: 1.367 | Acc: 50.352% (1289/2560)\n",
      "Loss: 1.369 | Acc: 50.114% (1315/2624)\n",
      "Loss: 1.367 | Acc: 50.112% (1347/2688)\n",
      "Loss: 1.366 | Acc: 50.218% (1382/2752)\n",
      "Loss: 1.367 | Acc: 50.213% (1414/2816)\n",
      "Loss: 1.365 | Acc: 50.208% (1446/2880)\n",
      "Loss: 1.362 | Acc: 50.476% (1486/2944)\n",
      "Loss: 1.360 | Acc: 50.598% (1522/3008)\n",
      "Loss: 1.357 | Acc: 50.553% (1553/3072)\n",
      "Loss: 1.359 | Acc: 50.574% (1586/3136)\n",
      "Loss: 1.357 | Acc: 50.656% (1621/3200)\n",
      "Loss: 1.357 | Acc: 50.674% (1654/3264)\n",
      "Loss: 1.358 | Acc: 50.571% (1683/3328)\n",
      "Loss: 1.360 | Acc: 50.560% (1715/3392)\n",
      "Loss: 1.360 | Acc: 50.608% (1749/3456)\n",
      "Loss: 1.363 | Acc: 50.455% (1776/3520)\n",
      "Loss: 1.361 | Acc: 50.558% (1812/3584)\n",
      "Loss: 1.364 | Acc: 50.384% (1838/3648)\n",
      "Loss: 1.360 | Acc: 50.404% (1871/3712)\n",
      "Loss: 1.361 | Acc: 50.344% (1901/3776)\n",
      "Loss: 1.357 | Acc: 50.495% (1939/3840)\n",
      "Loss: 1.356 | Acc: 50.487% (1971/3904)\n",
      "Loss: 1.355 | Acc: 50.605% (2008/3968)\n",
      "Loss: 1.354 | Acc: 50.719% (2045/4032)\n",
      "Loss: 1.356 | Acc: 50.781% (2080/4096)\n",
      "Loss: 1.358 | Acc: 50.769% (2112/4160)\n",
      "Loss: 1.356 | Acc: 50.923% (2151/4224)\n",
      "Loss: 1.355 | Acc: 50.956% (2185/4288)\n",
      "Loss: 1.351 | Acc: 51.103% (2224/4352)\n",
      "Loss: 1.349 | Acc: 51.223% (2262/4416)\n",
      "Loss: 1.347 | Acc: 51.362% (2301/4480)\n",
      "Loss: 1.349 | Acc: 51.298% (2331/4544)\n",
      "Loss: 1.351 | Acc: 51.215% (2360/4608)\n",
      "Loss: 1.348 | Acc: 51.327% (2398/4672)\n",
      "Loss: 1.346 | Acc: 51.394% (2434/4736)\n",
      "Loss: 1.348 | Acc: 51.396% (2467/4800)\n",
      "Loss: 1.345 | Acc: 51.501% (2505/4864)\n",
      "Loss: 1.344 | Acc: 51.583% (2542/4928)\n",
      "Loss: 1.345 | Acc: 51.542% (2573/4992)\n",
      "Loss: 1.346 | Acc: 51.444% (2601/5056)\n",
      "Loss: 1.348 | Acc: 51.465% (2635/5120)\n",
      "Loss: 1.346 | Acc: 51.562% (2673/5184)\n",
      "Loss: 1.347 | Acc: 51.410% (2698/5248)\n",
      "Loss: 1.348 | Acc: 51.393% (2730/5312)\n",
      "Loss: 1.350 | Acc: 51.395% (2763/5376)\n",
      "Loss: 1.350 | Acc: 51.305% (2791/5440)\n",
      "Loss: 1.351 | Acc: 51.399% (2829/5504)\n",
      "Loss: 1.353 | Acc: 51.419% (2863/5568)\n",
      "Loss: 1.356 | Acc: 51.403% (2895/5632)\n",
      "Loss: 1.357 | Acc: 51.440% (2930/5696)\n",
      "Loss: 1.356 | Acc: 51.424% (2962/5760)\n",
      "Loss: 1.355 | Acc: 51.459% (2997/5824)\n",
      "Loss: 1.357 | Acc: 51.444% (3029/5888)\n",
      "Loss: 1.358 | Acc: 51.478% (3064/5952)\n",
      "Loss: 1.358 | Acc: 51.463% (3096/6016)\n",
      "Loss: 1.359 | Acc: 51.332% (3121/6080)\n",
      "Loss: 1.358 | Acc: 51.416% (3159/6144)\n",
      "Loss: 1.360 | Acc: 51.337% (3187/6208)\n",
      "Loss: 1.362 | Acc: 51.260% (3215/6272)\n",
      "Loss: 1.360 | Acc: 51.294% (3250/6336)\n",
      "Loss: 1.361 | Acc: 51.203% (3277/6400)\n",
      "Loss: 1.364 | Acc: 51.176% (3308/6464)\n",
      "Loss: 1.364 | Acc: 51.180% (3341/6528)\n",
      "Loss: 1.364 | Acc: 51.153% (3372/6592)\n",
      "Loss: 1.365 | Acc: 51.097% (3401/6656)\n",
      "Loss: 1.365 | Acc: 51.086% (3433/6720)\n",
      "Loss: 1.365 | Acc: 51.106% (3467/6784)\n",
      "Loss: 1.363 | Acc: 51.197% (3506/6848)\n",
      "Loss: 1.366 | Acc: 51.128% (3534/6912)\n",
      "Loss: 1.367 | Acc: 51.147% (3568/6976)\n",
      "Loss: 1.369 | Acc: 51.122% (3599/7040)\n",
      "Loss: 1.368 | Acc: 51.182% (3636/7104)\n",
      "Loss: 1.369 | Acc: 51.242% (3673/7168)\n",
      "Loss: 1.370 | Acc: 51.134% (3698/7232)\n",
      "Loss: 1.368 | Acc: 51.138% (3731/7296)\n",
      "Loss: 1.366 | Acc: 51.196% (3768/7360)\n",
      "Loss: 1.366 | Acc: 51.145% (3797/7424)\n",
      "Loss: 1.364 | Acc: 51.122% (3828/7488)\n",
      "Loss: 1.364 | Acc: 51.126% (3861/7552)\n",
      "Loss: 1.364 | Acc: 51.116% (3893/7616)\n",
      "Loss: 1.363 | Acc: 51.159% (3929/7680)\n",
      "Loss: 1.363 | Acc: 51.175% (3963/7744)\n",
      "Loss: 1.362 | Acc: 51.204% (3998/7808)\n",
      "Loss: 1.364 | Acc: 51.181% (4029/7872)\n",
      "Loss: 1.363 | Acc: 51.159% (4060/7936)\n",
      "Loss: 1.363 | Acc: 51.138% (4091/8000)\n",
      "Loss: 1.363 | Acc: 51.153% (4125/8064)\n",
      "Loss: 1.364 | Acc: 51.107% (4154/8128)\n",
      "Loss: 1.363 | Acc: 51.147% (4190/8192)\n",
      "Loss: 1.365 | Acc: 51.102% (4219/8256)\n",
      "Loss: 1.366 | Acc: 50.998% (4243/8320)\n",
      "Loss: 1.366 | Acc: 51.002% (4276/8384)\n",
      "Loss: 1.365 | Acc: 51.077% (4315/8448)\n",
      "Loss: 1.365 | Acc: 51.057% (4346/8512)\n",
      "Loss: 1.366 | Acc: 51.038% (4377/8576)\n",
      "Loss: 1.365 | Acc: 51.065% (4412/8640)\n",
      "Loss: 1.366 | Acc: 50.988% (4438/8704)\n",
      "Loss: 1.368 | Acc: 50.924% (4465/8768)\n",
      "Loss: 1.366 | Acc: 50.996% (4504/8832)\n",
      "Loss: 1.366 | Acc: 51.000% (4537/8896)\n",
      "Loss: 1.365 | Acc: 51.027% (4572/8960)\n",
      "Loss: 1.365 | Acc: 51.053% (4607/9024)\n",
      "Loss: 1.366 | Acc: 51.012% (4636/9088)\n",
      "Loss: 1.366 | Acc: 51.027% (4670/9152)\n",
      "Loss: 1.364 | Acc: 51.118% (4711/9216)\n",
      "Loss: 1.363 | Acc: 51.121% (4744/9280)\n",
      "Loss: 1.363 | Acc: 51.092% (4774/9344)\n",
      "Loss: 1.364 | Acc: 51.095% (4807/9408)\n",
      "Loss: 1.365 | Acc: 51.056% (4836/9472)\n",
      "Loss: 1.364 | Acc: 51.091% (4872/9536)\n",
      "Loss: 1.362 | Acc: 51.188% (4914/9600)\n",
      "Loss: 1.362 | Acc: 51.200% (4948/9664)\n",
      "Loss: 1.361 | Acc: 51.223% (4983/9728)\n",
      "Loss: 1.363 | Acc: 51.195% (5013/9792)\n",
      "Loss: 1.362 | Acc: 51.218% (5048/9856)\n",
      "Loss: 1.364 | Acc: 51.169% (5076/9920)\n",
      "Loss: 1.365 | Acc: 51.102% (5102/9984)\n",
      "Loss: 1.364 | Acc: 51.080% (5108/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 51.08\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.138 | Acc: 60.938% (39/64)\n",
      "Loss: 1.161 | Acc: 60.938% (78/128)\n",
      "Loss: 1.122 | Acc: 61.458% (118/192)\n",
      "Loss: 1.151 | Acc: 58.984% (151/256)\n",
      "Loss: 1.143 | Acc: 58.125% (186/320)\n",
      "Loss: 1.169 | Acc: 57.292% (220/384)\n",
      "Loss: 1.170 | Acc: 58.259% (261/448)\n",
      "Loss: 1.195 | Acc: 58.008% (297/512)\n",
      "Loss: 1.187 | Acc: 58.333% (336/576)\n",
      "Loss: 1.191 | Acc: 58.125% (372/640)\n",
      "Loss: 1.186 | Acc: 58.097% (409/704)\n",
      "Loss: 1.194 | Acc: 57.943% (445/768)\n",
      "Loss: 1.219 | Acc: 57.212% (476/832)\n",
      "Loss: 1.236 | Acc: 56.808% (509/896)\n",
      "Loss: 1.241 | Acc: 56.354% (541/960)\n",
      "Loss: 1.247 | Acc: 56.055% (574/1024)\n",
      "Loss: 1.242 | Acc: 56.250% (612/1088)\n",
      "Loss: 1.242 | Acc: 56.337% (649/1152)\n",
      "Loss: 1.238 | Acc: 56.497% (687/1216)\n",
      "Loss: 1.225 | Acc: 57.109% (731/1280)\n",
      "Loss: 1.220 | Acc: 56.994% (766/1344)\n",
      "Loss: 1.221 | Acc: 57.031% (803/1408)\n",
      "Loss: 1.222 | Acc: 56.522% (832/1472)\n",
      "Loss: 1.229 | Acc: 56.120% (862/1536)\n",
      "Loss: 1.233 | Acc: 55.812% (893/1600)\n",
      "Loss: 1.231 | Acc: 55.950% (931/1664)\n",
      "Loss: 1.232 | Acc: 55.671% (962/1728)\n",
      "Loss: 1.230 | Acc: 55.636% (997/1792)\n",
      "Loss: 1.223 | Acc: 55.873% (1037/1856)\n",
      "Loss: 1.224 | Acc: 55.781% (1071/1920)\n",
      "Loss: 1.230 | Acc: 55.796% (1107/1984)\n",
      "Loss: 1.221 | Acc: 56.250% (1152/2048)\n",
      "Loss: 1.227 | Acc: 56.250% (1188/2112)\n",
      "Loss: 1.226 | Acc: 56.388% (1227/2176)\n",
      "Loss: 1.228 | Acc: 56.295% (1261/2240)\n",
      "Loss: 1.233 | Acc: 56.120% (1293/2304)\n",
      "Loss: 1.237 | Acc: 56.039% (1327/2368)\n",
      "Loss: 1.235 | Acc: 56.044% (1363/2432)\n",
      "Loss: 1.238 | Acc: 55.970% (1397/2496)\n",
      "Loss: 1.245 | Acc: 55.586% (1423/2560)\n",
      "Loss: 1.247 | Acc: 55.259% (1450/2624)\n",
      "Loss: 1.251 | Acc: 55.097% (1481/2688)\n",
      "Loss: 1.254 | Acc: 54.906% (1511/2752)\n",
      "Loss: 1.255 | Acc: 54.936% (1547/2816)\n",
      "Loss: 1.255 | Acc: 55.069% (1586/2880)\n",
      "Loss: 1.252 | Acc: 55.265% (1627/2944)\n",
      "Loss: 1.258 | Acc: 54.854% (1650/3008)\n",
      "Loss: 1.257 | Acc: 54.818% (1684/3072)\n",
      "Loss: 1.258 | Acc: 54.783% (1718/3136)\n",
      "Loss: 1.258 | Acc: 54.750% (1752/3200)\n",
      "Loss: 1.258 | Acc: 54.718% (1786/3264)\n",
      "Loss: 1.261 | Acc: 54.868% (1826/3328)\n",
      "Loss: 1.259 | Acc: 55.012% (1866/3392)\n",
      "Loss: 1.259 | Acc: 55.150% (1906/3456)\n",
      "Loss: 1.261 | Acc: 55.114% (1940/3520)\n",
      "Loss: 1.262 | Acc: 55.162% (1977/3584)\n",
      "Loss: 1.262 | Acc: 55.071% (2009/3648)\n",
      "Loss: 1.266 | Acc: 54.984% (2041/3712)\n",
      "Loss: 1.268 | Acc: 55.032% (2078/3776)\n",
      "Loss: 1.267 | Acc: 55.078% (2115/3840)\n",
      "Loss: 1.267 | Acc: 54.867% (2142/3904)\n",
      "Loss: 1.264 | Acc: 54.889% (2178/3968)\n",
      "Loss: 1.265 | Acc: 54.787% (2209/4032)\n",
      "Loss: 1.271 | Acc: 54.565% (2235/4096)\n",
      "Loss: 1.271 | Acc: 54.615% (2272/4160)\n",
      "Loss: 1.275 | Acc: 54.474% (2301/4224)\n",
      "Loss: 1.275 | Acc: 54.454% (2335/4288)\n",
      "Loss: 1.277 | Acc: 54.297% (2363/4352)\n",
      "Loss: 1.277 | Acc: 54.370% (2401/4416)\n",
      "Loss: 1.278 | Acc: 54.308% (2433/4480)\n",
      "Loss: 1.276 | Acc: 54.379% (2471/4544)\n",
      "Loss: 1.277 | Acc: 54.362% (2505/4608)\n",
      "Loss: 1.276 | Acc: 54.409% (2542/4672)\n",
      "Loss: 1.275 | Acc: 54.455% (2579/4736)\n",
      "Loss: 1.271 | Acc: 54.625% (2622/4800)\n",
      "Loss: 1.270 | Acc: 54.708% (2661/4864)\n",
      "Loss: 1.272 | Acc: 54.606% (2691/4928)\n",
      "Loss: 1.274 | Acc: 54.507% (2721/4992)\n",
      "Loss: 1.274 | Acc: 54.509% (2756/5056)\n",
      "Loss: 1.274 | Acc: 54.551% (2793/5120)\n",
      "Loss: 1.272 | Acc: 54.591% (2830/5184)\n",
      "Loss: 1.273 | Acc: 54.554% (2863/5248)\n",
      "Loss: 1.272 | Acc: 54.593% (2900/5312)\n",
      "Loss: 1.270 | Acc: 54.669% (2939/5376)\n",
      "Loss: 1.268 | Acc: 54.761% (2979/5440)\n",
      "Loss: 1.267 | Acc: 54.778% (3015/5504)\n",
      "Loss: 1.265 | Acc: 54.921% (3058/5568)\n",
      "Loss: 1.264 | Acc: 55.060% (3101/5632)\n",
      "Loss: 1.264 | Acc: 55.074% (3137/5696)\n",
      "Loss: 1.263 | Acc: 55.174% (3178/5760)\n",
      "Loss: 1.264 | Acc: 55.100% (3209/5824)\n",
      "Loss: 1.262 | Acc: 55.095% (3244/5888)\n",
      "Loss: 1.261 | Acc: 55.108% (3280/5952)\n",
      "Loss: 1.260 | Acc: 55.170% (3319/6016)\n",
      "Loss: 1.258 | Acc: 55.230% (3358/6080)\n",
      "Loss: 1.257 | Acc: 55.208% (3392/6144)\n",
      "Loss: 1.256 | Acc: 55.219% (3428/6208)\n",
      "Loss: 1.255 | Acc: 55.246% (3465/6272)\n",
      "Loss: 1.258 | Acc: 55.193% (3497/6336)\n",
      "Loss: 1.257 | Acc: 55.234% (3535/6400)\n",
      "Loss: 1.257 | Acc: 55.244% (3571/6464)\n",
      "Loss: 1.258 | Acc: 55.147% (3600/6528)\n",
      "Loss: 1.260 | Acc: 55.082% (3631/6592)\n",
      "Loss: 1.260 | Acc: 55.063% (3665/6656)\n",
      "Loss: 1.259 | Acc: 55.089% (3702/6720)\n",
      "Loss: 1.260 | Acc: 55.115% (3739/6784)\n",
      "Loss: 1.259 | Acc: 55.111% (3774/6848)\n",
      "Loss: 1.260 | Acc: 55.136% (3811/6912)\n",
      "Loss: 1.259 | Acc: 55.118% (3845/6976)\n",
      "Loss: 1.259 | Acc: 55.085% (3878/7040)\n",
      "Loss: 1.260 | Acc: 55.053% (3911/7104)\n",
      "Loss: 1.259 | Acc: 55.064% (3947/7168)\n",
      "Loss: 1.259 | Acc: 55.006% (3978/7232)\n",
      "Loss: 1.258 | Acc: 55.030% (4015/7296)\n",
      "Loss: 1.258 | Acc: 55.054% (4052/7360)\n",
      "Loss: 1.257 | Acc: 55.065% (4088/7424)\n",
      "Loss: 1.256 | Acc: 55.101% (4126/7488)\n",
      "Loss: 1.257 | Acc: 55.085% (4160/7552)\n",
      "Loss: 1.259 | Acc: 55.029% (4191/7616)\n",
      "Loss: 1.259 | Acc: 55.078% (4230/7680)\n",
      "Loss: 1.261 | Acc: 55.023% (4261/7744)\n",
      "Loss: 1.260 | Acc: 55.085% (4301/7808)\n",
      "Loss: 1.261 | Acc: 55.081% (4336/7872)\n",
      "Loss: 1.261 | Acc: 55.015% (4366/7936)\n",
      "Loss: 1.261 | Acc: 55.050% (4404/8000)\n",
      "Loss: 1.260 | Acc: 55.060% (4440/8064)\n",
      "Loss: 1.262 | Acc: 54.983% (4469/8128)\n",
      "Loss: 1.262 | Acc: 54.956% (4502/8192)\n",
      "Loss: 1.262 | Acc: 54.881% (4531/8256)\n",
      "Loss: 1.261 | Acc: 54.892% (4567/8320)\n",
      "Loss: 1.260 | Acc: 54.902% (4603/8384)\n",
      "Loss: 1.261 | Acc: 54.877% (4636/8448)\n",
      "Loss: 1.260 | Acc: 54.875% (4671/8512)\n",
      "Loss: 1.260 | Acc: 54.862% (4705/8576)\n",
      "Loss: 1.259 | Acc: 54.861% (4740/8640)\n",
      "Loss: 1.257 | Acc: 54.940% (4782/8704)\n",
      "Loss: 1.258 | Acc: 54.950% (4818/8768)\n",
      "Loss: 1.257 | Acc: 55.005% (4858/8832)\n",
      "Loss: 1.258 | Acc: 54.946% (4888/8896)\n",
      "Loss: 1.258 | Acc: 54.922% (4921/8960)\n",
      "Loss: 1.258 | Acc: 54.898% (4954/9024)\n",
      "Loss: 1.258 | Acc: 54.897% (4989/9088)\n",
      "Loss: 1.260 | Acc: 54.862% (5021/9152)\n",
      "Loss: 1.260 | Acc: 54.829% (5053/9216)\n",
      "Loss: 1.261 | Acc: 54.784% (5084/9280)\n",
      "Loss: 1.262 | Acc: 54.720% (5113/9344)\n",
      "Loss: 1.265 | Acc: 54.581% (5135/9408)\n",
      "Loss: 1.266 | Acc: 54.582% (5170/9472)\n",
      "Loss: 1.267 | Acc: 54.520% (5199/9536)\n",
      "Loss: 1.267 | Acc: 54.510% (5233/9600)\n",
      "Loss: 1.267 | Acc: 54.512% (5268/9664)\n",
      "Loss: 1.267 | Acc: 54.502% (5302/9728)\n",
      "Loss: 1.267 | Acc: 54.524% (5339/9792)\n",
      "Loss: 1.267 | Acc: 54.525% (5374/9856)\n",
      "Loss: 1.267 | Acc: 54.567% (5413/9920)\n",
      "Loss: 1.267 | Acc: 54.617% (5453/9984)\n",
      "Loss: 1.267 | Acc: 54.648% (5491/10048)\n",
      "Loss: 1.269 | Acc: 54.648% (5526/10112)\n",
      "Loss: 1.269 | Acc: 54.619% (5558/10176)\n",
      "Loss: 1.268 | Acc: 54.658% (5597/10240)\n",
      "Loss: 1.267 | Acc: 54.610% (5627/10304)\n",
      "Loss: 1.268 | Acc: 54.552% (5656/10368)\n",
      "Loss: 1.268 | Acc: 54.525% (5688/10432)\n",
      "Loss: 1.267 | Acc: 54.545% (5725/10496)\n",
      "Loss: 1.267 | Acc: 54.517% (5757/10560)\n",
      "Loss: 1.266 | Acc: 54.546% (5795/10624)\n",
      "Loss: 1.265 | Acc: 54.585% (5834/10688)\n",
      "Loss: 1.267 | Acc: 54.557% (5866/10752)\n",
      "Loss: 1.267 | Acc: 54.503% (5895/10816)\n",
      "Loss: 1.268 | Acc: 54.439% (5923/10880)\n",
      "Loss: 1.269 | Acc: 54.423% (5956/10944)\n",
      "Loss: 1.268 | Acc: 54.451% (5994/11008)\n",
      "Loss: 1.269 | Acc: 54.426% (6026/11072)\n",
      "Loss: 1.269 | Acc: 54.400% (6058/11136)\n",
      "Loss: 1.269 | Acc: 54.402% (6093/11200)\n",
      "Loss: 1.268 | Acc: 54.403% (6128/11264)\n",
      "Loss: 1.269 | Acc: 54.423% (6165/11328)\n",
      "Loss: 1.268 | Acc: 54.468% (6205/11392)\n",
      "Loss: 1.269 | Acc: 54.469% (6240/11456)\n",
      "Loss: 1.270 | Acc: 54.453% (6273/11520)\n",
      "Loss: 1.269 | Acc: 54.463% (6309/11584)\n",
      "Loss: 1.271 | Acc: 54.396% (6336/11648)\n",
      "Loss: 1.270 | Acc: 54.406% (6372/11712)\n",
      "Loss: 1.271 | Acc: 54.407% (6407/11776)\n",
      "Loss: 1.270 | Acc: 54.392% (6440/11840)\n",
      "Loss: 1.270 | Acc: 54.402% (6476/11904)\n",
      "Loss: 1.269 | Acc: 54.437% (6515/11968)\n",
      "Loss: 1.271 | Acc: 54.405% (6546/12032)\n",
      "Loss: 1.271 | Acc: 54.390% (6579/12096)\n",
      "Loss: 1.271 | Acc: 54.350% (6609/12160)\n",
      "Loss: 1.272 | Acc: 54.344% (6643/12224)\n",
      "Loss: 1.271 | Acc: 54.370% (6681/12288)\n",
      "Loss: 1.271 | Acc: 54.364% (6715/12352)\n",
      "Loss: 1.271 | Acc: 54.365% (6750/12416)\n",
      "Loss: 1.271 | Acc: 54.383% (6787/12480)\n",
      "Loss: 1.271 | Acc: 54.361% (6819/12544)\n",
      "Loss: 1.271 | Acc: 54.346% (6852/12608)\n",
      "Loss: 1.270 | Acc: 54.372% (6890/12672)\n",
      "Loss: 1.271 | Acc: 54.373% (6925/12736)\n",
      "Loss: 1.271 | Acc: 54.383% (6961/12800)\n",
      "Loss: 1.271 | Acc: 54.415% (7000/12864)\n",
      "Loss: 1.271 | Acc: 54.355% (7027/12928)\n",
      "Loss: 1.272 | Acc: 54.341% (7060/12992)\n",
      "Loss: 1.272 | Acc: 54.297% (7089/13056)\n",
      "Loss: 1.272 | Acc: 54.306% (7125/13120)\n",
      "Loss: 1.272 | Acc: 54.339% (7164/13184)\n",
      "Loss: 1.272 | Acc: 54.318% (7196/13248)\n",
      "Loss: 1.272 | Acc: 54.342% (7234/13312)\n",
      "Loss: 1.272 | Acc: 54.388% (7275/13376)\n",
      "Loss: 1.272 | Acc: 54.375% (7308/13440)\n",
      "Loss: 1.272 | Acc: 54.421% (7349/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.272 | Acc: 54.393% (7380/13568)\n",
      "Loss: 1.272 | Acc: 54.401% (7416/13632)\n",
      "Loss: 1.272 | Acc: 54.381% (7448/13696)\n",
      "Loss: 1.271 | Acc: 54.404% (7486/13760)\n",
      "Loss: 1.271 | Acc: 54.405% (7521/13824)\n",
      "Loss: 1.272 | Acc: 54.385% (7553/13888)\n",
      "Loss: 1.272 | Acc: 54.379% (7587/13952)\n",
      "Loss: 1.272 | Acc: 54.374% (7621/14016)\n",
      "Loss: 1.272 | Acc: 54.389% (7658/14080)\n",
      "Loss: 1.272 | Acc: 54.355% (7688/14144)\n",
      "Loss: 1.273 | Acc: 54.307% (7716/14208)\n",
      "Loss: 1.273 | Acc: 54.288% (7748/14272)\n",
      "Loss: 1.274 | Acc: 54.283% (7782/14336)\n",
      "Loss: 1.273 | Acc: 54.299% (7819/14400)\n",
      "Loss: 1.273 | Acc: 54.252% (7847/14464)\n",
      "Loss: 1.275 | Acc: 54.240% (7880/14528)\n",
      "Loss: 1.274 | Acc: 54.249% (7916/14592)\n",
      "Loss: 1.273 | Acc: 54.244% (7950/14656)\n",
      "Loss: 1.274 | Acc: 54.192% (7977/14720)\n",
      "Loss: 1.273 | Acc: 54.241% (8019/14784)\n",
      "Loss: 1.273 | Acc: 54.277% (8059/14848)\n",
      "Loss: 1.273 | Acc: 54.265% (8092/14912)\n",
      "Loss: 1.273 | Acc: 54.240% (8123/14976)\n",
      "Loss: 1.273 | Acc: 54.262% (8161/15040)\n",
      "Loss: 1.274 | Acc: 54.264% (8196/15104)\n",
      "Loss: 1.273 | Acc: 54.285% (8234/15168)\n",
      "Loss: 1.274 | Acc: 54.267% (8266/15232)\n",
      "Loss: 1.274 | Acc: 54.315% (8308/15296)\n",
      "Loss: 1.273 | Acc: 54.336% (8346/15360)\n",
      "Loss: 1.273 | Acc: 54.324% (8379/15424)\n",
      "Loss: 1.274 | Acc: 54.307% (8411/15488)\n",
      "Loss: 1.275 | Acc: 54.244% (8436/15552)\n",
      "Loss: 1.275 | Acc: 54.265% (8474/15616)\n",
      "Loss: 1.275 | Acc: 54.254% (8507/15680)\n",
      "Loss: 1.276 | Acc: 54.230% (8538/15744)\n",
      "Loss: 1.276 | Acc: 54.251% (8576/15808)\n",
      "Loss: 1.275 | Acc: 54.259% (8612/15872)\n",
      "Loss: 1.276 | Acc: 54.255% (8646/15936)\n",
      "Loss: 1.276 | Acc: 54.250% (8680/16000)\n",
      "Loss: 1.276 | Acc: 54.264% (8717/16064)\n",
      "Loss: 1.276 | Acc: 54.260% (8751/16128)\n",
      "Loss: 1.276 | Acc: 54.280% (8789/16192)\n",
      "Loss: 1.275 | Acc: 54.263% (8821/16256)\n",
      "Loss: 1.275 | Acc: 54.246% (8853/16320)\n",
      "Loss: 1.275 | Acc: 54.230% (8885/16384)\n",
      "Loss: 1.275 | Acc: 54.250% (8923/16448)\n",
      "Loss: 1.274 | Acc: 54.276% (8962/16512)\n",
      "Loss: 1.274 | Acc: 54.289% (8999/16576)\n",
      "Loss: 1.275 | Acc: 54.291% (9034/16640)\n",
      "Loss: 1.275 | Acc: 54.274% (9066/16704)\n",
      "Loss: 1.275 | Acc: 54.276% (9101/16768)\n",
      "Loss: 1.275 | Acc: 54.266% (9134/16832)\n",
      "Loss: 1.276 | Acc: 54.285% (9172/16896)\n",
      "Loss: 1.275 | Acc: 54.316% (9212/16960)\n",
      "Loss: 1.276 | Acc: 54.276% (9240/17024)\n",
      "Loss: 1.275 | Acc: 54.272% (9274/17088)\n",
      "Loss: 1.275 | Acc: 54.297% (9313/17152)\n",
      "Loss: 1.275 | Acc: 54.293% (9347/17216)\n",
      "Loss: 1.275 | Acc: 54.288% (9381/17280)\n",
      "Loss: 1.275 | Acc: 54.278% (9414/17344)\n",
      "Loss: 1.276 | Acc: 54.257% (9445/17408)\n",
      "Loss: 1.275 | Acc: 54.310% (9489/17472)\n",
      "Loss: 1.275 | Acc: 54.305% (9523/17536)\n",
      "Loss: 1.275 | Acc: 54.301% (9557/17600)\n",
      "Loss: 1.275 | Acc: 54.286% (9589/17664)\n",
      "Loss: 1.276 | Acc: 54.276% (9622/17728)\n",
      "Loss: 1.277 | Acc: 54.243% (9651/17792)\n",
      "Loss: 1.277 | Acc: 54.228% (9683/17856)\n",
      "Loss: 1.277 | Acc: 54.219% (9716/17920)\n",
      "Loss: 1.276 | Acc: 54.243% (9755/17984)\n",
      "Loss: 1.276 | Acc: 54.239% (9789/18048)\n",
      "Loss: 1.277 | Acc: 54.240% (9824/18112)\n",
      "Loss: 1.276 | Acc: 54.242% (9859/18176)\n",
      "Loss: 1.276 | Acc: 54.232% (9892/18240)\n",
      "Loss: 1.275 | Acc: 54.229% (9926/18304)\n",
      "Loss: 1.275 | Acc: 54.241% (9963/18368)\n",
      "Loss: 1.275 | Acc: 54.259% (10001/18432)\n",
      "Loss: 1.276 | Acc: 54.239% (10032/18496)\n",
      "Loss: 1.275 | Acc: 54.235% (10066/18560)\n",
      "Loss: 1.275 | Acc: 54.220% (10098/18624)\n",
      "Loss: 1.275 | Acc: 54.217% (10132/18688)\n",
      "Loss: 1.275 | Acc: 54.234% (10170/18752)\n",
      "Loss: 1.275 | Acc: 54.230% (10204/18816)\n",
      "Loss: 1.275 | Acc: 54.253% (10243/18880)\n",
      "Loss: 1.275 | Acc: 54.249% (10277/18944)\n",
      "Loss: 1.274 | Acc: 54.267% (10315/19008)\n",
      "Loss: 1.275 | Acc: 54.210% (10339/19072)\n",
      "Loss: 1.276 | Acc: 54.202% (10372/19136)\n",
      "Loss: 1.275 | Acc: 54.219% (10410/19200)\n",
      "Loss: 1.275 | Acc: 54.179% (10437/19264)\n",
      "Loss: 1.275 | Acc: 54.201% (10476/19328)\n",
      "Loss: 1.274 | Acc: 54.223% (10515/19392)\n",
      "Loss: 1.275 | Acc: 54.225% (10550/19456)\n",
      "Loss: 1.274 | Acc: 54.232% (10586/19520)\n",
      "Loss: 1.275 | Acc: 54.248% (10624/19584)\n",
      "Loss: 1.275 | Acc: 54.250% (10659/19648)\n",
      "Loss: 1.275 | Acc: 54.246% (10693/19712)\n",
      "Loss: 1.275 | Acc: 54.268% (10732/19776)\n",
      "Loss: 1.275 | Acc: 54.274% (10768/19840)\n",
      "Loss: 1.275 | Acc: 54.276% (10803/19904)\n",
      "Loss: 1.275 | Acc: 54.272% (10837/19968)\n",
      "Loss: 1.275 | Acc: 54.268% (10871/20032)\n",
      "Loss: 1.274 | Acc: 54.289% (10910/20096)\n",
      "Loss: 1.274 | Acc: 54.301% (10947/20160)\n",
      "Loss: 1.274 | Acc: 54.317% (10985/20224)\n",
      "Loss: 1.274 | Acc: 54.328% (11022/20288)\n",
      "Loss: 1.274 | Acc: 54.324% (11056/20352)\n",
      "Loss: 1.274 | Acc: 54.301% (11086/20416)\n",
      "Loss: 1.274 | Acc: 54.321% (11125/20480)\n",
      "Loss: 1.273 | Acc: 54.337% (11163/20544)\n",
      "Loss: 1.274 | Acc: 54.333% (11197/20608)\n",
      "Loss: 1.274 | Acc: 54.305% (11226/20672)\n",
      "Loss: 1.275 | Acc: 54.268% (11253/20736)\n",
      "Loss: 1.275 | Acc: 54.245% (11283/20800)\n",
      "Loss: 1.275 | Acc: 54.256% (11320/20864)\n",
      "Loss: 1.275 | Acc: 54.277% (11359/20928)\n",
      "Loss: 1.276 | Acc: 54.273% (11393/20992)\n",
      "Loss: 1.276 | Acc: 54.270% (11427/21056)\n",
      "Loss: 1.276 | Acc: 54.242% (11456/21120)\n",
      "Loss: 1.277 | Acc: 54.215% (11485/21184)\n",
      "Loss: 1.277 | Acc: 54.189% (11514/21248)\n",
      "Loss: 1.277 | Acc: 54.190% (11549/21312)\n",
      "Loss: 1.277 | Acc: 54.182% (11582/21376)\n",
      "Loss: 1.278 | Acc: 54.179% (11616/21440)\n",
      "Loss: 1.278 | Acc: 54.162% (11647/21504)\n",
      "Loss: 1.278 | Acc: 54.150% (11679/21568)\n",
      "Loss: 1.278 | Acc: 54.142% (11712/21632)\n",
      "Loss: 1.278 | Acc: 54.144% (11747/21696)\n",
      "Loss: 1.277 | Acc: 54.168% (11787/21760)\n",
      "Loss: 1.277 | Acc: 54.170% (11822/21824)\n",
      "Loss: 1.277 | Acc: 54.185% (11860/21888)\n",
      "Loss: 1.277 | Acc: 54.168% (11891/21952)\n",
      "Loss: 1.276 | Acc: 54.197% (11932/22016)\n",
      "Loss: 1.276 | Acc: 54.203% (11968/22080)\n",
      "Loss: 1.277 | Acc: 54.200% (12002/22144)\n",
      "Loss: 1.276 | Acc: 54.219% (12041/22208)\n",
      "Loss: 1.276 | Acc: 54.243% (12081/22272)\n",
      "Loss: 1.275 | Acc: 54.253% (12118/22336)\n",
      "Loss: 1.276 | Acc: 54.228% (12147/22400)\n",
      "Loss: 1.276 | Acc: 54.233% (12183/22464)\n",
      "Loss: 1.276 | Acc: 54.244% (12220/22528)\n",
      "Loss: 1.276 | Acc: 54.249% (12256/22592)\n",
      "Loss: 1.276 | Acc: 54.246% (12290/22656)\n",
      "Loss: 1.276 | Acc: 54.243% (12324/22720)\n",
      "Loss: 1.276 | Acc: 54.235% (12357/22784)\n",
      "Loss: 1.276 | Acc: 54.237% (12392/22848)\n",
      "Loss: 1.276 | Acc: 54.251% (12430/22912)\n",
      "Loss: 1.276 | Acc: 54.265% (12468/22976)\n",
      "Loss: 1.275 | Acc: 54.280% (12506/23040)\n",
      "Loss: 1.275 | Acc: 54.289% (12543/23104)\n",
      "Loss: 1.275 | Acc: 54.282% (12576/23168)\n",
      "Loss: 1.275 | Acc: 54.266% (12607/23232)\n",
      "Loss: 1.275 | Acc: 54.271% (12643/23296)\n",
      "Loss: 1.275 | Acc: 54.264% (12676/23360)\n",
      "Loss: 1.275 | Acc: 54.256% (12709/23424)\n",
      "Loss: 1.275 | Acc: 54.270% (12747/23488)\n",
      "Loss: 1.275 | Acc: 54.263% (12780/23552)\n",
      "Loss: 1.275 | Acc: 54.268% (12816/23616)\n",
      "Loss: 1.275 | Acc: 54.278% (12853/23680)\n",
      "Loss: 1.275 | Acc: 54.258% (12883/23744)\n",
      "Loss: 1.275 | Acc: 54.288% (12925/23808)\n",
      "Loss: 1.275 | Acc: 54.285% (12959/23872)\n",
      "Loss: 1.275 | Acc: 54.270% (12990/23936)\n",
      "Loss: 1.275 | Acc: 54.279% (13027/24000)\n",
      "Loss: 1.275 | Acc: 54.272% (13060/24064)\n",
      "Loss: 1.275 | Acc: 54.302% (13102/24128)\n",
      "Loss: 1.275 | Acc: 54.307% (13138/24192)\n",
      "Loss: 1.276 | Acc: 54.292% (13169/24256)\n",
      "Loss: 1.276 | Acc: 54.297% (13205/24320)\n",
      "Loss: 1.275 | Acc: 54.310% (13243/24384)\n",
      "Loss: 1.275 | Acc: 54.328% (13282/24448)\n",
      "Loss: 1.275 | Acc: 54.341% (13320/24512)\n",
      "Loss: 1.274 | Acc: 54.350% (13357/24576)\n",
      "Loss: 1.274 | Acc: 54.379% (13399/24640)\n",
      "Loss: 1.274 | Acc: 54.360% (13429/24704)\n",
      "Loss: 1.274 | Acc: 54.369% (13466/24768)\n",
      "Loss: 1.274 | Acc: 54.369% (13501/24832)\n",
      "Loss: 1.274 | Acc: 54.334% (13527/24896)\n",
      "Loss: 1.274 | Acc: 54.307% (13555/24960)\n",
      "Loss: 1.275 | Acc: 54.296% (13587/25024)\n",
      "Loss: 1.275 | Acc: 54.297% (13622/25088)\n",
      "Loss: 1.276 | Acc: 54.286% (13654/25152)\n",
      "Loss: 1.276 | Acc: 54.291% (13690/25216)\n",
      "Loss: 1.275 | Acc: 54.296% (13726/25280)\n",
      "Loss: 1.275 | Acc: 54.293% (13760/25344)\n",
      "Loss: 1.275 | Acc: 54.302% (13797/25408)\n",
      "Loss: 1.275 | Acc: 54.291% (13829/25472)\n",
      "Loss: 1.275 | Acc: 54.292% (13864/25536)\n",
      "Loss: 1.275 | Acc: 54.297% (13900/25600)\n",
      "Loss: 1.275 | Acc: 54.306% (13937/25664)\n",
      "Loss: 1.275 | Acc: 54.299% (13970/25728)\n",
      "Loss: 1.275 | Acc: 54.296% (14004/25792)\n",
      "Loss: 1.274 | Acc: 54.312% (14043/25856)\n",
      "Loss: 1.274 | Acc: 54.321% (14080/25920)\n",
      "Loss: 1.274 | Acc: 54.318% (14114/25984)\n",
      "Loss: 1.274 | Acc: 54.319% (14149/26048)\n",
      "Loss: 1.275 | Acc: 54.308% (14181/26112)\n",
      "Loss: 1.274 | Acc: 54.313% (14217/26176)\n",
      "Loss: 1.274 | Acc: 54.329% (14256/26240)\n",
      "Loss: 1.274 | Acc: 54.319% (14288/26304)\n",
      "Loss: 1.274 | Acc: 54.316% (14322/26368)\n",
      "Loss: 1.275 | Acc: 54.298% (14352/26432)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.275 | Acc: 54.295% (14386/26496)\n",
      "Loss: 1.274 | Acc: 54.300% (14422/26560)\n",
      "Loss: 1.274 | Acc: 54.312% (14460/26624)\n",
      "Loss: 1.274 | Acc: 54.320% (14497/26688)\n",
      "Loss: 1.274 | Acc: 54.302% (14527/26752)\n",
      "Loss: 1.274 | Acc: 54.303% (14562/26816)\n",
      "Loss: 1.274 | Acc: 54.293% (14594/26880)\n",
      "Loss: 1.274 | Acc: 54.279% (14625/26944)\n",
      "Loss: 1.274 | Acc: 54.284% (14661/27008)\n",
      "Loss: 1.274 | Acc: 54.263% (14690/27072)\n",
      "Loss: 1.274 | Acc: 54.245% (14720/27136)\n",
      "Loss: 1.275 | Acc: 54.228% (14750/27200)\n",
      "Loss: 1.275 | Acc: 54.225% (14784/27264)\n",
      "Loss: 1.275 | Acc: 54.194% (14810/27328)\n",
      "Loss: 1.276 | Acc: 54.209% (14849/27392)\n",
      "Loss: 1.276 | Acc: 54.185% (14877/27456)\n",
      "Loss: 1.276 | Acc: 54.208% (14918/27520)\n",
      "Loss: 1.277 | Acc: 54.194% (14949/27584)\n",
      "Loss: 1.277 | Acc: 54.192% (14983/27648)\n",
      "Loss: 1.277 | Acc: 54.193% (15018/27712)\n",
      "Loss: 1.277 | Acc: 54.205% (15056/27776)\n",
      "Loss: 1.277 | Acc: 54.174% (15082/27840)\n",
      "Loss: 1.277 | Acc: 54.175% (15117/27904)\n",
      "Loss: 1.277 | Acc: 54.191% (15156/27968)\n",
      "Loss: 1.276 | Acc: 54.185% (15189/28032)\n",
      "Loss: 1.276 | Acc: 54.179% (15222/28096)\n",
      "Loss: 1.276 | Acc: 54.183% (15258/28160)\n",
      "Loss: 1.276 | Acc: 54.181% (15292/28224)\n",
      "Loss: 1.276 | Acc: 54.189% (15329/28288)\n",
      "Loss: 1.276 | Acc: 54.183% (15362/28352)\n",
      "Loss: 1.277 | Acc: 54.177% (15395/28416)\n",
      "Loss: 1.277 | Acc: 54.171% (15428/28480)\n",
      "Loss: 1.276 | Acc: 54.173% (15463/28544)\n",
      "Loss: 1.277 | Acc: 54.156% (15493/28608)\n",
      "Loss: 1.277 | Acc: 54.171% (15532/28672)\n",
      "Loss: 1.278 | Acc: 54.162% (15564/28736)\n",
      "Loss: 1.277 | Acc: 54.163% (15599/28800)\n",
      "Loss: 1.277 | Acc: 54.189% (15641/28864)\n",
      "Loss: 1.276 | Acc: 54.217% (15684/28928)\n",
      "Loss: 1.276 | Acc: 54.208% (15716/28992)\n",
      "Loss: 1.277 | Acc: 54.209% (15751/29056)\n",
      "Loss: 1.276 | Acc: 54.224% (15790/29120)\n",
      "Loss: 1.276 | Acc: 54.228% (15826/29184)\n",
      "Loss: 1.276 | Acc: 54.243% (15865/29248)\n",
      "Loss: 1.276 | Acc: 54.247% (15901/29312)\n",
      "Loss: 1.276 | Acc: 54.248% (15936/29376)\n",
      "Loss: 1.276 | Acc: 54.243% (15969/29440)\n",
      "Loss: 1.276 | Acc: 54.240% (16003/29504)\n",
      "Loss: 1.276 | Acc: 54.211% (16029/29568)\n",
      "Loss: 1.277 | Acc: 54.188% (16057/29632)\n",
      "Loss: 1.277 | Acc: 54.219% (16101/29696)\n",
      "Loss: 1.277 | Acc: 54.194% (16128/29760)\n",
      "Loss: 1.277 | Acc: 54.171% (16156/29824)\n",
      "Loss: 1.277 | Acc: 54.166% (16189/29888)\n",
      "Loss: 1.277 | Acc: 54.190% (16231/29952)\n",
      "Loss: 1.276 | Acc: 54.201% (16269/30016)\n",
      "Loss: 1.276 | Acc: 54.209% (16306/30080)\n",
      "Loss: 1.276 | Acc: 54.200% (16338/30144)\n",
      "Loss: 1.276 | Acc: 54.221% (16379/30208)\n",
      "Loss: 1.276 | Acc: 54.232% (16417/30272)\n",
      "Loss: 1.276 | Acc: 54.236% (16453/30336)\n",
      "Loss: 1.275 | Acc: 54.237% (16488/30400)\n",
      "Loss: 1.276 | Acc: 54.225% (16519/30464)\n",
      "Loss: 1.275 | Acc: 54.242% (16559/30528)\n",
      "Loss: 1.275 | Acc: 54.253% (16597/30592)\n",
      "Loss: 1.275 | Acc: 54.231% (16625/30656)\n",
      "Loss: 1.275 | Acc: 54.212% (16654/30720)\n",
      "Loss: 1.275 | Acc: 54.229% (16694/30784)\n",
      "Loss: 1.275 | Acc: 54.221% (16726/30848)\n",
      "Loss: 1.275 | Acc: 54.199% (16754/30912)\n",
      "Loss: 1.275 | Acc: 54.210% (16792/30976)\n",
      "Loss: 1.276 | Acc: 54.207% (16826/31040)\n",
      "Loss: 1.275 | Acc: 54.215% (16863/31104)\n",
      "Loss: 1.275 | Acc: 54.235% (16904/31168)\n",
      "Loss: 1.274 | Acc: 54.252% (16944/31232)\n",
      "Loss: 1.274 | Acc: 54.250% (16978/31296)\n",
      "Loss: 1.274 | Acc: 54.247% (17012/31360)\n",
      "Loss: 1.274 | Acc: 54.252% (17048/31424)\n",
      "Loss: 1.275 | Acc: 54.233% (17077/31488)\n",
      "Loss: 1.275 | Acc: 54.237% (17113/31552)\n",
      "Loss: 1.275 | Acc: 54.216% (17141/31616)\n",
      "Loss: 1.275 | Acc: 54.223% (17178/31680)\n",
      "Loss: 1.274 | Acc: 54.228% (17214/31744)\n",
      "Loss: 1.275 | Acc: 54.228% (17249/31808)\n",
      "Loss: 1.274 | Acc: 54.229% (17284/31872)\n",
      "Loss: 1.274 | Acc: 54.237% (17321/31936)\n",
      "Loss: 1.273 | Acc: 54.237% (17356/32000)\n",
      "Loss: 1.274 | Acc: 54.226% (17387/32064)\n",
      "Loss: 1.274 | Acc: 54.230% (17423/32128)\n",
      "Loss: 1.274 | Acc: 54.200% (17448/32192)\n",
      "Loss: 1.275 | Acc: 54.179% (17476/32256)\n",
      "Loss: 1.274 | Acc: 54.183% (17512/32320)\n",
      "Loss: 1.274 | Acc: 54.190% (17549/32384)\n",
      "Loss: 1.274 | Acc: 54.201% (17587/32448)\n",
      "Loss: 1.275 | Acc: 54.192% (17619/32512)\n",
      "Loss: 1.274 | Acc: 54.206% (17658/32576)\n",
      "Loss: 1.274 | Acc: 54.206% (17693/32640)\n",
      "Loss: 1.274 | Acc: 54.217% (17731/32704)\n",
      "Loss: 1.274 | Acc: 54.221% (17767/32768)\n",
      "Loss: 1.275 | Acc: 54.200% (17795/32832)\n",
      "Loss: 1.274 | Acc: 54.225% (17838/32896)\n",
      "Loss: 1.274 | Acc: 54.242% (17878/32960)\n",
      "Loss: 1.273 | Acc: 54.258% (17918/33024)\n",
      "Loss: 1.274 | Acc: 54.252% (17951/33088)\n",
      "Loss: 1.274 | Acc: 54.253% (17986/33152)\n",
      "Loss: 1.273 | Acc: 54.254% (18021/33216)\n",
      "Loss: 1.273 | Acc: 54.270% (18061/33280)\n",
      "Loss: 1.273 | Acc: 54.253% (18090/33344)\n",
      "Loss: 1.273 | Acc: 54.262% (18128/33408)\n",
      "Loss: 1.273 | Acc: 54.251% (18159/33472)\n",
      "Loss: 1.274 | Acc: 54.237% (18189/33536)\n",
      "Loss: 1.273 | Acc: 54.241% (18225/33600)\n",
      "Loss: 1.273 | Acc: 54.236% (18258/33664)\n",
      "Loss: 1.273 | Acc: 54.255% (18299/33728)\n",
      "Loss: 1.273 | Acc: 54.252% (18333/33792)\n",
      "Loss: 1.273 | Acc: 54.280% (18377/33856)\n",
      "Loss: 1.273 | Acc: 54.275% (18410/33920)\n",
      "Loss: 1.274 | Acc: 54.264% (18441/33984)\n",
      "Loss: 1.274 | Acc: 54.268% (18477/34048)\n",
      "Loss: 1.274 | Acc: 54.262% (18510/34112)\n",
      "Loss: 1.273 | Acc: 54.278% (18550/34176)\n",
      "Loss: 1.273 | Acc: 54.270% (18582/34240)\n",
      "Loss: 1.273 | Acc: 54.291% (18624/34304)\n",
      "Loss: 1.274 | Acc: 54.269% (18651/34368)\n",
      "Loss: 1.274 | Acc: 54.261% (18683/34432)\n",
      "Loss: 1.274 | Acc: 54.247% (18713/34496)\n",
      "Loss: 1.274 | Acc: 54.210% (18735/34560)\n",
      "Loss: 1.275 | Acc: 54.205% (18768/34624)\n",
      "Loss: 1.274 | Acc: 54.206% (18803/34688)\n",
      "Loss: 1.274 | Acc: 54.210% (18839/34752)\n",
      "Loss: 1.274 | Acc: 54.222% (18878/34816)\n",
      "Loss: 1.274 | Acc: 54.229% (18915/34880)\n",
      "Loss: 1.274 | Acc: 54.224% (18948/34944)\n",
      "Loss: 1.273 | Acc: 54.250% (18992/35008)\n",
      "Loss: 1.273 | Acc: 54.260% (19030/35072)\n",
      "Loss: 1.273 | Acc: 54.255% (19063/35136)\n",
      "Loss: 1.273 | Acc: 54.256% (19098/35200)\n",
      "Loss: 1.273 | Acc: 54.237% (19126/35264)\n",
      "Loss: 1.273 | Acc: 54.232% (19159/35328)\n",
      "Loss: 1.273 | Acc: 54.252% (19201/35392)\n",
      "Loss: 1.273 | Acc: 54.250% (19235/35456)\n",
      "Loss: 1.272 | Acc: 54.268% (19276/35520)\n",
      "Loss: 1.272 | Acc: 54.274% (19313/35584)\n",
      "Loss: 1.272 | Acc: 54.295% (19355/35648)\n",
      "Loss: 1.272 | Acc: 54.298% (19391/35712)\n",
      "Loss: 1.272 | Acc: 54.299% (19426/35776)\n",
      "Loss: 1.272 | Acc: 54.280% (19454/35840)\n",
      "Loss: 1.272 | Acc: 54.306% (19498/35904)\n",
      "Loss: 1.271 | Acc: 54.309% (19534/35968)\n",
      "Loss: 1.271 | Acc: 54.305% (19567/36032)\n",
      "Loss: 1.272 | Acc: 54.289% (19596/36096)\n",
      "Loss: 1.271 | Acc: 54.300% (19635/36160)\n",
      "Loss: 1.271 | Acc: 54.290% (19666/36224)\n",
      "Loss: 1.272 | Acc: 54.269% (19693/36288)\n",
      "Loss: 1.272 | Acc: 54.253% (19722/36352)\n",
      "Loss: 1.272 | Acc: 54.240% (19752/36416)\n",
      "Loss: 1.271 | Acc: 54.246% (19789/36480)\n",
      "Loss: 1.271 | Acc: 54.272% (19833/36544)\n",
      "Loss: 1.271 | Acc: 54.267% (19866/36608)\n",
      "Loss: 1.271 | Acc: 54.278% (19905/36672)\n",
      "Loss: 1.270 | Acc: 54.290% (19944/36736)\n",
      "Loss: 1.270 | Acc: 54.285% (19977/36800)\n",
      "Loss: 1.270 | Acc: 54.281% (20010/36864)\n",
      "Loss: 1.270 | Acc: 54.295% (20050/36928)\n",
      "Loss: 1.270 | Acc: 54.287% (20082/36992)\n",
      "Loss: 1.270 | Acc: 54.296% (20120/37056)\n",
      "Loss: 1.270 | Acc: 54.302% (20157/37120)\n",
      "Loss: 1.270 | Acc: 54.303% (20192/37184)\n",
      "Loss: 1.270 | Acc: 54.298% (20225/37248)\n",
      "Loss: 1.270 | Acc: 54.302% (20261/37312)\n",
      "Loss: 1.270 | Acc: 54.302% (20296/37376)\n",
      "Loss: 1.270 | Acc: 54.295% (20328/37440)\n",
      "Loss: 1.270 | Acc: 54.285% (20359/37504)\n",
      "Loss: 1.270 | Acc: 54.310% (20403/37568)\n",
      "Loss: 1.270 | Acc: 54.297% (20433/37632)\n",
      "Loss: 1.270 | Acc: 54.298% (20468/37696)\n",
      "Loss: 1.270 | Acc: 54.277% (20495/37760)\n",
      "Loss: 1.270 | Acc: 54.280% (20531/37824)\n",
      "Loss: 1.270 | Acc: 54.284% (20567/37888)\n",
      "Loss: 1.270 | Acc: 54.311% (20612/37952)\n",
      "Loss: 1.270 | Acc: 54.317% (20649/38016)\n",
      "Loss: 1.269 | Acc: 54.315% (20683/38080)\n",
      "Loss: 1.270 | Acc: 54.302% (20713/38144)\n",
      "Loss: 1.269 | Acc: 54.303% (20748/38208)\n",
      "Loss: 1.269 | Acc: 54.327% (20792/38272)\n",
      "Loss: 1.269 | Acc: 54.328% (20827/38336)\n",
      "Loss: 1.269 | Acc: 54.315% (20857/38400)\n",
      "Loss: 1.270 | Acc: 54.305% (20888/38464)\n",
      "Loss: 1.270 | Acc: 54.311% (20925/38528)\n",
      "Loss: 1.270 | Acc: 54.314% (20961/38592)\n",
      "Loss: 1.269 | Acc: 54.310% (20994/38656)\n",
      "Loss: 1.269 | Acc: 54.300% (21025/38720)\n",
      "Loss: 1.270 | Acc: 54.301% (21060/38784)\n",
      "Loss: 1.270 | Acc: 54.304% (21096/38848)\n",
      "Loss: 1.269 | Acc: 54.330% (21141/38912)\n",
      "Loss: 1.269 | Acc: 54.328% (21175/38976)\n",
      "Loss: 1.269 | Acc: 54.334% (21212/39040)\n",
      "Loss: 1.268 | Acc: 54.342% (21250/39104)\n",
      "Loss: 1.269 | Acc: 54.325% (21278/39168)\n",
      "Loss: 1.269 | Acc: 54.331% (21315/39232)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.269 | Acc: 54.334% (21351/39296)\n",
      "Loss: 1.269 | Acc: 54.324% (21382/39360)\n",
      "Loss: 1.269 | Acc: 54.327% (21418/39424)\n",
      "Loss: 1.269 | Acc: 54.343% (21459/39488)\n",
      "Loss: 1.269 | Acc: 54.339% (21492/39552)\n",
      "Loss: 1.269 | Acc: 54.324% (21521/39616)\n",
      "Loss: 1.269 | Acc: 54.325% (21556/39680)\n",
      "Loss: 1.269 | Acc: 54.338% (21596/39744)\n",
      "Loss: 1.269 | Acc: 54.328% (21627/39808)\n",
      "Loss: 1.269 | Acc: 54.316% (21657/39872)\n",
      "Loss: 1.269 | Acc: 54.314% (21691/39936)\n",
      "Loss: 1.269 | Acc: 54.310% (21724/40000)\n",
      "Loss: 1.269 | Acc: 54.321% (21763/40064)\n",
      "Loss: 1.269 | Acc: 54.321% (21798/40128)\n",
      "Loss: 1.269 | Acc: 54.319% (21832/40192)\n",
      "Loss: 1.269 | Acc: 54.322% (21868/40256)\n",
      "Loss: 1.268 | Acc: 54.320% (21902/40320)\n",
      "Loss: 1.268 | Acc: 54.321% (21937/40384)\n",
      "Loss: 1.268 | Acc: 54.319% (21971/40448)\n",
      "Loss: 1.268 | Acc: 54.325% (22008/40512)\n",
      "Loss: 1.268 | Acc: 54.330% (22045/40576)\n",
      "Loss: 1.268 | Acc: 54.328% (22079/40640)\n",
      "Loss: 1.268 | Acc: 54.346% (22121/40704)\n",
      "Loss: 1.268 | Acc: 54.344% (22155/40768)\n",
      "Loss: 1.268 | Acc: 54.340% (22188/40832)\n",
      "Loss: 1.268 | Acc: 54.348% (22226/40896)\n",
      "Loss: 1.268 | Acc: 54.346% (22260/40960)\n",
      "Loss: 1.268 | Acc: 54.363% (22302/41024)\n",
      "Loss: 1.268 | Acc: 54.352% (22332/41088)\n",
      "Loss: 1.268 | Acc: 54.359% (22370/41152)\n",
      "Loss: 1.268 | Acc: 54.372% (22410/41216)\n",
      "Loss: 1.268 | Acc: 54.370% (22444/41280)\n",
      "Loss: 1.268 | Acc: 54.371% (22479/41344)\n",
      "Loss: 1.268 | Acc: 54.361% (22510/41408)\n",
      "Loss: 1.268 | Acc: 54.367% (22547/41472)\n",
      "Loss: 1.268 | Acc: 54.372% (22584/41536)\n",
      "Loss: 1.268 | Acc: 54.365% (22616/41600)\n",
      "Loss: 1.268 | Acc: 54.359% (22648/41664)\n",
      "Loss: 1.268 | Acc: 54.354% (22681/41728)\n",
      "Loss: 1.268 | Acc: 54.353% (22715/41792)\n",
      "Loss: 1.268 | Acc: 54.365% (22755/41856)\n",
      "Loss: 1.268 | Acc: 54.358% (22787/41920)\n",
      "Loss: 1.268 | Acc: 54.378% (22830/41984)\n",
      "Loss: 1.268 | Acc: 54.383% (22867/42048)\n",
      "Loss: 1.267 | Acc: 54.407% (22912/42112)\n",
      "Loss: 1.267 | Acc: 54.403% (22945/42176)\n",
      "Loss: 1.267 | Acc: 54.401% (22979/42240)\n",
      "Loss: 1.267 | Acc: 54.404% (23015/42304)\n",
      "Loss: 1.267 | Acc: 54.397% (23047/42368)\n",
      "Loss: 1.267 | Acc: 54.407% (23086/42432)\n",
      "Loss: 1.267 | Acc: 54.424% (23128/42496)\n",
      "Loss: 1.267 | Acc: 54.429% (23165/42560)\n",
      "Loss: 1.267 | Acc: 54.427% (23199/42624)\n",
      "Loss: 1.266 | Acc: 54.439% (23239/42688)\n",
      "Loss: 1.267 | Acc: 54.437% (23273/42752)\n",
      "Loss: 1.267 | Acc: 54.419% (23300/42816)\n",
      "Loss: 1.267 | Acc: 54.419% (23335/42880)\n",
      "Loss: 1.267 | Acc: 54.420% (23370/42944)\n",
      "Loss: 1.267 | Acc: 54.420% (23405/43008)\n",
      "Loss: 1.266 | Acc: 54.441% (23449/43072)\n",
      "Loss: 1.266 | Acc: 54.444% (23485/43136)\n",
      "Loss: 1.266 | Acc: 54.449% (23522/43200)\n",
      "Loss: 1.266 | Acc: 54.452% (23558/43264)\n",
      "Loss: 1.266 | Acc: 54.471% (23601/43328)\n",
      "Loss: 1.265 | Acc: 54.473% (23637/43392)\n",
      "Loss: 1.265 | Acc: 54.478% (23674/43456)\n",
      "Loss: 1.265 | Acc: 54.474% (23707/43520)\n",
      "Loss: 1.266 | Acc: 54.467% (23739/43584)\n",
      "Loss: 1.265 | Acc: 54.474% (23777/43648)\n",
      "Loss: 1.265 | Acc: 54.486% (23817/43712)\n",
      "Loss: 1.265 | Acc: 54.489% (23853/43776)\n",
      "Loss: 1.265 | Acc: 54.498% (23892/43840)\n",
      "Loss: 1.265 | Acc: 54.485% (23921/43904)\n",
      "Loss: 1.265 | Acc: 54.478% (23953/43968)\n",
      "Loss: 1.265 | Acc: 54.469% (23984/44032)\n",
      "Loss: 1.265 | Acc: 54.472% (24020/44096)\n",
      "Loss: 1.265 | Acc: 54.461% (24050/44160)\n",
      "Loss: 1.265 | Acc: 54.468% (24088/44224)\n",
      "Loss: 1.265 | Acc: 54.462% (24120/44288)\n",
      "Loss: 1.265 | Acc: 54.455% (24152/44352)\n",
      "Loss: 1.265 | Acc: 54.462% (24190/44416)\n",
      "Loss: 1.265 | Acc: 54.456% (24222/44480)\n",
      "Loss: 1.265 | Acc: 54.456% (24257/44544)\n",
      "Loss: 1.266 | Acc: 54.457% (24292/44608)\n",
      "Loss: 1.266 | Acc: 54.443% (24321/44672)\n",
      "Loss: 1.266 | Acc: 54.455% (24361/44736)\n",
      "Loss: 1.266 | Acc: 54.449% (24393/44800)\n",
      "Loss: 1.265 | Acc: 54.451% (24429/44864)\n",
      "Loss: 1.265 | Acc: 54.456% (24466/44928)\n",
      "Loss: 1.265 | Acc: 54.450% (24498/44992)\n",
      "Loss: 1.265 | Acc: 54.461% (24538/45056)\n",
      "Loss: 1.265 | Acc: 54.477% (24580/45120)\n",
      "Loss: 1.265 | Acc: 54.493% (24622/45184)\n",
      "Loss: 1.265 | Acc: 54.480% (24651/45248)\n",
      "Loss: 1.266 | Acc: 54.469% (24681/45312)\n",
      "Loss: 1.265 | Acc: 54.472% (24717/45376)\n",
      "Loss: 1.265 | Acc: 54.461% (24747/45440)\n",
      "Loss: 1.266 | Acc: 54.448% (24776/45504)\n",
      "Loss: 1.265 | Acc: 54.450% (24812/45568)\n",
      "Loss: 1.265 | Acc: 54.460% (24851/45632)\n",
      "Loss: 1.265 | Acc: 54.462% (24887/45696)\n",
      "Loss: 1.265 | Acc: 54.476% (24928/45760)\n",
      "Loss: 1.265 | Acc: 54.469% (24960/45824)\n",
      "Loss: 1.265 | Acc: 54.474% (24997/45888)\n",
      "Loss: 1.265 | Acc: 54.476% (25033/45952)\n",
      "Loss: 1.265 | Acc: 54.472% (25066/46016)\n",
      "Loss: 1.264 | Acc: 54.490% (25109/46080)\n",
      "Loss: 1.264 | Acc: 54.479% (25139/46144)\n",
      "Loss: 1.264 | Acc: 54.495% (25181/46208)\n",
      "Loss: 1.264 | Acc: 54.502% (25219/46272)\n",
      "Loss: 1.264 | Acc: 54.502% (25254/46336)\n",
      "Loss: 1.264 | Acc: 54.498% (25287/46400)\n",
      "Loss: 1.264 | Acc: 54.496% (25321/46464)\n",
      "Loss: 1.264 | Acc: 54.488% (25352/46528)\n",
      "Loss: 1.264 | Acc: 54.501% (25393/46592)\n",
      "Loss: 1.264 | Acc: 54.490% (25423/46656)\n",
      "Loss: 1.264 | Acc: 54.501% (25463/46720)\n",
      "Loss: 1.264 | Acc: 54.504% (25499/46784)\n",
      "Loss: 1.264 | Acc: 54.504% (25534/46848)\n",
      "Loss: 1.263 | Acc: 54.502% (25568/46912)\n",
      "Loss: 1.264 | Acc: 54.500% (25602/46976)\n",
      "Loss: 1.264 | Acc: 54.496% (25635/47040)\n",
      "Loss: 1.264 | Acc: 54.496% (25670/47104)\n",
      "Loss: 1.264 | Acc: 54.490% (25702/47168)\n",
      "Loss: 1.264 | Acc: 54.488% (25736/47232)\n",
      "Loss: 1.264 | Acc: 54.510% (25781/47296)\n",
      "Loss: 1.264 | Acc: 54.519% (25820/47360)\n",
      "Loss: 1.264 | Acc: 54.504% (25848/47424)\n",
      "Loss: 1.264 | Acc: 54.513% (25887/47488)\n",
      "Loss: 1.264 | Acc: 54.511% (25921/47552)\n",
      "Loss: 1.264 | Acc: 54.513% (25957/47616)\n",
      "Loss: 1.264 | Acc: 54.518% (25994/47680)\n",
      "Loss: 1.264 | Acc: 54.509% (26025/47744)\n",
      "Loss: 1.264 | Acc: 54.514% (26062/47808)\n",
      "Loss: 1.264 | Acc: 54.504% (26092/47872)\n",
      "Loss: 1.264 | Acc: 54.489% (26120/47936)\n",
      "Loss: 1.264 | Acc: 54.490% (26155/48000)\n",
      "Loss: 1.265 | Acc: 54.484% (26187/48064)\n",
      "Loss: 1.264 | Acc: 54.492% (26226/48128)\n",
      "Loss: 1.264 | Acc: 54.505% (26267/48192)\n",
      "Loss: 1.264 | Acc: 54.518% (26308/48256)\n",
      "Loss: 1.264 | Acc: 54.526% (26347/48320)\n",
      "Loss: 1.264 | Acc: 54.530% (26384/48384)\n",
      "Loss: 1.263 | Acc: 54.535% (26421/48448)\n",
      "Loss: 1.264 | Acc: 54.535% (26456/48512)\n",
      "Loss: 1.264 | Acc: 54.529% (26488/48576)\n",
      "Loss: 1.263 | Acc: 54.544% (26530/48640)\n",
      "Loss: 1.263 | Acc: 54.544% (26565/48704)\n",
      "Loss: 1.263 | Acc: 54.546% (26601/48768)\n",
      "Loss: 1.263 | Acc: 54.542% (26634/48832)\n",
      "Loss: 1.263 | Acc: 54.538% (26667/48896)\n",
      "Loss: 1.263 | Acc: 54.542% (26704/48960)\n",
      "Loss: 1.263 | Acc: 54.553% (26731/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 54.553061224489795\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.007 | Acc: 67.188% (43/64)\n",
      "Loss: 1.124 | Acc: 58.594% (75/128)\n",
      "Loss: 1.204 | Acc: 53.646% (103/192)\n",
      "Loss: 1.308 | Acc: 50.781% (130/256)\n",
      "Loss: 1.276 | Acc: 52.500% (168/320)\n",
      "Loss: 1.291 | Acc: 52.865% (203/384)\n",
      "Loss: 1.300 | Acc: 51.562% (231/448)\n",
      "Loss: 1.298 | Acc: 51.562% (264/512)\n",
      "Loss: 1.265 | Acc: 51.736% (298/576)\n",
      "Loss: 1.253 | Acc: 52.656% (337/640)\n",
      "Loss: 1.276 | Acc: 51.705% (364/704)\n",
      "Loss: 1.273 | Acc: 52.083% (400/768)\n",
      "Loss: 1.275 | Acc: 52.764% (439/832)\n",
      "Loss: 1.280 | Acc: 52.455% (470/896)\n",
      "Loss: 1.276 | Acc: 52.188% (501/960)\n",
      "Loss: 1.265 | Acc: 53.125% (544/1024)\n",
      "Loss: 1.269 | Acc: 53.033% (577/1088)\n",
      "Loss: 1.266 | Acc: 53.472% (616/1152)\n",
      "Loss: 1.261 | Acc: 54.030% (657/1216)\n",
      "Loss: 1.277 | Acc: 53.203% (681/1280)\n",
      "Loss: 1.273 | Acc: 53.051% (713/1344)\n",
      "Loss: 1.270 | Acc: 53.267% (750/1408)\n",
      "Loss: 1.268 | Acc: 53.193% (783/1472)\n",
      "Loss: 1.274 | Acc: 53.190% (817/1536)\n",
      "Loss: 1.277 | Acc: 53.125% (850/1600)\n",
      "Loss: 1.287 | Acc: 52.885% (880/1664)\n",
      "Loss: 1.288 | Acc: 52.778% (912/1728)\n",
      "Loss: 1.284 | Acc: 52.902% (948/1792)\n",
      "Loss: 1.285 | Acc: 52.856% (981/1856)\n",
      "Loss: 1.282 | Acc: 52.865% (1015/1920)\n",
      "Loss: 1.281 | Acc: 52.974% (1051/1984)\n",
      "Loss: 1.284 | Acc: 52.930% (1084/2048)\n",
      "Loss: 1.278 | Acc: 53.172% (1123/2112)\n",
      "Loss: 1.285 | Acc: 52.895% (1151/2176)\n",
      "Loss: 1.282 | Acc: 53.125% (1190/2240)\n",
      "Loss: 1.277 | Acc: 53.299% (1228/2304)\n",
      "Loss: 1.277 | Acc: 53.294% (1262/2368)\n",
      "Loss: 1.279 | Acc: 53.248% (1295/2432)\n",
      "Loss: 1.275 | Acc: 53.405% (1333/2496)\n",
      "Loss: 1.283 | Acc: 53.125% (1360/2560)\n",
      "Loss: 1.283 | Acc: 53.087% (1393/2624)\n",
      "Loss: 1.281 | Acc: 53.274% (1432/2688)\n",
      "Loss: 1.279 | Acc: 53.452% (1471/2752)\n",
      "Loss: 1.280 | Acc: 53.445% (1505/2816)\n",
      "Loss: 1.280 | Acc: 53.403% (1538/2880)\n",
      "Loss: 1.279 | Acc: 53.567% (1577/2944)\n",
      "Loss: 1.279 | Acc: 53.590% (1612/3008)\n",
      "Loss: 1.283 | Acc: 53.451% (1642/3072)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.282 | Acc: 53.571% (1680/3136)\n",
      "Loss: 1.280 | Acc: 53.594% (1715/3200)\n",
      "Loss: 1.278 | Acc: 53.646% (1751/3264)\n",
      "Loss: 1.277 | Acc: 53.786% (1790/3328)\n",
      "Loss: 1.276 | Acc: 53.862% (1827/3392)\n",
      "Loss: 1.278 | Acc: 53.819% (1860/3456)\n",
      "Loss: 1.281 | Acc: 53.750% (1892/3520)\n",
      "Loss: 1.281 | Acc: 53.739% (1926/3584)\n",
      "Loss: 1.281 | Acc: 53.810% (1963/3648)\n",
      "Loss: 1.280 | Acc: 53.798% (1997/3712)\n",
      "Loss: 1.280 | Acc: 53.814% (2032/3776)\n",
      "Loss: 1.277 | Acc: 54.010% (2074/3840)\n",
      "Loss: 1.275 | Acc: 54.098% (2112/3904)\n",
      "Loss: 1.277 | Acc: 54.057% (2145/3968)\n",
      "Loss: 1.275 | Acc: 54.191% (2185/4032)\n",
      "Loss: 1.275 | Acc: 54.175% (2219/4096)\n",
      "Loss: 1.276 | Acc: 54.183% (2254/4160)\n",
      "Loss: 1.275 | Acc: 54.190% (2289/4224)\n",
      "Loss: 1.274 | Acc: 54.314% (2329/4288)\n",
      "Loss: 1.273 | Acc: 54.389% (2367/4352)\n",
      "Loss: 1.273 | Acc: 54.438% (2404/4416)\n",
      "Loss: 1.272 | Acc: 54.531% (2443/4480)\n",
      "Loss: 1.270 | Acc: 54.577% (2480/4544)\n",
      "Loss: 1.272 | Acc: 54.514% (2512/4608)\n",
      "Loss: 1.271 | Acc: 54.495% (2546/4672)\n",
      "Loss: 1.267 | Acc: 54.540% (2583/4736)\n",
      "Loss: 1.269 | Acc: 54.562% (2619/4800)\n",
      "Loss: 1.267 | Acc: 54.461% (2649/4864)\n",
      "Loss: 1.267 | Acc: 54.545% (2688/4928)\n",
      "Loss: 1.269 | Acc: 54.407% (2716/4992)\n",
      "Loss: 1.269 | Acc: 54.391% (2750/5056)\n",
      "Loss: 1.274 | Acc: 54.180% (2774/5120)\n",
      "Loss: 1.273 | Acc: 54.263% (2813/5184)\n",
      "Loss: 1.276 | Acc: 54.059% (2837/5248)\n",
      "Loss: 1.277 | Acc: 54.010% (2869/5312)\n",
      "Loss: 1.281 | Acc: 53.943% (2900/5376)\n",
      "Loss: 1.282 | Acc: 53.879% (2931/5440)\n",
      "Loss: 1.282 | Acc: 53.888% (2966/5504)\n",
      "Loss: 1.285 | Acc: 53.790% (2995/5568)\n",
      "Loss: 1.287 | Acc: 53.800% (3030/5632)\n",
      "Loss: 1.287 | Acc: 53.845% (3067/5696)\n",
      "Loss: 1.286 | Acc: 53.854% (3102/5760)\n",
      "Loss: 1.284 | Acc: 53.880% (3138/5824)\n",
      "Loss: 1.287 | Acc: 53.736% (3164/5888)\n",
      "Loss: 1.287 | Acc: 53.780% (3201/5952)\n",
      "Loss: 1.287 | Acc: 53.757% (3234/6016)\n",
      "Loss: 1.287 | Acc: 53.717% (3266/6080)\n",
      "Loss: 1.287 | Acc: 53.678% (3298/6144)\n",
      "Loss: 1.288 | Acc: 53.640% (3330/6208)\n",
      "Loss: 1.289 | Acc: 53.571% (3360/6272)\n",
      "Loss: 1.288 | Acc: 53.630% (3398/6336)\n",
      "Loss: 1.288 | Acc: 53.562% (3428/6400)\n",
      "Loss: 1.290 | Acc: 53.481% (3457/6464)\n",
      "Loss: 1.290 | Acc: 53.523% (3494/6528)\n",
      "Loss: 1.293 | Acc: 53.413% (3521/6592)\n",
      "Loss: 1.293 | Acc: 53.395% (3554/6656)\n",
      "Loss: 1.293 | Acc: 53.393% (3588/6720)\n",
      "Loss: 1.293 | Acc: 53.464% (3627/6784)\n",
      "Loss: 1.291 | Acc: 53.490% (3663/6848)\n",
      "Loss: 1.293 | Acc: 53.414% (3692/6912)\n",
      "Loss: 1.293 | Acc: 53.440% (3728/6976)\n",
      "Loss: 1.295 | Acc: 53.395% (3759/7040)\n",
      "Loss: 1.295 | Acc: 53.392% (3793/7104)\n",
      "Loss: 1.296 | Acc: 53.460% (3832/7168)\n",
      "Loss: 1.298 | Acc: 53.402% (3862/7232)\n",
      "Loss: 1.295 | Acc: 53.358% (3893/7296)\n",
      "Loss: 1.294 | Acc: 53.370% (3928/7360)\n",
      "Loss: 1.294 | Acc: 53.341% (3960/7424)\n",
      "Loss: 1.292 | Acc: 53.352% (3995/7488)\n",
      "Loss: 1.291 | Acc: 53.443% (4036/7552)\n",
      "Loss: 1.291 | Acc: 53.466% (4072/7616)\n",
      "Loss: 1.289 | Acc: 53.516% (4110/7680)\n",
      "Loss: 1.288 | Acc: 53.603% (4151/7744)\n",
      "Loss: 1.287 | Acc: 53.701% (4193/7808)\n",
      "Loss: 1.288 | Acc: 53.709% (4228/7872)\n",
      "Loss: 1.287 | Acc: 53.730% (4264/7936)\n",
      "Loss: 1.289 | Acc: 53.725% (4298/8000)\n",
      "Loss: 1.289 | Acc: 53.745% (4334/8064)\n",
      "Loss: 1.290 | Acc: 53.666% (4362/8128)\n",
      "Loss: 1.288 | Acc: 53.723% (4401/8192)\n",
      "Loss: 1.289 | Acc: 53.694% (4433/8256)\n",
      "Loss: 1.291 | Acc: 53.618% (4461/8320)\n",
      "Loss: 1.291 | Acc: 53.566% (4491/8384)\n",
      "Loss: 1.290 | Acc: 53.622% (4530/8448)\n",
      "Loss: 1.292 | Acc: 53.548% (4558/8512)\n",
      "Loss: 1.291 | Acc: 53.568% (4594/8576)\n",
      "Loss: 1.292 | Acc: 53.542% (4626/8640)\n",
      "Loss: 1.292 | Acc: 53.493% (4656/8704)\n",
      "Loss: 1.294 | Acc: 53.467% (4688/8768)\n",
      "Loss: 1.292 | Acc: 53.567% (4731/8832)\n",
      "Loss: 1.292 | Acc: 53.575% (4766/8896)\n",
      "Loss: 1.292 | Acc: 53.583% (4801/8960)\n",
      "Loss: 1.291 | Acc: 53.635% (4840/9024)\n",
      "Loss: 1.291 | Acc: 53.631% (4874/9088)\n",
      "Loss: 1.292 | Acc: 53.617% (4907/9152)\n",
      "Loss: 1.290 | Acc: 53.700% (4949/9216)\n",
      "Loss: 1.289 | Acc: 53.728% (4986/9280)\n",
      "Loss: 1.289 | Acc: 53.778% (5025/9344)\n",
      "Loss: 1.290 | Acc: 53.699% (5052/9408)\n",
      "Loss: 1.292 | Acc: 53.685% (5085/9472)\n",
      "Loss: 1.290 | Acc: 53.754% (5126/9536)\n",
      "Loss: 1.289 | Acc: 53.854% (5170/9600)\n",
      "Loss: 1.290 | Acc: 53.818% (5201/9664)\n",
      "Loss: 1.290 | Acc: 53.845% (5238/9728)\n",
      "Loss: 1.290 | Acc: 53.819% (5270/9792)\n",
      "Loss: 1.291 | Acc: 53.754% (5298/9856)\n",
      "Loss: 1.292 | Acc: 53.669% (5324/9920)\n",
      "Loss: 1.294 | Acc: 53.606% (5352/9984)\n",
      "Loss: 1.294 | Acc: 53.590% (5359/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 53.59\n",
      "\n",
      "Final train set accuracy is 54.553061224489795\n",
      "Final test set accuracy is 53.59\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, \n",
    "            input_dims, \n",
    "            output_dims, \n",
    "            num_trans_layers, \n",
    "            num_heads, \n",
    "            image_k, \n",
    "            patch_k)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b924f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
