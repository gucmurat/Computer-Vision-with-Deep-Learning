{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Attentional Networks in Computer Vision\n",
    "Prepared by comp411 Teaching Unit (TA Can Küçüksözen) in the context of Computer Vision with Deep Learning Course. Do not hesitate to ask in case you have any questions, contact me at: ckucuksozen19@ku.edu.tr\n",
    "\n",
    "Up until this point, we have worked with deep fully-connected networks, convolutional networks and recurrent networks using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, on the other hand, most successful image processing methods use convolutional networks. However recent state-of-the-art results on computer vision realm are acquired using Attentional layers and Transformer architectures.\n",
    "\n",
    "First you will implement several layer types that are used in fully attentional networks. You will then use these layers to train an Attentional Image Classification network, specifically a smaller version of Vision Transformer (VIT) on the CIFAR-10 dataset. The original paper can be accessed via the following link: https://arxiv.org/pdf/2010.11929.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation\n",
    "\n",
    "First, we load the CIFAR-10 dataset. This might take a couple minutes the first time you do it, but the files should stay cached after that.\n",
    "\n",
    "In previous parts of the assignment we had to write our own code to download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches; PyTorch provides convenient tools to automate this process for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('./comp411/datasets', train=True, download=False,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('./comp411/datasets', train=False, download=False, \n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "You have an option to **use GPU by setting the flag to True below**. It is not necessary to use GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fallback to CPU mode.\n",
    "\n",
    "The global variables `dtype` and `device` will control the data types throughout this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will be using float throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Barebones Transformers: Self-Attentional Layer\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `SelfAttention`, which will perform the forward pass of a self-attentional layer. Our implementation of the SelfAttentional layer will include three distinct fully connected layers which will be responsible of:\n",
    "\n",
    "1. A fully connected layer, `W_Q`, which will be used to project our input into `queries`\n",
    "2. A fully connected layer, `W_K`, which will be used to project our input into `keys`\n",
    "3. A fully connected layer, `W_V`, which will be used to project our input into `values`\n",
    "\n",
    "After defining such three fully connected layers, and obtain our `queries, keys, and values` variables at the beginning of our forward pass, the following operations should be carried out in order to complete the attentional layer implementation.\n",
    "\n",
    "1. Seperate each of `query, key, and value` projections into their respective heads. In other words, split the feature vector dimension of each matrix into necessarry number of chunks.\n",
    "\n",
    "2. Compute the `Attention Scores` between each pair of sequence elements via conducting a scaled dot product operation between every pair of `queries` and `keys`. Note that `Attention Scores` matrix should have the size of `[# of queries , # of keys]`\n",
    "\n",
    "3. Calculate the `Attention Weights` of each query by applying the non-linear `Softmax` normalization accross the `keys` dimension of the `Attention Scores` matrix.\n",
    "\n",
    "4. Obtain the output combination of `values` by matrix multiplying `Attention Weights` with `values`\n",
    "\n",
    "5. Reassemble heads into one flat vector and return the output.\n",
    "\n",
    "**HINT**: For a more detailed explanation of the self attentional layer, examine the Appendix A of the original ViT manuscript here:  https://arxiv.org/pdf/2010.11929.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dims, head_dims=128, num_heads=2,  bias=False):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        ## initialize module's instance variables\n",
    "        self.input_dims = input_dims\n",
    "        self.head_dims = head_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_dims = head_dims * num_heads\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        self.W_Q = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_K = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "        self.W_V = nn.Linear(input_dims, self.proj_dims,bias=bias)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## Input of shape, [B, N, D] where:\n",
    "        ## - B denotes the batch size\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D corresponds to model dimensionality\n",
    "        b,n,d = x.shape\n",
    "        \n",
    "        ## Construct queries,keys,values\n",
    "        q_ = self.W_Q(x)\n",
    "        k_ = self.W_K(x)\n",
    "        v_ = self.W_V(x)\n",
    "        \n",
    "        ## Seperate q,k,v into their corresponding heads,\n",
    "        ## After this operation each q,k,v will have the shape: [B,H,N,D//H] where\n",
    "        ## - B denotes the batch size\n",
    "        ## - H denotes number of heads\n",
    "        ## - N denotes number of sequence elements. I.e. the number of patches + the class token \n",
    "        ## - D//H corresponds to per head dimensionality\n",
    "        q, k, v = map(lambda z: torch.reshape(z, (b,n,self.num_heads,self.head_dims)).permute(0,2,1,3), [q_,k_,v_])\n",
    "       \n",
    "        #########################################################################################\n",
    "        # TODO: Complete the forward pass of the SelfAttention layer, follow the comments below #\n",
    "        #########################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        ## Compute attention logits. Note that this operation is conducted as a\n",
    "        ## batched matrix multiplication between q and k, the output is scaled by 1/(D//H)^(1/2)\n",
    "        ## inputs are queries and keys that are both of size [B,H,N,D//H]\n",
    "        ## Output Attention logits should have the size: [B,H,N,N]\n",
    "        \n",
    "        attention_logits =  torch.matmul(q, k.transpose(-2, -1)) * (1 / ((d//self.num_heads)**(1/2)))\n",
    "    \n",
    "        ## Compute attention Weights. Note that this operation is conducted as a\n",
    "        ## Softmax Normalization across the keys dimension. \n",
    "        ## Hint: You can apply the Softmax operation across the final dimension\n",
    "\n",
    "        attention_weights = torch.softmax(attention_logits, dim = -1)\n",
    "        \n",
    "        ## Compute output values. Note that this operation is conducted as a \n",
    "        ## batched matrix multiplication between the Attention Weights matrix and \n",
    "        ## the values tensor. After computing output values, the output should be reshaped\n",
    "        ## Inputs are Attention Weights with size [B, H, N, N], values with size [B, H, N, D//H]\n",
    "        ## Output should be of size [B, N, D]\n",
    "        ## Hint: you should use torch.matmul, torch.permute, torch.reshape in that order\n",
    "        \n",
    "        attn_out = torch.matmul(attention_weights, v).permute(0, 2, 1, 3).reshape(-1, n, self.proj_dims)\n",
    "        \n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ################################################################################\n",
    "        #                                 END OF YOUR CODE                             \n",
    "        ################################################################################\n",
    "    \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Self-Attentional Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 256])\n"
     ]
    }
   ],
   "source": [
    "def test_self_attn_layer():\n",
    "    x = torch.zeros((64, 16, 32), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 32\n",
    "    layer = SelfAttention(32,64,4)\n",
    "    out = layer(x)\n",
    "    print(out.size())  # you should see [64,16,256]\n",
    "test_self_attn_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Barebones Transformers: Transformer Encoder Block\n",
    "\n",
    "Here you will complete the implementation of the Pytorch nn.module `TransformerBlock`, which will perform the forward pass of a Transfomer Encoder Block. You can refer to Figure 1 of the original manuscript of ViT from this link: https://arxiv.org/pdf/2010.11929.pdf in order to get yourself familiar with the architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of a two layer GELU activated Fully Connected Network is provided for you below:\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(input_dims, hidden_dims, bias=bias)\n",
    "        self.fc_2 = nn.Linear(hidden_dims, output_dims, bias=bias)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = F.relu(self.fc_1(x))\n",
    "        o = self.fc_2(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build from scratch a TransformerBlock Module. Note that the architecture of this\n",
    "## module follows a simple computational pipeline:\n",
    "## input --> layernorm --> SelfAttention --> skip connection \n",
    "##       --> layernorm --> MLP ---> skip connection ---> output\n",
    "## Note that the TransformerBlock module works on a single hidden dimension hidden_dims,\n",
    "## in order to faciliate skip connections with ease. Be careful about the input arguments\n",
    "## to the SelfAttention block.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads=4, bias=False):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "###############################################################\n",
    "# TODO: Complete the consturctor of  TransformerBlock module  #\n",
    "###############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "        self.norm_layer = nn.LayerNorm(hidden_dims)\n",
    "        self.attention = SelfAttention(hidden_dims, hidden_dims//num_heads, num_heads, bias=bias)\n",
    "        self.norm_layer2 = nn.LayerNorm(hidden_dims)\n",
    "        self.mlp = MLP(hidden_dims, hidden_dims, hidden_dims, bias=bias)\n",
    "        \n",
    "        \n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "##############################################################\n",
    "# TODO: Complete the forward of TransformerBlock module      #\n",
    "##############################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)****\n",
    "\n",
    "        norm_layer = self.norm_layer(x)\n",
    "        attention  = self.attention(norm_layer)\n",
    "        norm_layer2 = self.norm_layer2(attention + x)\n",
    "        mlp = self.mlp(norm_layer2)\n",
    "        output = mlp + attention + x\n",
    "        return output\n",
    "        \n",
    " # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "###################################################################\n",
    "#                                 END OF YOUR CODE                #             \n",
    "###################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the Transformer Block Layer above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 16, 128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 128])\n"
     ]
    }
   ],
   "source": [
    "def test_transfomerblock_layer():\n",
    "    x = torch.zeros((64, 16, 128), dtype=dtype)  # minibatch size 64, sequence elements size 16, feature channels size 128\n",
    "    layer = TransformerBlock(128,4) # hidden dims size 128, heads size 4\n",
    "    out = layer(x)\n",
    "    print(out.size()) \n",
    "test_transfomerblock_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV The Vision Transformer (ViT)\n",
    "\n",
    "The final implementation for the Pytorch nn.module `ViT` is given to you below, which will perform the forward pass of the Vision Transformer. Study it and get yourself familiar with the API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4, bias=False):\n",
    "        super(ViT, self).__init__()\n",
    "                \n",
    "        ## initialize module's instance variables\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_trans_layers = num_trans_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.image_k = image_k\n",
    "        self.patch_k = patch_k\n",
    "        \n",
    "        self.image_height = self.image_width = image_k\n",
    "        self.patch_height = self.patch_width = patch_k\n",
    "        \n",
    "        assert self.image_height % self.patch_height == 0 and self.image_width % self.patch_width == 0,\\\n",
    "                'Image size must be divisible by the patch size.'\n",
    "\n",
    "        self.num_patches = (self.image_height // self.patch_height) * (self.image_width // self.patch_width)\n",
    "        self.patch_flat_len = self.patch_height * self.patch_width\n",
    "        \n",
    "        ## Declare module's parameters\n",
    "        \n",
    "        ## ViT's flattened patch embedding projection:\n",
    "        self.linear_embed = nn.Linear(self.input_dims*self.patch_flat_len, self.hidden_dims)\n",
    "        \n",
    "        ## Learnable positional embeddings, an embedding is learned for each patch location and the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, self.hidden_dims))\n",
    "        \n",
    "        ## Learnable classt token and its index among attention sequence elements.\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.hidden_dims))\n",
    "        self.cls_index = torch.LongTensor([0])\n",
    "        \n",
    "        ## Declare cascaded Transformer blocks:\n",
    "        transformer_encoder_list = []\n",
    "        for _ in range(self.num_trans_layers):\n",
    "            transformer_encoder_list.append(TransformerBlock(self.hidden_dims, self.num_heads, bias))\n",
    "        self.transformer_encoder = nn.Sequential(*transformer_encoder_list)\n",
    "        \n",
    "        ## Declare the output mlp:\n",
    "        self.out_mlp = MLP(self.hidden_dims, self.hidden_dims, self.output_dims)\n",
    "         \n",
    "    def unfold(self, x, f = 7, st = 4, p = 0):\n",
    "        ## Create sliding window pathes using nn.Functional.unfold\n",
    "        ## Input dimensions: [B,D,H,W] where\n",
    "        ## --B : input batch size\n",
    "        ## --D : input channels\n",
    "        ## --H, W: input height and width\n",
    "        ## Output dimensions: [B,N,H*W,D]\n",
    "        ## --N : number of patches, decided according to sliding window kernel size (f),\n",
    "        ##      sliding window stride and padding.\n",
    "        b,d,h,w = x.shape\n",
    "        x_unf = F.unfold(x, (f,f), stride=st, padding=p)    \n",
    "        x_unf = torch.reshape(x_unf.permute(0,2,1), (b,-1,d,f*f)).transpose(-1,-2)\n",
    "        n = x_unf.size(1)\n",
    "        return x_unf,n\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.size(0)\n",
    "        ## create sliding window patches from the input image\n",
    "        x_patches,n = self.unfold(x, self.patch_height, self.patch_height, 0)\n",
    "        ## flatten each patch into a 1d vector: i.e. 3x4x4 image patch turned into 1x1x48\n",
    "        x_patch_flat = torch.reshape(x_patches, (b,n,-1))\n",
    "        ## linearly embed each flattened patch\n",
    "        x_embed = self.linear_embed(x_patch_flat)\n",
    "        \n",
    "        ## retrieve class token \n",
    "        cls_tokens = self.cls_token.repeat(b,1,1)\n",
    "        ## concatanate class token to input patches\n",
    "        xcls_embed = torch.cat([cls_tokens, x_embed], dim=-2)\n",
    "        \n",
    "        ## add positional embedding to input patches + class token \n",
    "        xcls_pos_embed = xcls_embed + self.pos_embedding\n",
    "        \n",
    "        ## pass through the transformer encoder\n",
    "        trans_out = self.transformer_encoder(xcls_pos_embed)\n",
    "        \n",
    "        ## select the class token \n",
    "        out_cls_token = torch.index_select(trans_out, -2, self.cls_index.to(trans_out.device))\n",
    "        \n",
    "        ## create output\n",
    "        out = self.out_mlp(out_cls_token)\n",
    "        \n",
    "        return out.squeeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the forward pass of the ViT above, run the following cell to test your implementation.\n",
    "\n",
    "When you run this function, output should have shape (64, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "def test_vit():\n",
    "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size 3,32,32\n",
    "    model = ViT(hidden_dims=128, input_dims=3, output_dims=10, num_trans_layers = 4, num_heads=4, image_k=32, patch_k=4)\n",
    "    out = model(x)\n",
    "    print(out.size()) \n",
    "test_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train the ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy\n",
    "Given any minibatch of input data and desired targets, we can check the classification accuracy of a neural network. \n",
    "\n",
    "The check_batch_accuracy function is provided for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def check_batch_accuracy(out, target,eps=1e-7):\n",
    "    b, c = out.shape\n",
    "    with torch.no_grad():\n",
    "        _, pred = out.max(-1) \n",
    "        correct = np.sum(np.equal(pred.cpu().numpy(), target.cpu().numpy()))\n",
    "    return correct, float(correct) / (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "As we have already seen in the Second Assignment, in our PyTorch based training loops, we use an Optimizer object from the `torch.optim` package, which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(network, optimizer, trainloader):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - trainloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall training accuracy for the epoch\n",
    "    \"\"\"\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    network.train()  # put model to training mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = Variable(inputs.to(device)), targets.to(device)  # move to device, e.g. GPU\n",
    "            \n",
    "        outputs = network(inputs)\n",
    "        loss =  F.cross_entropy(outputs, targets)\n",
    "            \n",
    "        # Zero out all of the gradients for the variables which the optimizer\n",
    "        # will update.\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # This is the backwards pass: compute the gradient of the loss with\n",
    "        # respect to each  parameter of the model.\n",
    "        loss.backward()\n",
    "            \n",
    "        # Actually update the parameters of the model using the gradients\n",
    "        # computed by the backwards pass.\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.detach()\n",
    "        train_loss += loss.item()\n",
    "        correct_p, _ = check_batch_accuracy(outputs, targets) \n",
    "        correct += correct_p\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        \n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Loop\n",
    "We have also prepared a Evaluation loop in order to determine our networks capabilities in terms of classification accuracy on a given dataset, either the training, or the validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, evalloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CIFAR-10 using the PyTorch Module API for a single epoch\n",
    "    \n",
    "    Inputs:\n",
    "    - network: A PyTorch Module giving the model to train.\n",
    "    - evalloader: Iterable DataLoader object that fetches the minibatches\n",
    "    \n",
    "    Returns: overall evaluation accuracy for the epoch\n",
    "    \"\"\"\n",
    "    network.eval() # put model to evaluation mode\n",
    "    network = network.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print('\\n---- Evaluation in process ----')\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(evalloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # move to device, e.g. GPU\n",
    "            outputs = network(inputs)\n",
    "            loss = F.cross_entropy(outputs, targets)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "            correct_p, _ = check_batch_accuracy(outputs, targets)\n",
    "            correct += correct_p\n",
    "            total += targets.size(0)\n",
    "            print('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (eval_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit a ViT\n",
    "Now we are ready to run the training loop. A nice trick is to train your model with just a few training samples in order to see if your implementation is actually bug free. \n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `ViT`. \n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `ViT`. We prefer to use `Adam` optimizer for this part.\n",
    "\n",
    "You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For overfitting experiments, the subset of the dataset that is used has 100 sample images\n",
      "==> Data ready, batchsize = 25\n"
     ]
    }
   ],
   "source": [
    "sample_idx_tr = torch.randperm(len(cifar10_train))[:100]\n",
    "sample_idx_val = torch.randperm(len(cifar10_train))[-100:]\n",
    "\n",
    "trainset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_tr)\n",
    "valset_sub = torch.utils.data.Subset(cifar10_train, sample_idx_val)\n",
    "\n",
    "print(\"For overfitting experiments, the subset of the dataset that is used has {} sample images\".format(len(trainset_sub)))\n",
    "\n",
    "batch_size_sub = 25\n",
    "trainloader_sub = torch.utils.data.DataLoader(trainset_sub, batch_size=batch_size_sub, shuffle=True)\n",
    "valloader_sub = torch.utils.data.DataLoader(valset_sub, batch_size=batch_size_sub, shuffle=False)\n",
    "\n",
    "print('==> Data ready, batchsize = {}'.format(batch_size_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.734 | Acc: 16.000% (4/25)\n",
      "Loss: 4.913 | Acc: 12.000% (6/50)\n",
      "Loss: 4.549 | Acc: 10.667% (8/75)\n",
      "Loss: 4.202 | Acc: 12.000% (12/100)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 12.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.228 | Acc: 0.000% (0/25)\n",
      "Loss: 3.087 | Acc: 12.000% (6/50)\n",
      "Loss: 3.135 | Acc: 13.333% (10/75)\n",
      "Loss: 3.122 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 0 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 3.560 | Acc: 28.000% (7/25)\n",
      "Loss: 2.860 | Acc: 30.000% (15/50)\n",
      "Loss: 2.836 | Acc: 24.000% (18/75)\n",
      "Loss: 2.758 | Acc: 21.000% (21/100)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 21.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.408 | Acc: 24.000% (6/25)\n",
      "Loss: 2.624 | Acc: 16.000% (8/50)\n",
      "Loss: 2.704 | Acc: 14.667% (11/75)\n",
      "Loss: 2.768 | Acc: 14.000% (14/100)\n",
      "Evaluation of Epoch 1 is completed, Validation accuracy for this epoch is 14.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.588 | Acc: 48.000% (12/25)\n",
      "Loss: 1.882 | Acc: 40.000% (20/50)\n",
      "Loss: 1.956 | Acc: 36.000% (27/75)\n",
      "Loss: 2.053 | Acc: 32.000% (32/100)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 32.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.366 | Acc: 16.000% (4/25)\n",
      "Loss: 2.503 | Acc: 14.000% (7/50)\n",
      "Loss: 2.327 | Acc: 20.000% (15/75)\n",
      "Loss: 2.392 | Acc: 19.000% (19/100)\n",
      "Evaluation of Epoch 2 is completed, Validation accuracy for this epoch is 19.0\n",
      "\n",
      "Epoch: 3\n",
      "Loss: 1.731 | Acc: 44.000% (11/25)\n",
      "Loss: 1.724 | Acc: 42.000% (21/50)\n",
      "Loss: 1.714 | Acc: 42.667% (32/75)\n",
      "Loss: 1.946 | Acc: 35.000% (35/100)\n",
      "Epoch 3 of training is completed, Training accuracy for this epoch is 35.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.540 | Acc: 12.000% (3/25)\n",
      "Loss: 2.664 | Acc: 14.000% (7/50)\n",
      "Loss: 2.628 | Acc: 14.667% (11/75)\n",
      "Loss: 2.710 | Acc: 13.000% (13/100)\n",
      "Evaluation of Epoch 3 is completed, Validation accuracy for this epoch is 13.0\n",
      "\n",
      "Epoch: 4\n",
      "Loss: 1.539 | Acc: 44.000% (11/25)\n",
      "Loss: 1.394 | Acc: 46.000% (23/50)\n",
      "Loss: 1.390 | Acc: 48.000% (36/75)\n",
      "Loss: 1.464 | Acc: 44.000% (44/100)\n",
      "Epoch 4 of training is completed, Training accuracy for this epoch is 44.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.444 | Acc: 24.000% (6/25)\n",
      "Loss: 2.557 | Acc: 20.000% (10/50)\n",
      "Loss: 2.521 | Acc: 20.000% (15/75)\n",
      "Loss: 2.598 | Acc: 21.000% (21/100)\n",
      "Evaluation of Epoch 4 is completed, Validation accuracy for this epoch is 21.0\n",
      "\n",
      "Epoch: 5\n",
      "Loss: 1.212 | Acc: 52.000% (13/25)\n",
      "Loss: 1.405 | Acc: 52.000% (26/50)\n",
      "Loss: 1.475 | Acc: 48.000% (36/75)\n",
      "Loss: 1.448 | Acc: 45.000% (45/100)\n",
      "Epoch 5 of training is completed, Training accuracy for this epoch is 45.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.193 | Acc: 32.000% (8/25)\n",
      "Loss: 2.296 | Acc: 32.000% (16/50)\n",
      "Loss: 2.289 | Acc: 29.333% (22/75)\n",
      "Loss: 2.426 | Acc: 26.000% (26/100)\n",
      "Evaluation of Epoch 5 is completed, Validation accuracy for this epoch is 26.0\n",
      "\n",
      "Epoch: 6\n",
      "Loss: 1.185 | Acc: 60.000% (15/25)\n",
      "Loss: 1.119 | Acc: 62.000% (31/50)\n",
      "Loss: 1.039 | Acc: 66.667% (50/75)\n",
      "Loss: 1.193 | Acc: 61.000% (61/100)\n",
      "Epoch 6 of training is completed, Training accuracy for this epoch is 61.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.959 | Acc: 4.000% (1/25)\n",
      "Loss: 2.808 | Acc: 16.000% (8/50)\n",
      "Loss: 2.630 | Acc: 20.000% (15/75)\n",
      "Loss: 2.706 | Acc: 18.000% (18/100)\n",
      "Evaluation of Epoch 6 is completed, Validation accuracy for this epoch is 18.0\n",
      "\n",
      "Epoch: 7\n",
      "Loss: 1.149 | Acc: 56.000% (14/25)\n",
      "Loss: 1.125 | Acc: 58.000% (29/50)\n",
      "Loss: 0.977 | Acc: 64.000% (48/75)\n",
      "Loss: 1.018 | Acc: 62.000% (62/100)\n",
      "Epoch 7 of training is completed, Training accuracy for this epoch is 62.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.855 | Acc: 4.000% (1/25)\n",
      "Loss: 2.785 | Acc: 12.000% (6/50)\n",
      "Loss: 2.652 | Acc: 16.000% (12/75)\n",
      "Loss: 2.725 | Acc: 16.000% (16/100)\n",
      "Evaluation of Epoch 7 is completed, Validation accuracy for this epoch is 16.0\n",
      "\n",
      "Epoch: 8\n",
      "Loss: 0.885 | Acc: 64.000% (16/25)\n",
      "Loss: 0.756 | Acc: 74.000% (37/50)\n",
      "Loss: 0.755 | Acc: 76.000% (57/75)\n",
      "Loss: 0.795 | Acc: 75.000% (75/100)\n",
      "Epoch 8 of training is completed, Training accuracy for this epoch is 75.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.484 | Acc: 28.000% (7/25)\n",
      "Loss: 2.599 | Acc: 26.000% (13/50)\n",
      "Loss: 2.481 | Acc: 29.333% (22/75)\n",
      "Loss: 2.597 | Acc: 28.000% (28/100)\n",
      "Evaluation of Epoch 8 is completed, Validation accuracy for this epoch is 28.0\n",
      "\n",
      "Epoch: 9\n",
      "Loss: 0.654 | Acc: 84.000% (21/25)\n",
      "Loss: 0.665 | Acc: 78.000% (39/50)\n",
      "Loss: 0.751 | Acc: 78.667% (59/75)\n",
      "Loss: 0.666 | Acc: 82.000% (82/100)\n",
      "Epoch 9 of training is completed, Training accuracy for this epoch is 82.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.452 | Acc: 32.000% (8/25)\n",
      "Loss: 2.711 | Acc: 28.000% (14/50)\n",
      "Loss: 2.686 | Acc: 26.667% (20/75)\n",
      "Loss: 2.817 | Acc: 24.000% (24/100)\n",
      "Evaluation of Epoch 9 is completed, Validation accuracy for this epoch is 24.0\n",
      "\n",
      "Epoch: 10\n",
      "Loss: 0.357 | Acc: 96.000% (24/25)\n",
      "Loss: 0.397 | Acc: 94.000% (47/50)\n",
      "Loss: 0.443 | Acc: 89.333% (67/75)\n",
      "Loss: 0.503 | Acc: 87.000% (87/100)\n",
      "Epoch 10 of training is completed, Training accuracy for this epoch is 87.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.164 | Acc: 20.000% (5/25)\n",
      "Loss: 3.036 | Acc: 26.000% (13/50)\n",
      "Loss: 2.842 | Acc: 29.333% (22/75)\n",
      "Loss: 2.897 | Acc: 27.000% (27/100)\n",
      "Evaluation of Epoch 10 is completed, Validation accuracy for this epoch is 27.0\n",
      "\n",
      "Epoch: 11\n",
      "Loss: 0.378 | Acc: 92.000% (23/25)\n",
      "Loss: 0.286 | Acc: 96.000% (48/50)\n",
      "Loss: 0.355 | Acc: 92.000% (69/75)\n",
      "Loss: 0.367 | Acc: 91.000% (91/100)\n",
      "Epoch 11 of training is completed, Training accuracy for this epoch is 91.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.042 | Acc: 16.000% (4/25)\n",
      "Loss: 3.068 | Acc: 24.000% (12/50)\n",
      "Loss: 2.923 | Acc: 26.667% (20/75)\n",
      "Loss: 3.068 | Acc: 26.000% (26/100)\n",
      "Evaluation of Epoch 11 is completed, Validation accuracy for this epoch is 26.0\n",
      "\n",
      "Epoch: 12\n",
      "Loss: 0.225 | Acc: 96.000% (24/25)\n",
      "Loss: 0.175 | Acc: 98.000% (49/50)\n",
      "Loss: 0.179 | Acc: 96.000% (72/75)\n",
      "Loss: 0.194 | Acc: 97.000% (97/100)\n",
      "Epoch 12 of training is completed, Training accuracy for this epoch is 97.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.705 | Acc: 32.000% (8/25)\n",
      "Loss: 3.029 | Acc: 24.000% (12/50)\n",
      "Loss: 3.020 | Acc: 24.000% (18/75)\n",
      "Loss: 3.187 | Acc: 22.000% (22/100)\n",
      "Evaluation of Epoch 12 is completed, Validation accuracy for this epoch is 22.0\n",
      "\n",
      "Epoch: 13\n",
      "Loss: 0.149 | Acc: 100.000% (25/25)\n",
      "Loss: 0.142 | Acc: 100.000% (50/50)\n",
      "Loss: 0.129 | Acc: 100.000% (75/75)\n",
      "Loss: 0.119 | Acc: 100.000% (100/100)\n",
      "Epoch 13 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 2.915 | Acc: 28.000% (7/25)\n",
      "Loss: 3.099 | Acc: 28.000% (14/50)\n",
      "Loss: 2.997 | Acc: 29.333% (22/75)\n",
      "Loss: 3.151 | Acc: 26.000% (26/100)\n",
      "Evaluation of Epoch 13 is completed, Validation accuracy for this epoch is 26.0\n",
      "\n",
      "Epoch: 14\n",
      "Loss: 0.078 | Acc: 100.000% (25/25)\n",
      "Loss: 0.078 | Acc: 100.000% (50/50)\n",
      "Loss: 0.077 | Acc: 100.000% (75/75)\n",
      "Loss: 0.071 | Acc: 100.000% (100/100)\n",
      "Epoch 14 of training is completed, Training accuracy for this epoch is 100.0\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 3.145 | Acc: 24.000% (6/25)\n",
      "Loss: 3.249 | Acc: 28.000% (14/50)\n",
      "Loss: 3.118 | Acc: 28.000% (21/75)\n",
      "Loss: 3.266 | Acc: 25.000% (25/100)\n",
      "Evaluation of Epoch 14 is completed, Validation accuracy for this epoch is 25.0\n",
      "\n",
      "Final train set accuracy is 100.0\n",
      "Final val set accuracy is 25.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, \n",
    "              input_dims, \n",
    "              output_dims, \n",
    "              num_trans_layers, \n",
    "              num_heads, \n",
    "              image_k, \n",
    "              patch_k)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "eval_accs=[]\n",
    "for epoch in range(15):\n",
    "    tr_acc = train(network, optimizer, trainloader_sub)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    eval_acc = evaluate(network, valloader_sub)\n",
    "    print('Evaluation of Epoch {} is completed, Validation accuracy for this epoch is {}'\\\n",
    "              .format(epoch, eval_acc))  \n",
    "    tr_accs.append(tr_acc)\n",
    "    eval_accs.append(eval_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final val set accuracy is {}\".format(eval_accs[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the four-layer ViT network for three epochs, with untuned hyperparameters that are initialized as below,  you should achieve greater than 50% accuracy both on the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Loss: 3.637 | Acc: 6.250% (4/64)\n",
      "Loss: 4.643 | Acc: 8.594% (11/128)\n",
      "Loss: 4.635 | Acc: 9.375% (18/192)\n",
      "Loss: 4.477 | Acc: 9.766% (25/256)\n",
      "Loss: 4.268 | Acc: 11.875% (38/320)\n",
      "Loss: 4.235 | Acc: 12.760% (49/384)\n",
      "Loss: 4.031 | Acc: 14.955% (67/448)\n",
      "Loss: 3.853 | Acc: 15.430% (79/512)\n",
      "Loss: 3.717 | Acc: 16.146% (93/576)\n",
      "Loss: 3.592 | Acc: 16.875% (108/640)\n",
      "Loss: 3.508 | Acc: 16.903% (119/704)\n",
      "Loss: 3.397 | Acc: 17.708% (136/768)\n",
      "Loss: 3.304 | Acc: 17.788% (148/832)\n",
      "Loss: 3.225 | Acc: 17.746% (159/896)\n",
      "Loss: 3.167 | Acc: 18.333% (176/960)\n",
      "Loss: 3.125 | Acc: 18.750% (192/1024)\n",
      "Loss: 3.050 | Acc: 19.210% (209/1088)\n",
      "Loss: 2.995 | Acc: 19.358% (223/1152)\n",
      "Loss: 2.958 | Acc: 19.326% (235/1216)\n",
      "Loss: 2.921 | Acc: 19.766% (253/1280)\n",
      "Loss: 2.909 | Acc: 19.420% (261/1344)\n",
      "Loss: 2.878 | Acc: 19.673% (277/1408)\n",
      "Loss: 2.841 | Acc: 19.837% (292/1472)\n",
      "Loss: 2.823 | Acc: 19.596% (301/1536)\n",
      "Loss: 2.800 | Acc: 19.438% (311/1600)\n",
      "Loss: 2.781 | Acc: 19.231% (320/1664)\n",
      "Loss: 2.755 | Acc: 19.387% (335/1728)\n",
      "Loss: 2.730 | Acc: 19.587% (351/1792)\n",
      "Loss: 2.711 | Acc: 19.666% (365/1856)\n",
      "Loss: 2.691 | Acc: 20.000% (384/1920)\n",
      "Loss: 2.671 | Acc: 19.758% (392/1984)\n",
      "Loss: 2.657 | Acc: 19.873% (407/2048)\n",
      "Loss: 2.644 | Acc: 19.886% (420/2112)\n",
      "Loss: 2.628 | Acc: 19.899% (433/2176)\n",
      "Loss: 2.612 | Acc: 20.089% (450/2240)\n",
      "Loss: 2.596 | Acc: 20.399% (470/2304)\n",
      "Loss: 2.588 | Acc: 20.397% (483/2368)\n",
      "Loss: 2.570 | Acc: 20.641% (502/2432)\n",
      "Loss: 2.558 | Acc: 20.633% (515/2496)\n",
      "Loss: 2.541 | Acc: 20.703% (530/2560)\n",
      "Loss: 2.529 | Acc: 20.732% (544/2624)\n",
      "Loss: 2.519 | Acc: 20.573% (553/2688)\n",
      "Loss: 2.506 | Acc: 20.785% (572/2752)\n",
      "Loss: 2.495 | Acc: 20.774% (585/2816)\n",
      "Loss: 2.490 | Acc: 20.868% (601/2880)\n",
      "Loss: 2.479 | Acc: 20.958% (617/2944)\n",
      "Loss: 2.464 | Acc: 21.310% (641/3008)\n",
      "Loss: 2.459 | Acc: 21.419% (658/3072)\n",
      "Loss: 2.455 | Acc: 21.429% (672/3136)\n",
      "Loss: 2.450 | Acc: 21.562% (690/3200)\n",
      "Loss: 2.444 | Acc: 21.538% (703/3264)\n",
      "Loss: 2.430 | Acc: 21.815% (726/3328)\n",
      "Loss: 2.421 | Acc: 21.904% (743/3392)\n",
      "Loss: 2.412 | Acc: 22.049% (762/3456)\n",
      "Loss: 2.404 | Acc: 22.188% (781/3520)\n",
      "Loss: 2.397 | Acc: 22.349% (801/3584)\n",
      "Loss: 2.391 | Acc: 22.423% (818/3648)\n",
      "Loss: 2.382 | Acc: 22.602% (839/3712)\n",
      "Loss: 2.377 | Acc: 22.537% (851/3776)\n",
      "Loss: 2.371 | Acc: 22.656% (870/3840)\n",
      "Loss: 2.364 | Acc: 22.823% (891/3904)\n",
      "Loss: 2.356 | Acc: 23.034% (914/3968)\n",
      "Loss: 2.350 | Acc: 23.214% (936/4032)\n",
      "Loss: 2.346 | Acc: 23.193% (950/4096)\n",
      "Loss: 2.342 | Acc: 23.149% (963/4160)\n",
      "Loss: 2.334 | Acc: 23.248% (982/4224)\n",
      "Loss: 2.329 | Acc: 23.228% (996/4288)\n",
      "Loss: 2.325 | Acc: 23.300% (1014/4352)\n",
      "Loss: 2.318 | Acc: 23.392% (1033/4416)\n",
      "Loss: 2.312 | Acc: 23.460% (1051/4480)\n",
      "Loss: 2.306 | Acc: 23.548% (1070/4544)\n",
      "Loss: 2.303 | Acc: 23.568% (1086/4608)\n",
      "Loss: 2.299 | Acc: 23.566% (1101/4672)\n",
      "Loss: 2.295 | Acc: 23.522% (1114/4736)\n",
      "Loss: 2.289 | Acc: 23.750% (1140/4800)\n",
      "Loss: 2.283 | Acc: 24.034% (1169/4864)\n",
      "Loss: 2.281 | Acc: 24.026% (1184/4928)\n",
      "Loss: 2.279 | Acc: 24.018% (1199/4992)\n",
      "Loss: 2.275 | Acc: 24.051% (1216/5056)\n",
      "Loss: 2.271 | Acc: 24.102% (1234/5120)\n",
      "Loss: 2.266 | Acc: 24.113% (1250/5184)\n",
      "Loss: 2.261 | Acc: 24.123% (1266/5248)\n",
      "Loss: 2.258 | Acc: 24.059% (1278/5312)\n",
      "Loss: 2.254 | Acc: 24.070% (1294/5376)\n",
      "Loss: 2.250 | Acc: 24.210% (1317/5440)\n",
      "Loss: 2.244 | Acc: 24.346% (1340/5504)\n",
      "Loss: 2.239 | Acc: 24.353% (1356/5568)\n",
      "Loss: 2.234 | Acc: 24.538% (1382/5632)\n",
      "Loss: 2.231 | Acc: 24.614% (1402/5696)\n",
      "Loss: 2.228 | Acc: 24.653% (1420/5760)\n",
      "Loss: 2.223 | Acc: 24.725% (1440/5824)\n",
      "Loss: 2.217 | Acc: 24.881% (1465/5888)\n",
      "Loss: 2.214 | Acc: 24.950% (1485/5952)\n",
      "Loss: 2.209 | Acc: 25.150% (1513/6016)\n",
      "Loss: 2.206 | Acc: 25.181% (1531/6080)\n",
      "Loss: 2.201 | Acc: 25.212% (1549/6144)\n",
      "Loss: 2.196 | Acc: 25.370% (1575/6208)\n",
      "Loss: 2.192 | Acc: 25.446% (1596/6272)\n",
      "Loss: 2.184 | Acc: 25.679% (1627/6336)\n",
      "Loss: 2.179 | Acc: 25.766% (1649/6400)\n",
      "Loss: 2.175 | Acc: 25.835% (1670/6464)\n",
      "Loss: 2.172 | Acc: 25.888% (1690/6528)\n",
      "Loss: 2.169 | Acc: 25.956% (1711/6592)\n",
      "Loss: 2.167 | Acc: 25.962% (1728/6656)\n",
      "Loss: 2.163 | Acc: 26.071% (1752/6720)\n",
      "Loss: 2.161 | Acc: 26.047% (1767/6784)\n",
      "Loss: 2.157 | Acc: 26.154% (1791/6848)\n",
      "Loss: 2.155 | Acc: 26.273% (1816/6912)\n",
      "Loss: 2.151 | Acc: 26.304% (1835/6976)\n",
      "Loss: 2.149 | Acc: 26.378% (1857/7040)\n",
      "Loss: 2.147 | Acc: 26.408% (1876/7104)\n",
      "Loss: 2.143 | Acc: 26.465% (1897/7168)\n",
      "Loss: 2.139 | Acc: 26.604% (1924/7232)\n",
      "Loss: 2.136 | Acc: 26.617% (1942/7296)\n",
      "Loss: 2.132 | Acc: 26.712% (1966/7360)\n",
      "Loss: 2.127 | Acc: 26.805% (1990/7424)\n",
      "Loss: 2.123 | Acc: 26.950% (2018/7488)\n",
      "Loss: 2.122 | Acc: 26.960% (2036/7552)\n",
      "Loss: 2.118 | Acc: 27.101% (2064/7616)\n",
      "Loss: 2.116 | Acc: 27.109% (2082/7680)\n",
      "Loss: 2.113 | Acc: 27.169% (2104/7744)\n",
      "Loss: 2.112 | Acc: 27.203% (2124/7808)\n",
      "Loss: 2.112 | Acc: 27.134% (2136/7872)\n",
      "Loss: 2.109 | Acc: 27.268% (2164/7936)\n",
      "Loss: 2.107 | Acc: 27.262% (2181/8000)\n",
      "Loss: 2.104 | Acc: 27.344% (2205/8064)\n",
      "Loss: 2.103 | Acc: 27.362% (2224/8128)\n",
      "Loss: 2.098 | Acc: 27.563% (2258/8192)\n",
      "Loss: 2.095 | Acc: 27.677% (2285/8256)\n",
      "Loss: 2.094 | Acc: 27.704% (2305/8320)\n",
      "Loss: 2.089 | Acc: 27.803% (2331/8384)\n",
      "Loss: 2.086 | Acc: 27.853% (2353/8448)\n",
      "Loss: 2.085 | Acc: 27.867% (2372/8512)\n",
      "Loss: 2.082 | Acc: 27.950% (2397/8576)\n",
      "Loss: 2.079 | Acc: 28.056% (2424/8640)\n",
      "Loss: 2.077 | Acc: 28.125% (2448/8704)\n",
      "Loss: 2.075 | Acc: 28.193% (2472/8768)\n",
      "Loss: 2.073 | Acc: 28.317% (2501/8832)\n",
      "Loss: 2.071 | Acc: 28.372% (2524/8896)\n",
      "Loss: 2.068 | Acc: 28.426% (2547/8960)\n",
      "Loss: 2.067 | Acc: 28.446% (2567/9024)\n",
      "Loss: 2.065 | Acc: 28.499% (2590/9088)\n",
      "Loss: 2.061 | Acc: 28.617% (2619/9152)\n",
      "Loss: 2.060 | Acc: 28.646% (2640/9216)\n",
      "Loss: 2.057 | Acc: 28.739% (2667/9280)\n",
      "Loss: 2.056 | Acc: 28.789% (2690/9344)\n",
      "Loss: 2.053 | Acc: 28.858% (2715/9408)\n",
      "Loss: 2.051 | Acc: 28.864% (2734/9472)\n",
      "Loss: 2.049 | Acc: 28.891% (2755/9536)\n",
      "Loss: 2.049 | Acc: 28.875% (2772/9600)\n",
      "Loss: 2.048 | Acc: 28.953% (2798/9664)\n",
      "Loss: 2.045 | Acc: 29.040% (2825/9728)\n",
      "Loss: 2.044 | Acc: 29.003% (2840/9792)\n",
      "Loss: 2.041 | Acc: 29.109% (2869/9856)\n",
      "Loss: 2.038 | Acc: 29.224% (2899/9920)\n",
      "Loss: 2.037 | Acc: 29.237% (2919/9984)\n",
      "Loss: 2.035 | Acc: 29.319% (2946/10048)\n",
      "Loss: 2.033 | Acc: 29.341% (2967/10112)\n",
      "Loss: 2.031 | Acc: 29.432% (2995/10176)\n",
      "Loss: 2.030 | Acc: 29.453% (3016/10240)\n",
      "Loss: 2.026 | Acc: 29.571% (3047/10304)\n",
      "Loss: 2.025 | Acc: 29.543% (3063/10368)\n",
      "Loss: 2.023 | Acc: 29.592% (3087/10432)\n",
      "Loss: 2.021 | Acc: 29.640% (3111/10496)\n",
      "Loss: 2.019 | Acc: 29.706% (3137/10560)\n",
      "Loss: 2.017 | Acc: 29.791% (3165/10624)\n",
      "Loss: 2.015 | Acc: 29.865% (3192/10688)\n",
      "Loss: 2.014 | Acc: 29.911% (3216/10752)\n",
      "Loss: 2.012 | Acc: 29.937% (3238/10816)\n",
      "Loss: 2.010 | Acc: 29.982% (3262/10880)\n",
      "Loss: 2.010 | Acc: 29.989% (3282/10944)\n",
      "Loss: 2.007 | Acc: 30.051% (3308/11008)\n",
      "Loss: 2.005 | Acc: 30.157% (3339/11072)\n",
      "Loss: 2.003 | Acc: 30.244% (3368/11136)\n",
      "Loss: 2.001 | Acc: 30.268% (3390/11200)\n",
      "Loss: 1.998 | Acc: 30.336% (3417/11264)\n",
      "Loss: 1.996 | Acc: 30.332% (3436/11328)\n",
      "Loss: 1.995 | Acc: 30.390% (3462/11392)\n",
      "Loss: 1.993 | Acc: 30.447% (3488/11456)\n",
      "Loss: 1.991 | Acc: 30.512% (3515/11520)\n",
      "Loss: 1.989 | Acc: 30.594% (3544/11584)\n",
      "Loss: 1.988 | Acc: 30.666% (3572/11648)\n",
      "Loss: 1.986 | Acc: 30.738% (3600/11712)\n",
      "Loss: 1.983 | Acc: 30.766% (3623/11776)\n",
      "Loss: 1.982 | Acc: 30.769% (3643/11840)\n",
      "Loss: 1.981 | Acc: 30.771% (3663/11904)\n",
      "Loss: 1.980 | Acc: 30.757% (3681/11968)\n",
      "Loss: 1.978 | Acc: 30.768% (3702/12032)\n",
      "Loss: 1.976 | Acc: 30.804% (3726/12096)\n",
      "Loss: 1.974 | Acc: 30.872% (3754/12160)\n",
      "Loss: 1.972 | Acc: 30.906% (3778/12224)\n",
      "Loss: 1.972 | Acc: 30.941% (3802/12288)\n",
      "Loss: 1.971 | Acc: 30.991% (3828/12352)\n",
      "Loss: 1.969 | Acc: 31.041% (3854/12416)\n",
      "Loss: 1.969 | Acc: 31.042% (3874/12480)\n",
      "Loss: 1.968 | Acc: 31.059% (3896/12544)\n",
      "Loss: 1.967 | Acc: 31.083% (3919/12608)\n",
      "Loss: 1.965 | Acc: 31.147% (3947/12672)\n",
      "Loss: 1.963 | Acc: 31.164% (3969/12736)\n",
      "Loss: 1.962 | Acc: 31.203% (3994/12800)\n",
      "Loss: 1.961 | Acc: 31.211% (4015/12864)\n",
      "Loss: 1.962 | Acc: 31.165% (4029/12928)\n",
      "Loss: 1.960 | Acc: 31.235% (4058/12992)\n",
      "Loss: 1.958 | Acc: 31.258% (4081/13056)\n",
      "Loss: 1.958 | Acc: 31.258% (4101/13120)\n",
      "Loss: 1.957 | Acc: 31.311% (4128/13184)\n",
      "Loss: 1.955 | Acc: 31.356% (4154/13248)\n",
      "Loss: 1.954 | Acc: 31.370% (4176/13312)\n",
      "Loss: 1.953 | Acc: 31.437% (4205/13376)\n",
      "Loss: 1.952 | Acc: 31.436% (4225/13440)\n",
      "Loss: 1.950 | Acc: 31.539% (4259/13504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.949 | Acc: 31.552% (4281/13568)\n",
      "Loss: 1.947 | Acc: 31.609% (4309/13632)\n",
      "Loss: 1.946 | Acc: 31.600% (4328/13696)\n",
      "Loss: 1.945 | Acc: 31.584% (4346/13760)\n",
      "Loss: 1.944 | Acc: 31.619% (4371/13824)\n",
      "Loss: 1.942 | Acc: 31.675% (4399/13888)\n",
      "Loss: 1.941 | Acc: 31.702% (4423/13952)\n",
      "Loss: 1.941 | Acc: 31.685% (4441/14016)\n",
      "Loss: 1.939 | Acc: 31.697% (4463/14080)\n",
      "Loss: 1.938 | Acc: 31.745% (4490/14144)\n",
      "Loss: 1.938 | Acc: 31.750% (4511/14208)\n",
      "Loss: 1.937 | Acc: 31.740% (4530/14272)\n",
      "Loss: 1.937 | Acc: 31.759% (4553/14336)\n",
      "Loss: 1.936 | Acc: 31.764% (4574/14400)\n",
      "Loss: 1.934 | Acc: 31.775% (4596/14464)\n",
      "Loss: 1.933 | Acc: 31.849% (4627/14528)\n",
      "Loss: 1.931 | Acc: 31.860% (4649/14592)\n",
      "Loss: 1.930 | Acc: 31.871% (4671/14656)\n",
      "Loss: 1.929 | Acc: 31.902% (4696/14720)\n",
      "Loss: 1.929 | Acc: 31.893% (4715/14784)\n",
      "Loss: 1.927 | Acc: 31.903% (4737/14848)\n",
      "Loss: 1.926 | Acc: 31.934% (4762/14912)\n",
      "Loss: 1.925 | Acc: 31.978% (4789/14976)\n",
      "Loss: 1.924 | Acc: 32.035% (4818/15040)\n",
      "Loss: 1.923 | Acc: 32.044% (4840/15104)\n",
      "Loss: 1.921 | Acc: 32.054% (4862/15168)\n",
      "Loss: 1.921 | Acc: 32.077% (4886/15232)\n",
      "Loss: 1.921 | Acc: 32.074% (4906/15296)\n",
      "Loss: 1.920 | Acc: 32.090% (4929/15360)\n",
      "Loss: 1.918 | Acc: 32.099% (4951/15424)\n",
      "Loss: 1.916 | Acc: 32.173% (4983/15488)\n",
      "Loss: 1.916 | Acc: 32.202% (5008/15552)\n",
      "Loss: 1.916 | Acc: 32.204% (5029/15616)\n",
      "Loss: 1.915 | Acc: 32.245% (5056/15680)\n",
      "Loss: 1.913 | Acc: 32.285% (5083/15744)\n",
      "Loss: 1.910 | Acc: 32.382% (5119/15808)\n",
      "Loss: 1.910 | Acc: 32.447% (5150/15872)\n",
      "Loss: 1.909 | Acc: 32.455% (5172/15936)\n",
      "Loss: 1.909 | Acc: 32.456% (5193/16000)\n",
      "Loss: 1.907 | Acc: 32.514% (5223/16064)\n",
      "Loss: 1.906 | Acc: 32.564% (5252/16128)\n",
      "Loss: 1.905 | Acc: 32.621% (5282/16192)\n",
      "Loss: 1.903 | Acc: 32.689% (5314/16256)\n",
      "Loss: 1.902 | Acc: 32.702% (5337/16320)\n",
      "Loss: 1.901 | Acc: 32.721% (5361/16384)\n",
      "Loss: 1.901 | Acc: 32.727% (5383/16448)\n",
      "Loss: 1.899 | Acc: 32.758% (5409/16512)\n",
      "Loss: 1.897 | Acc: 32.849% (5445/16576)\n",
      "Loss: 1.897 | Acc: 32.855% (5467/16640)\n",
      "Loss: 1.896 | Acc: 32.872% (5491/16704)\n",
      "Loss: 1.897 | Acc: 32.872% (5512/16768)\n",
      "Loss: 1.895 | Acc: 32.919% (5541/16832)\n",
      "Loss: 1.894 | Acc: 32.972% (5571/16896)\n",
      "Loss: 1.893 | Acc: 32.995% (5596/16960)\n",
      "Loss: 1.893 | Acc: 33.000% (5618/17024)\n",
      "Loss: 1.892 | Acc: 33.011% (5641/17088)\n",
      "Loss: 1.892 | Acc: 33.040% (5667/17152)\n",
      "Loss: 1.892 | Acc: 33.068% (5693/17216)\n",
      "Loss: 1.891 | Acc: 33.084% (5717/17280)\n",
      "Loss: 1.891 | Acc: 33.089% (5739/17344)\n",
      "Loss: 1.890 | Acc: 33.105% (5763/17408)\n",
      "Loss: 1.890 | Acc: 33.110% (5785/17472)\n",
      "Loss: 1.889 | Acc: 33.177% (5818/17536)\n",
      "Loss: 1.889 | Acc: 33.188% (5841/17600)\n",
      "Loss: 1.888 | Acc: 33.197% (5864/17664)\n",
      "Loss: 1.887 | Acc: 33.179% (5882/17728)\n",
      "Loss: 1.887 | Acc: 33.195% (5906/17792)\n",
      "Loss: 1.885 | Acc: 33.216% (5931/17856)\n",
      "Loss: 1.884 | Acc: 33.225% (5954/17920)\n",
      "Loss: 1.884 | Acc: 33.268% (5983/17984)\n",
      "Loss: 1.883 | Acc: 33.272% (6005/18048)\n",
      "Loss: 1.883 | Acc: 33.287% (6029/18112)\n",
      "Loss: 1.881 | Acc: 33.346% (6061/18176)\n",
      "Loss: 1.881 | Acc: 33.344% (6082/18240)\n",
      "Loss: 1.880 | Acc: 33.348% (6104/18304)\n",
      "Loss: 1.879 | Acc: 33.373% (6130/18368)\n",
      "Loss: 1.878 | Acc: 33.420% (6160/18432)\n",
      "Loss: 1.876 | Acc: 33.445% (6186/18496)\n",
      "Loss: 1.876 | Acc: 33.459% (6210/18560)\n",
      "Loss: 1.876 | Acc: 33.451% (6230/18624)\n",
      "Loss: 1.876 | Acc: 33.439% (6249/18688)\n",
      "Loss: 1.875 | Acc: 33.442% (6271/18752)\n",
      "Loss: 1.874 | Acc: 33.493% (6302/18816)\n",
      "Loss: 1.873 | Acc: 33.490% (6323/18880)\n",
      "Loss: 1.873 | Acc: 33.499% (6346/18944)\n",
      "Loss: 1.872 | Acc: 33.533% (6374/19008)\n",
      "Loss: 1.872 | Acc: 33.557% (6400/19072)\n",
      "Loss: 1.872 | Acc: 33.575% (6425/19136)\n",
      "Loss: 1.871 | Acc: 33.620% (6455/19200)\n",
      "Loss: 1.871 | Acc: 33.622% (6477/19264)\n",
      "Loss: 1.870 | Acc: 33.630% (6500/19328)\n",
      "Loss: 1.870 | Acc: 33.617% (6519/19392)\n",
      "Loss: 1.869 | Acc: 33.619% (6541/19456)\n",
      "Loss: 1.868 | Acc: 33.658% (6570/19520)\n",
      "Loss: 1.867 | Acc: 33.691% (6598/19584)\n",
      "Loss: 1.866 | Acc: 33.744% (6630/19648)\n",
      "Loss: 1.865 | Acc: 33.776% (6658/19712)\n",
      "Loss: 1.865 | Acc: 33.799% (6684/19776)\n",
      "Loss: 1.864 | Acc: 33.805% (6707/19840)\n",
      "Loss: 1.863 | Acc: 33.837% (6735/19904)\n",
      "Loss: 1.863 | Acc: 33.839% (6757/19968)\n",
      "Loss: 1.862 | Acc: 33.856% (6782/20032)\n",
      "Loss: 1.862 | Acc: 33.872% (6807/20096)\n",
      "Loss: 1.861 | Acc: 33.924% (6839/20160)\n",
      "Loss: 1.860 | Acc: 33.965% (6869/20224)\n",
      "Loss: 1.859 | Acc: 33.995% (6897/20288)\n",
      "Loss: 1.858 | Acc: 34.041% (6928/20352)\n",
      "Loss: 1.857 | Acc: 34.057% (6953/20416)\n",
      "Loss: 1.856 | Acc: 34.077% (6979/20480)\n",
      "Loss: 1.855 | Acc: 34.102% (7006/20544)\n",
      "Loss: 1.855 | Acc: 34.098% (7027/20608)\n",
      "Loss: 1.854 | Acc: 34.114% (7052/20672)\n",
      "Loss: 1.853 | Acc: 34.153% (7082/20736)\n",
      "Loss: 1.853 | Acc: 34.159% (7105/20800)\n",
      "Loss: 1.852 | Acc: 34.188% (7133/20864)\n",
      "Loss: 1.851 | Acc: 34.251% (7168/20928)\n",
      "Loss: 1.850 | Acc: 34.261% (7192/20992)\n",
      "Loss: 1.850 | Acc: 34.252% (7212/21056)\n",
      "Loss: 1.849 | Acc: 34.290% (7242/21120)\n",
      "Loss: 1.848 | Acc: 34.323% (7271/21184)\n",
      "Loss: 1.847 | Acc: 34.337% (7296/21248)\n",
      "Loss: 1.846 | Acc: 34.366% (7324/21312)\n",
      "Loss: 1.846 | Acc: 34.380% (7349/21376)\n",
      "Loss: 1.845 | Acc: 34.422% (7380/21440)\n",
      "Loss: 1.844 | Acc: 34.445% (7407/21504)\n",
      "Loss: 1.843 | Acc: 34.468% (7434/21568)\n",
      "Loss: 1.842 | Acc: 34.454% (7453/21632)\n",
      "Loss: 1.842 | Acc: 34.440% (7472/21696)\n",
      "Loss: 1.842 | Acc: 34.439% (7494/21760)\n",
      "Loss: 1.841 | Acc: 34.471% (7523/21824)\n",
      "Loss: 1.841 | Acc: 34.489% (7549/21888)\n",
      "Loss: 1.839 | Acc: 34.507% (7575/21952)\n",
      "Loss: 1.838 | Acc: 34.548% (7606/22016)\n",
      "Loss: 1.838 | Acc: 34.556% (7630/22080)\n",
      "Loss: 1.838 | Acc: 34.551% (7651/22144)\n",
      "Loss: 1.838 | Acc: 34.573% (7678/22208)\n",
      "Loss: 1.838 | Acc: 34.573% (7700/22272)\n",
      "Loss: 1.837 | Acc: 34.585% (7725/22336)\n",
      "Loss: 1.836 | Acc: 34.612% (7753/22400)\n",
      "Loss: 1.836 | Acc: 34.606% (7774/22464)\n",
      "Loss: 1.836 | Acc: 34.601% (7795/22528)\n",
      "Loss: 1.836 | Acc: 34.614% (7820/22592)\n",
      "Loss: 1.835 | Acc: 34.618% (7843/22656)\n",
      "Loss: 1.835 | Acc: 34.648% (7872/22720)\n",
      "Loss: 1.834 | Acc: 34.682% (7902/22784)\n",
      "Loss: 1.834 | Acc: 34.703% (7929/22848)\n",
      "Loss: 1.833 | Acc: 34.707% (7952/22912)\n",
      "Loss: 1.833 | Acc: 34.723% (7978/22976)\n",
      "Loss: 1.833 | Acc: 34.735% (8003/23040)\n",
      "Loss: 1.832 | Acc: 34.747% (8028/23104)\n",
      "Loss: 1.832 | Acc: 34.772% (8056/23168)\n",
      "Loss: 1.831 | Acc: 34.762% (8076/23232)\n",
      "Loss: 1.831 | Acc: 34.787% (8104/23296)\n",
      "Loss: 1.831 | Acc: 34.786% (8126/23360)\n",
      "Loss: 1.830 | Acc: 34.815% (8155/23424)\n",
      "Loss: 1.830 | Acc: 34.843% (8184/23488)\n",
      "Loss: 1.830 | Acc: 34.851% (8208/23552)\n",
      "Loss: 1.829 | Acc: 34.887% (8239/23616)\n",
      "Loss: 1.828 | Acc: 34.903% (8265/23680)\n",
      "Loss: 1.827 | Acc: 34.952% (8299/23744)\n",
      "Loss: 1.826 | Acc: 34.959% (8323/23808)\n",
      "Loss: 1.825 | Acc: 34.999% (8355/23872)\n",
      "Loss: 1.824 | Acc: 35.039% (8387/23936)\n",
      "Loss: 1.824 | Acc: 35.046% (8411/24000)\n",
      "Loss: 1.823 | Acc: 35.065% (8438/24064)\n",
      "Loss: 1.823 | Acc: 35.067% (8461/24128)\n",
      "Loss: 1.822 | Acc: 35.098% (8491/24192)\n",
      "Loss: 1.822 | Acc: 35.101% (8514/24256)\n",
      "Loss: 1.821 | Acc: 35.107% (8538/24320)\n",
      "Loss: 1.820 | Acc: 35.167% (8575/24384)\n",
      "Loss: 1.819 | Acc: 35.218% (8610/24448)\n",
      "Loss: 1.818 | Acc: 35.228% (8635/24512)\n",
      "Loss: 1.818 | Acc: 35.234% (8659/24576)\n",
      "Loss: 1.817 | Acc: 35.276% (8692/24640)\n",
      "Loss: 1.816 | Acc: 35.294% (8719/24704)\n",
      "Loss: 1.816 | Acc: 35.316% (8747/24768)\n",
      "Loss: 1.815 | Acc: 35.321% (8771/24832)\n",
      "Loss: 1.814 | Acc: 35.331% (8796/24896)\n",
      "Loss: 1.814 | Acc: 35.361% (8826/24960)\n",
      "Loss: 1.813 | Acc: 35.386% (8855/25024)\n",
      "Loss: 1.813 | Acc: 35.375% (8875/25088)\n",
      "Loss: 1.813 | Acc: 35.377% (8898/25152)\n",
      "Loss: 1.813 | Acc: 35.374% (8920/25216)\n",
      "Loss: 1.812 | Acc: 35.380% (8944/25280)\n",
      "Loss: 1.812 | Acc: 35.401% (8972/25344)\n",
      "Loss: 1.811 | Acc: 35.426% (9001/25408)\n",
      "Loss: 1.811 | Acc: 35.439% (9027/25472)\n",
      "Loss: 1.810 | Acc: 35.452% (9053/25536)\n",
      "Loss: 1.810 | Acc: 35.469% (9080/25600)\n",
      "Loss: 1.809 | Acc: 35.478% (9105/25664)\n",
      "Loss: 1.809 | Acc: 35.498% (9133/25728)\n",
      "Loss: 1.808 | Acc: 35.511% (9159/25792)\n",
      "Loss: 1.807 | Acc: 35.535% (9188/25856)\n",
      "Loss: 1.807 | Acc: 35.567% (9219/25920)\n",
      "Loss: 1.806 | Acc: 35.580% (9245/25984)\n",
      "Loss: 1.805 | Acc: 35.592% (9271/26048)\n",
      "Loss: 1.805 | Acc: 35.597% (9295/26112)\n",
      "Loss: 1.805 | Acc: 35.594% (9317/26176)\n",
      "Loss: 1.804 | Acc: 35.621% (9347/26240)\n",
      "Loss: 1.804 | Acc: 35.618% (9369/26304)\n",
      "Loss: 1.803 | Acc: 35.661% (9403/26368)\n",
      "Loss: 1.802 | Acc: 35.680% (9431/26432)\n",
      "Loss: 1.802 | Acc: 35.722% (9465/26496)\n",
      "Loss: 1.801 | Acc: 35.757% (9497/26560)\n",
      "Loss: 1.800 | Acc: 35.776% (9525/26624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.800 | Acc: 35.795% (9553/26688)\n",
      "Loss: 1.800 | Acc: 35.810% (9580/26752)\n",
      "Loss: 1.800 | Acc: 35.833% (9609/26816)\n",
      "Loss: 1.799 | Acc: 35.848% (9636/26880)\n",
      "Loss: 1.798 | Acc: 35.878% (9667/26944)\n",
      "Loss: 1.798 | Acc: 35.889% (9693/27008)\n",
      "Loss: 1.797 | Acc: 35.915% (9723/27072)\n",
      "Loss: 1.796 | Acc: 35.952% (9756/27136)\n",
      "Loss: 1.795 | Acc: 35.978% (9786/27200)\n",
      "Loss: 1.795 | Acc: 35.989% (9812/27264)\n",
      "Loss: 1.795 | Acc: 36.000% (9838/27328)\n",
      "Loss: 1.794 | Acc: 35.996% (9860/27392)\n",
      "Loss: 1.794 | Acc: 36.007% (9886/27456)\n",
      "Loss: 1.793 | Acc: 36.017% (9912/27520)\n",
      "Loss: 1.793 | Acc: 36.032% (9939/27584)\n",
      "Loss: 1.792 | Acc: 36.060% (9970/27648)\n",
      "Loss: 1.791 | Acc: 36.089% (10001/27712)\n",
      "Loss: 1.791 | Acc: 36.100% (10027/27776)\n",
      "Loss: 1.790 | Acc: 36.106% (10052/27840)\n",
      "Loss: 1.790 | Acc: 36.127% (10081/27904)\n",
      "Loss: 1.789 | Acc: 36.152% (10111/27968)\n",
      "Loss: 1.788 | Acc: 36.180% (10142/28032)\n",
      "Loss: 1.788 | Acc: 36.201% (10171/28096)\n",
      "Loss: 1.787 | Acc: 36.229% (10202/28160)\n",
      "Loss: 1.787 | Acc: 36.246% (10230/28224)\n",
      "Loss: 1.786 | Acc: 36.266% (10259/28288)\n",
      "Loss: 1.785 | Acc: 36.290% (10289/28352)\n",
      "Loss: 1.785 | Acc: 36.303% (10316/28416)\n",
      "Loss: 1.784 | Acc: 36.320% (10344/28480)\n",
      "Loss: 1.783 | Acc: 36.358% (10378/28544)\n",
      "Loss: 1.783 | Acc: 36.364% (10403/28608)\n",
      "Loss: 1.782 | Acc: 36.370% (10428/28672)\n",
      "Loss: 1.781 | Acc: 36.411% (10463/28736)\n",
      "Loss: 1.780 | Acc: 36.448% (10497/28800)\n",
      "Loss: 1.779 | Acc: 36.478% (10529/28864)\n",
      "Loss: 1.779 | Acc: 36.477% (10552/28928)\n",
      "Loss: 1.779 | Acc: 36.489% (10579/28992)\n",
      "Loss: 1.778 | Acc: 36.526% (10613/29056)\n",
      "Loss: 1.777 | Acc: 36.542% (10641/29120)\n",
      "Loss: 1.776 | Acc: 36.571% (10673/29184)\n",
      "Loss: 1.776 | Acc: 36.577% (10698/29248)\n",
      "Loss: 1.775 | Acc: 36.572% (10720/29312)\n",
      "Loss: 1.775 | Acc: 36.601% (10752/29376)\n",
      "Loss: 1.775 | Acc: 36.586% (10771/29440)\n",
      "Loss: 1.775 | Acc: 36.588% (10795/29504)\n",
      "Loss: 1.774 | Acc: 36.597% (10821/29568)\n",
      "Loss: 1.774 | Acc: 36.619% (10851/29632)\n",
      "Loss: 1.773 | Acc: 36.641% (10881/29696)\n",
      "Loss: 1.773 | Acc: 36.647% (10906/29760)\n",
      "Loss: 1.773 | Acc: 36.662% (10934/29824)\n",
      "Loss: 1.772 | Acc: 36.707% (10971/29888)\n",
      "Loss: 1.772 | Acc: 36.709% (10995/29952)\n",
      "Loss: 1.771 | Acc: 36.714% (11020/30016)\n",
      "Loss: 1.771 | Acc: 36.729% (11048/30080)\n",
      "Loss: 1.770 | Acc: 36.760% (11081/30144)\n",
      "Loss: 1.770 | Acc: 36.782% (11111/30208)\n",
      "Loss: 1.769 | Acc: 36.800% (11140/30272)\n",
      "Loss: 1.768 | Acc: 36.814% (11168/30336)\n",
      "Loss: 1.768 | Acc: 36.832% (11197/30400)\n",
      "Loss: 1.767 | Acc: 36.863% (11230/30464)\n",
      "Loss: 1.767 | Acc: 36.894% (11263/30528)\n",
      "Loss: 1.767 | Acc: 36.902% (11289/30592)\n",
      "Loss: 1.766 | Acc: 36.916% (11317/30656)\n",
      "Loss: 1.766 | Acc: 36.927% (11344/30720)\n",
      "Loss: 1.765 | Acc: 36.954% (11376/30784)\n",
      "Loss: 1.765 | Acc: 36.968% (11404/30848)\n",
      "Loss: 1.764 | Acc: 36.976% (11430/30912)\n",
      "Loss: 1.764 | Acc: 37.006% (11463/30976)\n",
      "Loss: 1.764 | Acc: 36.997% (11484/31040)\n",
      "Loss: 1.763 | Acc: 36.998% (11508/31104)\n",
      "Loss: 1.763 | Acc: 37.012% (11536/31168)\n",
      "Loss: 1.763 | Acc: 37.045% (11570/31232)\n",
      "Loss: 1.762 | Acc: 37.069% (11601/31296)\n",
      "Loss: 1.761 | Acc: 37.076% (11627/31360)\n",
      "Loss: 1.761 | Acc: 37.102% (11659/31424)\n",
      "Loss: 1.760 | Acc: 37.128% (11691/31488)\n",
      "Loss: 1.760 | Acc: 37.142% (11719/31552)\n",
      "Loss: 1.760 | Acc: 37.158% (11748/31616)\n",
      "Loss: 1.759 | Acc: 37.175% (11777/31680)\n",
      "Loss: 1.759 | Acc: 37.169% (11799/31744)\n",
      "Loss: 1.758 | Acc: 37.189% (11829/31808)\n",
      "Loss: 1.758 | Acc: 37.174% (11848/31872)\n",
      "Loss: 1.758 | Acc: 37.199% (11880/31936)\n",
      "Loss: 1.758 | Acc: 37.225% (11912/32000)\n",
      "Loss: 1.757 | Acc: 37.238% (11940/32064)\n",
      "Loss: 1.757 | Acc: 37.245% (11966/32128)\n",
      "Loss: 1.756 | Acc: 37.264% (11996/32192)\n",
      "Loss: 1.756 | Acc: 37.286% (12027/32256)\n",
      "Loss: 1.755 | Acc: 37.308% (12058/32320)\n",
      "Loss: 1.755 | Acc: 37.299% (12079/32384)\n",
      "Loss: 1.755 | Acc: 37.303% (12104/32448)\n",
      "Loss: 1.754 | Acc: 37.306% (12129/32512)\n",
      "Loss: 1.754 | Acc: 37.319% (12157/32576)\n",
      "Loss: 1.754 | Acc: 37.325% (12183/32640)\n",
      "Loss: 1.753 | Acc: 37.329% (12208/32704)\n",
      "Loss: 1.753 | Acc: 37.350% (12239/32768)\n",
      "Loss: 1.753 | Acc: 37.342% (12260/32832)\n",
      "Loss: 1.752 | Acc: 37.363% (12291/32896)\n",
      "Loss: 1.752 | Acc: 37.348% (12310/32960)\n",
      "Loss: 1.752 | Acc: 37.361% (12338/33024)\n",
      "Loss: 1.751 | Acc: 37.391% (12372/33088)\n",
      "Loss: 1.750 | Acc: 37.406% (12401/33152)\n",
      "Loss: 1.750 | Acc: 37.431% (12433/33216)\n",
      "Loss: 1.750 | Acc: 37.434% (12458/33280)\n",
      "Loss: 1.749 | Acc: 37.455% (12489/33344)\n",
      "Loss: 1.749 | Acc: 37.467% (12517/33408)\n",
      "Loss: 1.749 | Acc: 37.467% (12541/33472)\n",
      "Loss: 1.748 | Acc: 37.470% (12566/33536)\n",
      "Loss: 1.748 | Acc: 37.491% (12597/33600)\n",
      "Loss: 1.748 | Acc: 37.488% (12620/33664)\n",
      "Loss: 1.747 | Acc: 37.473% (12639/33728)\n",
      "Loss: 1.747 | Acc: 37.500% (12672/33792)\n",
      "Loss: 1.747 | Acc: 37.500% (12696/33856)\n",
      "Loss: 1.746 | Acc: 37.503% (12721/33920)\n",
      "Loss: 1.746 | Acc: 37.518% (12750/33984)\n",
      "Loss: 1.746 | Acc: 37.521% (12775/34048)\n",
      "Loss: 1.745 | Acc: 37.526% (12801/34112)\n",
      "Loss: 1.745 | Acc: 37.515% (12821/34176)\n",
      "Loss: 1.745 | Acc: 37.532% (12851/34240)\n",
      "Loss: 1.744 | Acc: 37.544% (12879/34304)\n",
      "Loss: 1.744 | Acc: 37.541% (12902/34368)\n",
      "Loss: 1.743 | Acc: 37.570% (12936/34432)\n",
      "Loss: 1.743 | Acc: 37.572% (12961/34496)\n",
      "Loss: 1.743 | Acc: 37.564% (12982/34560)\n",
      "Loss: 1.742 | Acc: 37.584% (13013/34624)\n",
      "Loss: 1.742 | Acc: 37.601% (13043/34688)\n",
      "Loss: 1.741 | Acc: 37.609% (13070/34752)\n",
      "Loss: 1.741 | Acc: 37.626% (13100/34816)\n",
      "Loss: 1.741 | Acc: 37.643% (13130/34880)\n",
      "Loss: 1.741 | Acc: 37.663% (13161/34944)\n",
      "Loss: 1.740 | Acc: 37.674% (13189/35008)\n",
      "Loss: 1.740 | Acc: 37.677% (13214/35072)\n",
      "Loss: 1.739 | Acc: 37.682% (13240/35136)\n",
      "Loss: 1.739 | Acc: 37.696% (13269/35200)\n",
      "Loss: 1.739 | Acc: 37.704% (13296/35264)\n",
      "Loss: 1.738 | Acc: 37.738% (13332/35328)\n",
      "Loss: 1.737 | Acc: 37.766% (13366/35392)\n",
      "Loss: 1.737 | Acc: 37.776% (13394/35456)\n",
      "Loss: 1.736 | Acc: 37.796% (13425/35520)\n",
      "Loss: 1.736 | Acc: 37.809% (13454/35584)\n",
      "Loss: 1.736 | Acc: 37.831% (13486/35648)\n",
      "Loss: 1.735 | Acc: 37.864% (13522/35712)\n",
      "Loss: 1.735 | Acc: 37.863% (13546/35776)\n",
      "Loss: 1.735 | Acc: 37.863% (13570/35840)\n",
      "Loss: 1.734 | Acc: 37.876% (13599/35904)\n",
      "Loss: 1.734 | Acc: 37.895% (13630/35968)\n",
      "Loss: 1.733 | Acc: 37.914% (13661/36032)\n",
      "Loss: 1.733 | Acc: 37.927% (13690/36096)\n",
      "Loss: 1.732 | Acc: 37.937% (13718/36160)\n",
      "Loss: 1.731 | Acc: 37.955% (13749/36224)\n",
      "Loss: 1.731 | Acc: 37.968% (13778/36288)\n",
      "Loss: 1.731 | Acc: 37.981% (13807/36352)\n",
      "Loss: 1.730 | Acc: 37.992% (13835/36416)\n",
      "Loss: 1.729 | Acc: 38.018% (13869/36480)\n",
      "Loss: 1.729 | Acc: 38.028% (13897/36544)\n",
      "Loss: 1.728 | Acc: 38.046% (13928/36608)\n",
      "Loss: 1.728 | Acc: 38.075% (13963/36672)\n",
      "Loss: 1.728 | Acc: 38.074% (13987/36736)\n",
      "Loss: 1.727 | Acc: 38.090% (14017/36800)\n",
      "Loss: 1.727 | Acc: 38.110% (14049/36864)\n",
      "Loss: 1.726 | Acc: 38.120% (14077/36928)\n",
      "Loss: 1.726 | Acc: 38.138% (14108/36992)\n",
      "Loss: 1.725 | Acc: 38.169% (14144/37056)\n",
      "Loss: 1.725 | Acc: 38.195% (14178/37120)\n",
      "Loss: 1.724 | Acc: 38.199% (14204/37184)\n",
      "Loss: 1.724 | Acc: 38.222% (14237/37248)\n",
      "Loss: 1.724 | Acc: 38.234% (14266/37312)\n",
      "Loss: 1.723 | Acc: 38.252% (14297/37376)\n",
      "Loss: 1.723 | Acc: 38.261% (14325/37440)\n",
      "Loss: 1.723 | Acc: 38.257% (14348/37504)\n",
      "Loss: 1.722 | Acc: 38.280% (14381/37568)\n",
      "Loss: 1.722 | Acc: 38.273% (14403/37632)\n",
      "Loss: 1.722 | Acc: 38.267% (14425/37696)\n",
      "Loss: 1.721 | Acc: 38.287% (14457/37760)\n",
      "Loss: 1.721 | Acc: 38.312% (14491/37824)\n",
      "Loss: 1.720 | Acc: 38.323% (14520/37888)\n",
      "Loss: 1.719 | Acc: 38.343% (14552/37952)\n",
      "Loss: 1.719 | Acc: 38.347% (14578/38016)\n",
      "Loss: 1.719 | Acc: 38.356% (14606/38080)\n",
      "Loss: 1.719 | Acc: 38.370% (14636/38144)\n",
      "Loss: 1.718 | Acc: 38.377% (14663/38208)\n",
      "Loss: 1.717 | Acc: 38.399% (14696/38272)\n",
      "Loss: 1.717 | Acc: 38.408% (14724/38336)\n",
      "Loss: 1.717 | Acc: 38.430% (14757/38400)\n",
      "Loss: 1.716 | Acc: 38.439% (14785/38464)\n",
      "Loss: 1.716 | Acc: 38.437% (14809/38528)\n",
      "Loss: 1.716 | Acc: 38.435% (14833/38592)\n",
      "Loss: 1.716 | Acc: 38.447% (14862/38656)\n",
      "Loss: 1.716 | Acc: 38.463% (14893/38720)\n",
      "Loss: 1.715 | Acc: 38.477% (14923/38784)\n",
      "Loss: 1.715 | Acc: 38.488% (14952/38848)\n",
      "Loss: 1.715 | Acc: 38.505% (14983/38912)\n",
      "Loss: 1.715 | Acc: 38.503% (15007/38976)\n",
      "Loss: 1.715 | Acc: 38.519% (15038/39040)\n",
      "Loss: 1.714 | Acc: 38.536% (15069/39104)\n",
      "Loss: 1.714 | Acc: 38.524% (15089/39168)\n",
      "Loss: 1.714 | Acc: 38.525% (15114/39232)\n",
      "Loss: 1.713 | Acc: 38.538% (15144/39296)\n",
      "Loss: 1.713 | Acc: 38.547% (15172/39360)\n",
      "Loss: 1.713 | Acc: 38.555% (15200/39424)\n",
      "Loss: 1.712 | Acc: 38.571% (15231/39488)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.712 | Acc: 38.590% (15263/39552)\n",
      "Loss: 1.711 | Acc: 38.593% (15289/39616)\n",
      "Loss: 1.711 | Acc: 38.606% (15319/39680)\n",
      "Loss: 1.711 | Acc: 38.622% (15350/39744)\n",
      "Loss: 1.710 | Acc: 38.638% (15381/39808)\n",
      "Loss: 1.710 | Acc: 38.651% (15411/39872)\n",
      "Loss: 1.709 | Acc: 38.672% (15444/39936)\n",
      "Loss: 1.709 | Acc: 38.690% (15476/40000)\n",
      "Loss: 1.708 | Acc: 38.696% (15503/40064)\n",
      "Loss: 1.708 | Acc: 38.696% (15528/40128)\n",
      "Loss: 1.708 | Acc: 38.702% (15555/40192)\n",
      "Loss: 1.707 | Acc: 38.722% (15588/40256)\n",
      "Loss: 1.707 | Acc: 38.743% (15621/40320)\n",
      "Loss: 1.706 | Acc: 38.768% (15656/40384)\n",
      "Loss: 1.706 | Acc: 38.783% (15687/40448)\n",
      "Loss: 1.706 | Acc: 38.796% (15717/40512)\n",
      "Loss: 1.705 | Acc: 38.809% (15747/40576)\n",
      "Loss: 1.705 | Acc: 38.814% (15774/40640)\n",
      "Loss: 1.704 | Acc: 38.834% (15807/40704)\n",
      "Loss: 1.704 | Acc: 38.844% (15836/40768)\n",
      "Loss: 1.703 | Acc: 38.869% (15871/40832)\n",
      "Loss: 1.703 | Acc: 38.872% (15897/40896)\n",
      "Loss: 1.703 | Acc: 38.872% (15922/40960)\n",
      "Loss: 1.702 | Acc: 38.882% (15951/41024)\n",
      "Loss: 1.702 | Acc: 38.885% (15977/41088)\n",
      "Loss: 1.702 | Acc: 38.885% (16002/41152)\n",
      "Loss: 1.702 | Acc: 38.878% (16024/41216)\n",
      "Loss: 1.702 | Acc: 38.898% (16057/41280)\n",
      "Loss: 1.702 | Acc: 38.908% (16086/41344)\n",
      "Loss: 1.701 | Acc: 38.930% (16120/41408)\n",
      "Loss: 1.700 | Acc: 38.952% (16154/41472)\n",
      "Loss: 1.700 | Acc: 38.959% (16182/41536)\n",
      "Loss: 1.699 | Acc: 38.981% (16216/41600)\n",
      "Loss: 1.699 | Acc: 39.000% (16249/41664)\n",
      "Loss: 1.699 | Acc: 38.983% (16267/41728)\n",
      "Loss: 1.698 | Acc: 39.000% (16299/41792)\n",
      "Loss: 1.697 | Acc: 39.012% (16329/41856)\n",
      "Loss: 1.697 | Acc: 39.012% (16354/41920)\n",
      "Loss: 1.697 | Acc: 39.029% (16386/41984)\n",
      "Loss: 1.697 | Acc: 39.032% (16412/42048)\n",
      "Loss: 1.697 | Acc: 39.041% (16441/42112)\n",
      "Loss: 1.697 | Acc: 39.055% (16472/42176)\n",
      "Loss: 1.696 | Acc: 39.060% (16499/42240)\n",
      "Loss: 1.696 | Acc: 39.067% (16527/42304)\n",
      "Loss: 1.696 | Acc: 39.074% (16555/42368)\n",
      "Loss: 1.696 | Acc: 39.079% (16582/42432)\n",
      "Loss: 1.695 | Acc: 39.093% (16613/42496)\n",
      "Loss: 1.695 | Acc: 39.095% (16639/42560)\n",
      "Loss: 1.695 | Acc: 39.109% (16670/42624)\n",
      "Loss: 1.694 | Acc: 39.116% (16698/42688)\n",
      "Loss: 1.694 | Acc: 39.135% (16731/42752)\n",
      "Loss: 1.694 | Acc: 39.142% (16759/42816)\n",
      "Loss: 1.694 | Acc: 39.151% (16788/42880)\n",
      "Loss: 1.694 | Acc: 39.146% (16811/42944)\n",
      "Loss: 1.693 | Acc: 39.153% (16839/43008)\n",
      "Loss: 1.693 | Acc: 39.193% (16881/43072)\n",
      "Loss: 1.692 | Acc: 39.218% (16917/43136)\n",
      "Loss: 1.692 | Acc: 39.222% (16944/43200)\n",
      "Loss: 1.691 | Acc: 39.227% (16971/43264)\n",
      "Loss: 1.691 | Acc: 39.249% (17006/43328)\n",
      "Loss: 1.691 | Acc: 39.265% (17038/43392)\n",
      "Loss: 1.690 | Acc: 39.283% (17071/43456)\n",
      "Loss: 1.690 | Acc: 39.290% (17099/43520)\n",
      "Loss: 1.690 | Acc: 39.303% (17130/43584)\n",
      "Loss: 1.689 | Acc: 39.319% (17162/43648)\n",
      "Loss: 1.689 | Acc: 39.328% (17191/43712)\n",
      "Loss: 1.689 | Acc: 39.327% (17216/43776)\n",
      "Loss: 1.689 | Acc: 39.341% (17247/43840)\n",
      "Loss: 1.688 | Acc: 39.349% (17276/43904)\n",
      "Loss: 1.688 | Acc: 39.347% (17300/43968)\n",
      "Loss: 1.688 | Acc: 39.349% (17326/44032)\n",
      "Loss: 1.687 | Acc: 39.348% (17351/44096)\n",
      "Loss: 1.687 | Acc: 39.355% (17379/44160)\n",
      "Loss: 1.688 | Acc: 39.352% (17403/44224)\n",
      "Loss: 1.687 | Acc: 39.354% (17429/44288)\n",
      "Loss: 1.687 | Acc: 39.374% (17463/44352)\n",
      "Loss: 1.687 | Acc: 39.382% (17492/44416)\n",
      "Loss: 1.687 | Acc: 39.388% (17520/44480)\n",
      "Loss: 1.686 | Acc: 39.386% (17544/44544)\n",
      "Loss: 1.687 | Acc: 39.379% (17566/44608)\n",
      "Loss: 1.687 | Acc: 39.371% (17588/44672)\n",
      "Loss: 1.686 | Acc: 39.387% (17620/44736)\n",
      "Loss: 1.686 | Acc: 39.382% (17643/44800)\n",
      "Loss: 1.687 | Acc: 39.386% (17670/44864)\n",
      "Loss: 1.686 | Acc: 39.392% (17698/44928)\n",
      "Loss: 1.686 | Acc: 39.400% (17727/44992)\n",
      "Loss: 1.686 | Acc: 39.409% (17756/45056)\n",
      "Loss: 1.686 | Acc: 39.415% (17784/45120)\n",
      "Loss: 1.685 | Acc: 39.421% (17812/45184)\n",
      "Loss: 1.685 | Acc: 39.436% (17844/45248)\n",
      "Loss: 1.685 | Acc: 39.460% (17880/45312)\n",
      "Loss: 1.685 | Acc: 39.457% (17904/45376)\n",
      "Loss: 1.684 | Acc: 39.472% (17936/45440)\n",
      "Loss: 1.684 | Acc: 39.478% (17964/45504)\n",
      "Loss: 1.683 | Acc: 39.490% (17995/45568)\n",
      "Loss: 1.683 | Acc: 39.485% (18018/45632)\n",
      "Loss: 1.683 | Acc: 39.485% (18043/45696)\n",
      "Loss: 1.683 | Acc: 39.482% (18067/45760)\n",
      "Loss: 1.683 | Acc: 39.490% (18096/45824)\n",
      "Loss: 1.683 | Acc: 39.503% (18127/45888)\n",
      "Loss: 1.683 | Acc: 39.511% (18156/45952)\n",
      "Loss: 1.683 | Acc: 39.510% (18181/46016)\n",
      "Loss: 1.683 | Acc: 39.516% (18209/46080)\n",
      "Loss: 1.683 | Acc: 39.533% (18242/46144)\n",
      "Loss: 1.683 | Acc: 39.541% (18271/46208)\n",
      "Loss: 1.682 | Acc: 39.549% (18300/46272)\n",
      "Loss: 1.682 | Acc: 39.561% (18331/46336)\n",
      "Loss: 1.682 | Acc: 39.556% (18354/46400)\n",
      "Loss: 1.682 | Acc: 39.558% (18380/46464)\n",
      "Loss: 1.681 | Acc: 39.561% (18407/46528)\n",
      "Loss: 1.681 | Acc: 39.563% (18433/46592)\n",
      "Loss: 1.681 | Acc: 39.566% (18460/46656)\n",
      "Loss: 1.681 | Acc: 39.576% (18490/46720)\n",
      "Loss: 1.681 | Acc: 39.575% (18515/46784)\n",
      "Loss: 1.680 | Acc: 39.590% (18547/46848)\n",
      "Loss: 1.680 | Acc: 39.617% (18585/46912)\n",
      "Loss: 1.679 | Acc: 39.629% (18616/46976)\n",
      "Loss: 1.679 | Acc: 39.628% (18641/47040)\n",
      "Loss: 1.678 | Acc: 39.638% (18671/47104)\n",
      "Loss: 1.678 | Acc: 39.648% (18701/47168)\n",
      "Loss: 1.678 | Acc: 39.666% (18735/47232)\n",
      "Loss: 1.677 | Acc: 39.684% (18769/47296)\n",
      "Loss: 1.677 | Acc: 39.707% (18805/47360)\n",
      "Loss: 1.677 | Acc: 39.699% (18827/47424)\n",
      "Loss: 1.676 | Acc: 39.707% (18856/47488)\n",
      "Loss: 1.676 | Acc: 39.710% (18883/47552)\n",
      "Loss: 1.675 | Acc: 39.730% (18918/47616)\n",
      "Loss: 1.675 | Acc: 39.736% (18946/47680)\n",
      "Loss: 1.675 | Acc: 39.754% (18980/47744)\n",
      "Loss: 1.674 | Acc: 39.767% (19012/47808)\n",
      "Loss: 1.674 | Acc: 39.771% (19039/47872)\n",
      "Loss: 1.674 | Acc: 39.782% (19070/47936)\n",
      "Loss: 1.673 | Acc: 39.794% (19101/48000)\n",
      "Loss: 1.673 | Acc: 39.807% (19133/48064)\n",
      "Loss: 1.673 | Acc: 39.808% (19159/48128)\n",
      "Loss: 1.672 | Acc: 39.824% (19192/48192)\n",
      "Loss: 1.672 | Acc: 39.827% (19219/48256)\n",
      "Loss: 1.672 | Acc: 39.843% (19252/48320)\n",
      "Loss: 1.672 | Acc: 39.848% (19280/48384)\n",
      "Loss: 1.671 | Acc: 39.870% (19316/48448)\n",
      "Loss: 1.671 | Acc: 39.873% (19343/48512)\n",
      "Loss: 1.670 | Acc: 39.888% (19376/48576)\n",
      "Loss: 1.670 | Acc: 39.912% (19413/48640)\n",
      "Loss: 1.669 | Acc: 39.919% (19442/48704)\n",
      "Loss: 1.669 | Acc: 39.926% (19471/48768)\n",
      "Loss: 1.669 | Acc: 39.937% (19502/48832)\n",
      "Loss: 1.669 | Acc: 39.942% (19530/48896)\n",
      "Loss: 1.669 | Acc: 39.943% (19556/48960)\n",
      "Loss: 1.669 | Acc: 39.947% (19574/49000)\n",
      "Epoch 0 of training is completed, Training accuracy for this epoch is 39.946938775510205\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.562 | Acc: 37.500% (24/64)\n",
      "Loss: 1.412 | Acc: 43.750% (56/128)\n",
      "Loss: 1.456 | Acc: 43.750% (84/192)\n",
      "Loss: 1.492 | Acc: 42.578% (109/256)\n",
      "Loss: 1.494 | Acc: 45.000% (144/320)\n",
      "Loss: 1.505 | Acc: 44.531% (171/384)\n",
      "Loss: 1.545 | Acc: 43.527% (195/448)\n",
      "Loss: 1.534 | Acc: 43.359% (222/512)\n",
      "Loss: 1.522 | Acc: 43.576% (251/576)\n",
      "Loss: 1.507 | Acc: 43.750% (280/640)\n",
      "Loss: 1.530 | Acc: 43.466% (306/704)\n",
      "Loss: 1.526 | Acc: 43.490% (334/768)\n",
      "Loss: 1.524 | Acc: 43.750% (364/832)\n",
      "Loss: 1.516 | Acc: 43.527% (390/896)\n",
      "Loss: 1.509 | Acc: 43.750% (420/960)\n",
      "Loss: 1.499 | Acc: 44.727% (458/1024)\n",
      "Loss: 1.508 | Acc: 44.485% (484/1088)\n",
      "Loss: 1.504 | Acc: 44.358% (511/1152)\n",
      "Loss: 1.494 | Acc: 44.819% (545/1216)\n",
      "Loss: 1.489 | Acc: 45.156% (578/1280)\n",
      "Loss: 1.483 | Acc: 45.387% (610/1344)\n",
      "Loss: 1.482 | Acc: 45.312% (638/1408)\n",
      "Loss: 1.470 | Acc: 45.856% (675/1472)\n",
      "Loss: 1.469 | Acc: 45.964% (706/1536)\n",
      "Loss: 1.469 | Acc: 45.938% (735/1600)\n",
      "Loss: 1.469 | Acc: 46.154% (768/1664)\n",
      "Loss: 1.470 | Acc: 46.123% (797/1728)\n",
      "Loss: 1.476 | Acc: 45.926% (823/1792)\n",
      "Loss: 1.477 | Acc: 45.959% (853/1856)\n",
      "Loss: 1.478 | Acc: 46.198% (887/1920)\n",
      "Loss: 1.475 | Acc: 46.472% (922/1984)\n",
      "Loss: 1.476 | Acc: 46.436% (951/2048)\n",
      "Loss: 1.473 | Acc: 46.544% (983/2112)\n",
      "Loss: 1.474 | Acc: 46.507% (1012/2176)\n",
      "Loss: 1.468 | Acc: 46.830% (1049/2240)\n",
      "Loss: 1.470 | Acc: 46.701% (1076/2304)\n",
      "Loss: 1.472 | Acc: 46.579% (1103/2368)\n",
      "Loss: 1.473 | Acc: 46.423% (1129/2432)\n",
      "Loss: 1.470 | Acc: 46.474% (1160/2496)\n",
      "Loss: 1.476 | Acc: 46.445% (1189/2560)\n",
      "Loss: 1.475 | Acc: 46.456% (1219/2624)\n",
      "Loss: 1.476 | Acc: 46.429% (1248/2688)\n",
      "Loss: 1.476 | Acc: 46.294% (1274/2752)\n",
      "Loss: 1.476 | Acc: 46.129% (1299/2816)\n",
      "Loss: 1.475 | Acc: 46.146% (1329/2880)\n",
      "Loss: 1.471 | Acc: 46.298% (1363/2944)\n",
      "Loss: 1.471 | Acc: 46.310% (1393/3008)\n",
      "Loss: 1.469 | Acc: 46.419% (1426/3072)\n",
      "Loss: 1.467 | Acc: 46.301% (1452/3136)\n",
      "Loss: 1.466 | Acc: 46.312% (1482/3200)\n",
      "Loss: 1.464 | Acc: 46.354% (1513/3264)\n",
      "Loss: 1.461 | Acc: 46.484% (1547/3328)\n",
      "Loss: 1.460 | Acc: 46.551% (1579/3392)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.460 | Acc: 46.528% (1608/3456)\n",
      "Loss: 1.462 | Acc: 46.392% (1633/3520)\n",
      "Loss: 1.456 | Acc: 46.680% (1673/3584)\n",
      "Loss: 1.455 | Acc: 46.711% (1704/3648)\n",
      "Loss: 1.453 | Acc: 46.902% (1741/3712)\n",
      "Loss: 1.455 | Acc: 46.743% (1765/3776)\n",
      "Loss: 1.452 | Acc: 46.849% (1799/3840)\n",
      "Loss: 1.450 | Acc: 46.926% (1832/3904)\n",
      "Loss: 1.448 | Acc: 47.077% (1868/3968)\n",
      "Loss: 1.447 | Acc: 47.222% (1904/4032)\n",
      "Loss: 1.450 | Acc: 47.192% (1933/4096)\n",
      "Loss: 1.452 | Acc: 47.212% (1964/4160)\n",
      "Loss: 1.453 | Acc: 47.183% (1993/4224)\n",
      "Loss: 1.455 | Acc: 47.085% (2019/4288)\n",
      "Loss: 1.453 | Acc: 47.197% (2054/4352)\n",
      "Loss: 1.449 | Acc: 47.351% (2091/4416)\n",
      "Loss: 1.448 | Acc: 47.321% (2120/4480)\n",
      "Loss: 1.447 | Acc: 47.381% (2153/4544)\n",
      "Loss: 1.448 | Acc: 47.331% (2181/4608)\n",
      "Loss: 1.446 | Acc: 47.389% (2214/4672)\n",
      "Loss: 1.443 | Acc: 47.551% (2252/4736)\n",
      "Loss: 1.444 | Acc: 47.604% (2285/4800)\n",
      "Loss: 1.442 | Acc: 47.677% (2319/4864)\n",
      "Loss: 1.440 | Acc: 47.707% (2351/4928)\n",
      "Loss: 1.441 | Acc: 47.596% (2376/4992)\n",
      "Loss: 1.440 | Acc: 47.607% (2407/5056)\n",
      "Loss: 1.443 | Acc: 47.539% (2434/5120)\n",
      "Loss: 1.443 | Acc: 47.608% (2468/5184)\n",
      "Loss: 1.443 | Acc: 47.599% (2498/5248)\n",
      "Loss: 1.443 | Acc: 47.572% (2527/5312)\n",
      "Loss: 1.445 | Acc: 47.452% (2551/5376)\n",
      "Loss: 1.445 | Acc: 47.537% (2586/5440)\n",
      "Loss: 1.446 | Acc: 47.511% (2615/5504)\n",
      "Loss: 1.449 | Acc: 47.450% (2642/5568)\n",
      "Loss: 1.450 | Acc: 47.408% (2670/5632)\n",
      "Loss: 1.449 | Acc: 47.454% (2703/5696)\n",
      "Loss: 1.446 | Acc: 47.500% (2736/5760)\n",
      "Loss: 1.447 | Acc: 47.493% (2766/5824)\n",
      "Loss: 1.449 | Acc: 47.385% (2790/5888)\n",
      "Loss: 1.451 | Acc: 47.345% (2818/5952)\n",
      "Loss: 1.451 | Acc: 47.357% (2849/6016)\n",
      "Loss: 1.452 | Acc: 47.237% (2872/6080)\n",
      "Loss: 1.451 | Acc: 47.233% (2902/6144)\n",
      "Loss: 1.451 | Acc: 47.262% (2934/6208)\n",
      "Loss: 1.453 | Acc: 47.226% (2962/6272)\n",
      "Loss: 1.454 | Acc: 47.206% (2991/6336)\n",
      "Loss: 1.455 | Acc: 47.016% (3009/6400)\n",
      "Loss: 1.455 | Acc: 47.045% (3041/6464)\n",
      "Loss: 1.454 | Acc: 47.120% (3076/6528)\n",
      "Loss: 1.458 | Acc: 47.027% (3100/6592)\n",
      "Loss: 1.457 | Acc: 47.055% (3132/6656)\n",
      "Loss: 1.459 | Acc: 46.949% (3155/6720)\n",
      "Loss: 1.459 | Acc: 46.934% (3184/6784)\n",
      "Loss: 1.457 | Acc: 46.963% (3216/6848)\n",
      "Loss: 1.459 | Acc: 46.875% (3240/6912)\n",
      "Loss: 1.461 | Acc: 46.818% (3266/6976)\n",
      "Loss: 1.463 | Acc: 46.790% (3294/7040)\n",
      "Loss: 1.464 | Acc: 46.678% (3316/7104)\n",
      "Loss: 1.464 | Acc: 46.722% (3349/7168)\n",
      "Loss: 1.464 | Acc: 46.695% (3377/7232)\n",
      "Loss: 1.463 | Acc: 46.779% (3413/7296)\n",
      "Loss: 1.460 | Acc: 46.807% (3445/7360)\n",
      "Loss: 1.460 | Acc: 46.781% (3473/7424)\n",
      "Loss: 1.459 | Acc: 46.835% (3507/7488)\n",
      "Loss: 1.459 | Acc: 46.849% (3538/7552)\n",
      "Loss: 1.459 | Acc: 46.822% (3566/7616)\n",
      "Loss: 1.458 | Acc: 46.823% (3596/7680)\n",
      "Loss: 1.457 | Acc: 46.836% (3627/7744)\n",
      "Loss: 1.456 | Acc: 46.849% (3658/7808)\n",
      "Loss: 1.457 | Acc: 46.735% (3679/7872)\n",
      "Loss: 1.456 | Acc: 46.724% (3708/7936)\n",
      "Loss: 1.457 | Acc: 46.688% (3735/8000)\n",
      "Loss: 1.458 | Acc: 46.677% (3764/8064)\n",
      "Loss: 1.458 | Acc: 46.654% (3792/8128)\n",
      "Loss: 1.458 | Acc: 46.631% (3820/8192)\n",
      "Loss: 1.459 | Acc: 46.596% (3847/8256)\n",
      "Loss: 1.461 | Acc: 46.538% (3872/8320)\n",
      "Loss: 1.460 | Acc: 46.553% (3903/8384)\n",
      "Loss: 1.461 | Acc: 46.544% (3932/8448)\n",
      "Loss: 1.461 | Acc: 46.581% (3965/8512)\n",
      "Loss: 1.462 | Acc: 46.502% (3988/8576)\n",
      "Loss: 1.463 | Acc: 46.493% (4017/8640)\n",
      "Loss: 1.462 | Acc: 46.484% (4046/8704)\n",
      "Loss: 1.462 | Acc: 46.464% (4074/8768)\n",
      "Loss: 1.463 | Acc: 46.388% (4097/8832)\n",
      "Loss: 1.462 | Acc: 46.392% (4127/8896)\n",
      "Loss: 1.462 | Acc: 46.395% (4157/8960)\n",
      "Loss: 1.462 | Acc: 46.432% (4190/9024)\n",
      "Loss: 1.463 | Acc: 46.314% (4209/9088)\n",
      "Loss: 1.463 | Acc: 46.340% (4241/9152)\n",
      "Loss: 1.461 | Acc: 46.365% (4273/9216)\n",
      "Loss: 1.461 | Acc: 46.369% (4303/9280)\n",
      "Loss: 1.461 | Acc: 46.340% (4330/9344)\n",
      "Loss: 1.461 | Acc: 46.344% (4360/9408)\n",
      "Loss: 1.462 | Acc: 46.305% (4386/9472)\n",
      "Loss: 1.462 | Acc: 46.309% (4416/9536)\n",
      "Loss: 1.461 | Acc: 46.365% (4451/9600)\n",
      "Loss: 1.462 | Acc: 46.347% (4479/9664)\n",
      "Loss: 1.461 | Acc: 46.330% (4507/9728)\n",
      "Loss: 1.462 | Acc: 46.283% (4532/9792)\n",
      "Loss: 1.462 | Acc: 46.266% (4560/9856)\n",
      "Loss: 1.463 | Acc: 46.230% (4586/9920)\n",
      "Loss: 1.464 | Acc: 46.214% (4614/9984)\n",
      "Loss: 1.464 | Acc: 46.200% (4620/10000)\n",
      "Evaluation of Epoch 0 is completed, Test accuracy for this epoch is 46.2\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.288 | Acc: 57.812% (37/64)\n",
      "Loss: 1.490 | Acc: 51.562% (66/128)\n",
      "Loss: 1.426 | Acc: 51.042% (98/192)\n",
      "Loss: 1.475 | Acc: 47.656% (122/256)\n",
      "Loss: 1.419 | Acc: 49.688% (159/320)\n",
      "Loss: 1.373 | Acc: 50.781% (195/384)\n",
      "Loss: 1.356 | Acc: 50.893% (228/448)\n",
      "Loss: 1.357 | Acc: 50.781% (260/512)\n",
      "Loss: 1.350 | Acc: 50.868% (293/576)\n",
      "Loss: 1.365 | Acc: 50.625% (324/640)\n",
      "Loss: 1.360 | Acc: 50.852% (358/704)\n",
      "Loss: 1.364 | Acc: 50.521% (388/768)\n",
      "Loss: 1.377 | Acc: 50.240% (418/832)\n",
      "Loss: 1.386 | Acc: 50.223% (450/896)\n",
      "Loss: 1.381 | Acc: 50.521% (485/960)\n",
      "Loss: 1.376 | Acc: 50.879% (521/1024)\n",
      "Loss: 1.376 | Acc: 51.103% (556/1088)\n",
      "Loss: 1.379 | Acc: 50.868% (586/1152)\n",
      "Loss: 1.388 | Acc: 50.493% (614/1216)\n",
      "Loss: 1.383 | Acc: 50.703% (649/1280)\n",
      "Loss: 1.382 | Acc: 50.595% (680/1344)\n",
      "Loss: 1.380 | Acc: 50.923% (717/1408)\n",
      "Loss: 1.379 | Acc: 50.883% (749/1472)\n",
      "Loss: 1.381 | Acc: 50.977% (783/1536)\n",
      "Loss: 1.378 | Acc: 51.000% (816/1600)\n",
      "Loss: 1.385 | Acc: 50.541% (841/1664)\n",
      "Loss: 1.387 | Acc: 50.521% (873/1728)\n",
      "Loss: 1.387 | Acc: 50.558% (906/1792)\n",
      "Loss: 1.390 | Acc: 50.431% (936/1856)\n",
      "Loss: 1.393 | Acc: 50.365% (967/1920)\n",
      "Loss: 1.397 | Acc: 50.101% (994/1984)\n",
      "Loss: 1.394 | Acc: 50.244% (1029/2048)\n",
      "Loss: 1.393 | Acc: 50.237% (1061/2112)\n",
      "Loss: 1.397 | Acc: 50.092% (1090/2176)\n",
      "Loss: 1.398 | Acc: 49.955% (1119/2240)\n",
      "Loss: 1.399 | Acc: 50.130% (1155/2304)\n",
      "Loss: 1.399 | Acc: 50.253% (1190/2368)\n",
      "Loss: 1.396 | Acc: 50.288% (1223/2432)\n",
      "Loss: 1.393 | Acc: 50.160% (1252/2496)\n",
      "Loss: 1.395 | Acc: 49.922% (1278/2560)\n",
      "Loss: 1.394 | Acc: 49.962% (1311/2624)\n",
      "Loss: 1.394 | Acc: 49.814% (1339/2688)\n",
      "Loss: 1.393 | Acc: 49.673% (1367/2752)\n",
      "Loss: 1.395 | Acc: 49.645% (1398/2816)\n",
      "Loss: 1.399 | Acc: 49.479% (1425/2880)\n",
      "Loss: 1.402 | Acc: 49.355% (1453/2944)\n",
      "Loss: 1.403 | Acc: 49.335% (1484/3008)\n",
      "Loss: 1.401 | Acc: 49.382% (1517/3072)\n",
      "Loss: 1.401 | Acc: 49.235% (1544/3136)\n",
      "Loss: 1.403 | Acc: 49.188% (1574/3200)\n",
      "Loss: 1.407 | Acc: 48.897% (1596/3264)\n",
      "Loss: 1.407 | Acc: 48.978% (1630/3328)\n",
      "Loss: 1.408 | Acc: 48.998% (1662/3392)\n",
      "Loss: 1.413 | Acc: 48.814% (1687/3456)\n",
      "Loss: 1.409 | Acc: 48.949% (1723/3520)\n",
      "Loss: 1.408 | Acc: 49.023% (1757/3584)\n",
      "Loss: 1.408 | Acc: 48.931% (1785/3648)\n",
      "Loss: 1.405 | Acc: 48.949% (1817/3712)\n",
      "Loss: 1.403 | Acc: 48.967% (1849/3776)\n",
      "Loss: 1.406 | Acc: 48.828% (1875/3840)\n",
      "Loss: 1.405 | Acc: 49.027% (1914/3904)\n",
      "Loss: 1.403 | Acc: 49.068% (1947/3968)\n",
      "Loss: 1.402 | Acc: 49.033% (1977/4032)\n",
      "Loss: 1.397 | Acc: 49.194% (2015/4096)\n",
      "Loss: 1.397 | Acc: 49.135% (2044/4160)\n",
      "Loss: 1.397 | Acc: 49.100% (2074/4224)\n",
      "Loss: 1.399 | Acc: 49.067% (2104/4288)\n",
      "Loss: 1.395 | Acc: 49.196% (2141/4352)\n",
      "Loss: 1.393 | Acc: 49.139% (2170/4416)\n",
      "Loss: 1.392 | Acc: 49.129% (2201/4480)\n",
      "Loss: 1.394 | Acc: 49.120% (2232/4544)\n",
      "Loss: 1.396 | Acc: 49.045% (2260/4608)\n",
      "Loss: 1.397 | Acc: 48.973% (2288/4672)\n",
      "Loss: 1.400 | Acc: 48.818% (2312/4736)\n",
      "Loss: 1.398 | Acc: 48.854% (2345/4800)\n",
      "Loss: 1.394 | Acc: 48.972% (2382/4864)\n",
      "Loss: 1.395 | Acc: 49.026% (2416/4928)\n",
      "Loss: 1.396 | Acc: 48.998% (2446/4992)\n",
      "Loss: 1.398 | Acc: 48.932% (2474/5056)\n",
      "Loss: 1.401 | Acc: 48.848% (2501/5120)\n",
      "Loss: 1.400 | Acc: 48.862% (2533/5184)\n",
      "Loss: 1.399 | Acc: 48.876% (2565/5248)\n",
      "Loss: 1.399 | Acc: 48.870% (2596/5312)\n",
      "Loss: 1.401 | Acc: 48.865% (2627/5376)\n",
      "Loss: 1.401 | Acc: 48.879% (2659/5440)\n",
      "Loss: 1.401 | Acc: 48.783% (2685/5504)\n",
      "Loss: 1.401 | Acc: 48.833% (2719/5568)\n",
      "Loss: 1.399 | Acc: 48.881% (2753/5632)\n",
      "Loss: 1.400 | Acc: 48.859% (2783/5696)\n",
      "Loss: 1.401 | Acc: 48.872% (2815/5760)\n",
      "Loss: 1.403 | Acc: 48.798% (2842/5824)\n",
      "Loss: 1.404 | Acc: 48.811% (2874/5888)\n",
      "Loss: 1.404 | Acc: 48.841% (2907/5952)\n",
      "Loss: 1.406 | Acc: 48.803% (2936/6016)\n",
      "Loss: 1.408 | Acc: 48.734% (2963/6080)\n",
      "Loss: 1.408 | Acc: 48.796% (2998/6144)\n",
      "Loss: 1.408 | Acc: 48.744% (3026/6208)\n",
      "Loss: 1.408 | Acc: 48.740% (3057/6272)\n",
      "Loss: 1.411 | Acc: 48.690% (3085/6336)\n",
      "Loss: 1.411 | Acc: 48.734% (3119/6400)\n",
      "Loss: 1.412 | Acc: 48.700% (3148/6464)\n",
      "Loss: 1.411 | Acc: 48.820% (3187/6528)\n",
      "Loss: 1.411 | Acc: 48.802% (3217/6592)\n",
      "Loss: 1.410 | Acc: 48.873% (3253/6656)\n",
      "Loss: 1.411 | Acc: 48.884% (3285/6720)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.412 | Acc: 48.821% (3312/6784)\n",
      "Loss: 1.411 | Acc: 48.817% (3343/6848)\n",
      "Loss: 1.409 | Acc: 48.886% (3379/6912)\n",
      "Loss: 1.410 | Acc: 48.853% (3408/6976)\n",
      "Loss: 1.409 | Acc: 48.878% (3441/7040)\n",
      "Loss: 1.408 | Acc: 48.958% (3478/7104)\n",
      "Loss: 1.409 | Acc: 48.954% (3509/7168)\n",
      "Loss: 1.409 | Acc: 48.963% (3541/7232)\n",
      "Loss: 1.407 | Acc: 49.013% (3576/7296)\n",
      "Loss: 1.407 | Acc: 49.022% (3608/7360)\n",
      "Loss: 1.407 | Acc: 49.003% (3638/7424)\n",
      "Loss: 1.408 | Acc: 49.038% (3672/7488)\n",
      "Loss: 1.408 | Acc: 49.033% (3703/7552)\n",
      "Loss: 1.408 | Acc: 49.041% (3735/7616)\n",
      "Loss: 1.408 | Acc: 49.023% (3765/7680)\n",
      "Loss: 1.408 | Acc: 49.083% (3801/7744)\n",
      "Loss: 1.408 | Acc: 49.065% (3831/7808)\n",
      "Loss: 1.407 | Acc: 49.085% (3864/7872)\n",
      "Loss: 1.407 | Acc: 49.168% (3902/7936)\n",
      "Loss: 1.407 | Acc: 49.138% (3931/8000)\n",
      "Loss: 1.409 | Acc: 49.082% (3958/8064)\n",
      "Loss: 1.409 | Acc: 49.102% (3991/8128)\n",
      "Loss: 1.408 | Acc: 49.109% (4023/8192)\n",
      "Loss: 1.410 | Acc: 49.019% (4047/8256)\n",
      "Loss: 1.409 | Acc: 49.038% (4080/8320)\n",
      "Loss: 1.408 | Acc: 49.070% (4114/8384)\n",
      "Loss: 1.410 | Acc: 48.982% (4138/8448)\n",
      "Loss: 1.410 | Acc: 48.943% (4166/8512)\n",
      "Loss: 1.410 | Acc: 48.927% (4196/8576)\n",
      "Loss: 1.408 | Acc: 48.935% (4228/8640)\n",
      "Loss: 1.409 | Acc: 48.909% (4257/8704)\n",
      "Loss: 1.409 | Acc: 48.917% (4289/8768)\n",
      "Loss: 1.410 | Acc: 48.890% (4318/8832)\n",
      "Loss: 1.411 | Acc: 48.853% (4346/8896)\n",
      "Loss: 1.410 | Acc: 48.895% (4381/8960)\n",
      "Loss: 1.411 | Acc: 48.881% (4411/9024)\n",
      "Loss: 1.411 | Acc: 48.845% (4439/9088)\n",
      "Loss: 1.411 | Acc: 48.820% (4468/9152)\n",
      "Loss: 1.412 | Acc: 48.839% (4501/9216)\n",
      "Loss: 1.412 | Acc: 48.847% (4533/9280)\n",
      "Loss: 1.412 | Acc: 48.855% (4565/9344)\n",
      "Loss: 1.412 | Acc: 48.799% (4591/9408)\n",
      "Loss: 1.411 | Acc: 48.870% (4629/9472)\n",
      "Loss: 1.411 | Acc: 48.857% (4659/9536)\n",
      "Loss: 1.412 | Acc: 48.885% (4693/9600)\n",
      "Loss: 1.411 | Acc: 48.893% (4725/9664)\n",
      "Loss: 1.412 | Acc: 48.869% (4754/9728)\n",
      "Loss: 1.411 | Acc: 48.866% (4785/9792)\n",
      "Loss: 1.413 | Acc: 48.864% (4816/9856)\n",
      "Loss: 1.414 | Acc: 48.821% (4843/9920)\n",
      "Loss: 1.413 | Acc: 48.838% (4876/9984)\n",
      "Loss: 1.413 | Acc: 48.855% (4909/10048)\n",
      "Loss: 1.413 | Acc: 48.853% (4940/10112)\n",
      "Loss: 1.413 | Acc: 48.860% (4972/10176)\n",
      "Loss: 1.414 | Acc: 48.809% (4998/10240)\n",
      "Loss: 1.415 | Acc: 48.806% (5029/10304)\n",
      "Loss: 1.415 | Acc: 48.814% (5061/10368)\n",
      "Loss: 1.416 | Acc: 48.821% (5093/10432)\n",
      "Loss: 1.415 | Acc: 48.866% (5129/10496)\n",
      "Loss: 1.416 | Acc: 48.854% (5159/10560)\n",
      "Loss: 1.418 | Acc: 48.795% (5184/10624)\n",
      "Loss: 1.417 | Acc: 48.821% (5218/10688)\n",
      "Loss: 1.416 | Acc: 48.875% (5255/10752)\n",
      "Loss: 1.416 | Acc: 48.844% (5283/10816)\n",
      "Loss: 1.416 | Acc: 48.833% (5313/10880)\n",
      "Loss: 1.416 | Acc: 48.849% (5346/10944)\n",
      "Loss: 1.415 | Acc: 48.910% (5384/11008)\n",
      "Loss: 1.416 | Acc: 48.916% (5416/11072)\n",
      "Loss: 1.414 | Acc: 48.958% (5452/11136)\n",
      "Loss: 1.413 | Acc: 49.027% (5491/11200)\n",
      "Loss: 1.415 | Acc: 48.988% (5518/11264)\n",
      "Loss: 1.414 | Acc: 48.994% (5550/11328)\n",
      "Loss: 1.413 | Acc: 48.991% (5581/11392)\n",
      "Loss: 1.415 | Acc: 48.961% (5609/11456)\n",
      "Loss: 1.415 | Acc: 48.950% (5639/11520)\n",
      "Loss: 1.415 | Acc: 48.947% (5670/11584)\n",
      "Loss: 1.414 | Acc: 48.944% (5701/11648)\n",
      "Loss: 1.414 | Acc: 48.907% (5728/11712)\n",
      "Loss: 1.414 | Acc: 48.888% (5757/11776)\n",
      "Loss: 1.415 | Acc: 48.910% (5791/11840)\n",
      "Loss: 1.415 | Acc: 48.916% (5823/11904)\n",
      "Loss: 1.415 | Acc: 48.922% (5855/11968)\n",
      "Loss: 1.415 | Acc: 48.944% (5889/12032)\n",
      "Loss: 1.415 | Acc: 48.975% (5924/12096)\n",
      "Loss: 1.414 | Acc: 49.013% (5960/12160)\n",
      "Loss: 1.414 | Acc: 49.002% (5990/12224)\n",
      "Loss: 1.413 | Acc: 49.007% (6022/12288)\n",
      "Loss: 1.413 | Acc: 49.004% (6053/12352)\n",
      "Loss: 1.413 | Acc: 48.961% (6079/12416)\n",
      "Loss: 1.413 | Acc: 48.966% (6111/12480)\n",
      "Loss: 1.414 | Acc: 48.908% (6135/12544)\n",
      "Loss: 1.415 | Acc: 48.874% (6162/12608)\n",
      "Loss: 1.415 | Acc: 48.903% (6197/12672)\n",
      "Loss: 1.416 | Acc: 48.877% (6225/12736)\n",
      "Loss: 1.417 | Acc: 48.836% (6251/12800)\n",
      "Loss: 1.415 | Acc: 48.896% (6290/12864)\n",
      "Loss: 1.415 | Acc: 48.940% (6327/12928)\n",
      "Loss: 1.415 | Acc: 48.961% (6361/12992)\n",
      "Loss: 1.416 | Acc: 48.981% (6395/13056)\n",
      "Loss: 1.415 | Acc: 48.986% (6427/13120)\n",
      "Loss: 1.417 | Acc: 48.968% (6456/13184)\n",
      "Loss: 1.416 | Acc: 48.989% (6490/13248)\n",
      "Loss: 1.416 | Acc: 48.963% (6518/13312)\n",
      "Loss: 1.417 | Acc: 48.976% (6551/13376)\n",
      "Loss: 1.416 | Acc: 48.943% (6578/13440)\n",
      "Loss: 1.417 | Acc: 48.963% (6612/13504)\n",
      "Loss: 1.417 | Acc: 48.968% (6644/13568)\n",
      "Loss: 1.416 | Acc: 48.951% (6673/13632)\n",
      "Loss: 1.417 | Acc: 48.956% (6705/13696)\n",
      "Loss: 1.418 | Acc: 48.932% (6733/13760)\n",
      "Loss: 1.418 | Acc: 48.973% (6770/13824)\n",
      "Loss: 1.417 | Acc: 48.992% (6804/13888)\n",
      "Loss: 1.416 | Acc: 49.011% (6838/13952)\n",
      "Loss: 1.418 | Acc: 48.944% (6860/14016)\n",
      "Loss: 1.418 | Acc: 48.928% (6889/14080)\n",
      "Loss: 1.418 | Acc: 48.911% (6918/14144)\n",
      "Loss: 1.418 | Acc: 48.909% (6949/14208)\n",
      "Loss: 1.417 | Acc: 48.921% (6982/14272)\n",
      "Loss: 1.417 | Acc: 48.947% (7017/14336)\n",
      "Loss: 1.417 | Acc: 48.972% (7052/14400)\n",
      "Loss: 1.416 | Acc: 48.991% (7086/14464)\n",
      "Loss: 1.417 | Acc: 48.995% (7118/14528)\n",
      "Loss: 1.417 | Acc: 48.979% (7147/14592)\n",
      "Loss: 1.416 | Acc: 48.990% (7180/14656)\n",
      "Loss: 1.415 | Acc: 49.015% (7215/14720)\n",
      "Loss: 1.414 | Acc: 49.067% (7254/14784)\n",
      "Loss: 1.414 | Acc: 49.057% (7284/14848)\n",
      "Loss: 1.414 | Acc: 49.068% (7317/14912)\n",
      "Loss: 1.414 | Acc: 49.065% (7348/14976)\n",
      "Loss: 1.414 | Acc: 49.036% (7375/15040)\n",
      "Loss: 1.414 | Acc: 49.040% (7407/15104)\n",
      "Loss: 1.413 | Acc: 49.064% (7442/15168)\n",
      "Loss: 1.412 | Acc: 49.101% (7479/15232)\n",
      "Loss: 1.413 | Acc: 49.098% (7510/15296)\n",
      "Loss: 1.413 | Acc: 49.102% (7542/15360)\n",
      "Loss: 1.413 | Acc: 49.086% (7571/15424)\n",
      "Loss: 1.412 | Acc: 49.141% (7611/15488)\n",
      "Loss: 1.412 | Acc: 49.151% (7644/15552)\n",
      "Loss: 1.413 | Acc: 49.129% (7672/15616)\n",
      "Loss: 1.413 | Acc: 49.145% (7706/15680)\n",
      "Loss: 1.413 | Acc: 49.162% (7740/15744)\n",
      "Loss: 1.413 | Acc: 49.171% (7773/15808)\n",
      "Loss: 1.413 | Acc: 49.194% (7808/15872)\n",
      "Loss: 1.412 | Acc: 49.234% (7846/15936)\n",
      "Loss: 1.411 | Acc: 49.225% (7876/16000)\n",
      "Loss: 1.412 | Acc: 49.216% (7906/16064)\n",
      "Loss: 1.412 | Acc: 49.213% (7937/16128)\n",
      "Loss: 1.412 | Acc: 49.234% (7972/16192)\n",
      "Loss: 1.411 | Acc: 49.200% (7998/16256)\n",
      "Loss: 1.412 | Acc: 49.210% (8031/16320)\n",
      "Loss: 1.412 | Acc: 49.194% (8060/16384)\n",
      "Loss: 1.412 | Acc: 49.161% (8086/16448)\n",
      "Loss: 1.412 | Acc: 49.146% (8115/16512)\n",
      "Loss: 1.411 | Acc: 49.174% (8151/16576)\n",
      "Loss: 1.411 | Acc: 49.189% (8185/16640)\n",
      "Loss: 1.411 | Acc: 49.204% (8219/16704)\n",
      "Loss: 1.411 | Acc: 49.219% (8253/16768)\n",
      "Loss: 1.412 | Acc: 49.204% (8282/16832)\n",
      "Loss: 1.411 | Acc: 49.231% (8318/16896)\n",
      "Loss: 1.410 | Acc: 49.245% (8352/16960)\n",
      "Loss: 1.410 | Acc: 49.236% (8382/17024)\n",
      "Loss: 1.409 | Acc: 49.233% (8413/17088)\n",
      "Loss: 1.409 | Acc: 49.242% (8446/17152)\n",
      "Loss: 1.411 | Acc: 49.233% (8476/17216)\n",
      "Loss: 1.411 | Acc: 49.236% (8508/17280)\n",
      "Loss: 1.410 | Acc: 49.256% (8543/17344)\n",
      "Loss: 1.410 | Acc: 49.236% (8571/17408)\n",
      "Loss: 1.411 | Acc: 49.199% (8596/17472)\n",
      "Loss: 1.410 | Acc: 49.242% (8635/17536)\n",
      "Loss: 1.410 | Acc: 49.205% (8660/17600)\n",
      "Loss: 1.411 | Acc: 49.173% (8686/17664)\n",
      "Loss: 1.410 | Acc: 49.193% (8721/17728)\n",
      "Loss: 1.410 | Acc: 49.168% (8748/17792)\n",
      "Loss: 1.410 | Acc: 49.177% (8781/17856)\n",
      "Loss: 1.409 | Acc: 49.208% (8818/17920)\n",
      "Loss: 1.409 | Acc: 49.194% (8847/17984)\n",
      "Loss: 1.410 | Acc: 49.180% (8876/18048)\n",
      "Loss: 1.409 | Acc: 49.199% (8911/18112)\n",
      "Loss: 1.408 | Acc: 49.208% (8944/18176)\n",
      "Loss: 1.408 | Acc: 49.227% (8979/18240)\n",
      "Loss: 1.408 | Acc: 49.235% (9012/18304)\n",
      "Loss: 1.408 | Acc: 49.238% (9044/18368)\n",
      "Loss: 1.408 | Acc: 49.224% (9073/18432)\n",
      "Loss: 1.408 | Acc: 49.243% (9108/18496)\n",
      "Loss: 1.408 | Acc: 49.219% (9135/18560)\n",
      "Loss: 1.408 | Acc: 49.195% (9162/18624)\n",
      "Loss: 1.408 | Acc: 49.229% (9200/18688)\n",
      "Loss: 1.409 | Acc: 49.195% (9225/18752)\n",
      "Loss: 1.408 | Acc: 49.229% (9263/18816)\n",
      "Loss: 1.409 | Acc: 49.206% (9290/18880)\n",
      "Loss: 1.408 | Acc: 49.219% (9324/18944)\n",
      "Loss: 1.409 | Acc: 49.200% (9352/19008)\n",
      "Loss: 1.409 | Acc: 49.224% (9388/19072)\n",
      "Loss: 1.408 | Acc: 49.206% (9416/19136)\n",
      "Loss: 1.407 | Acc: 49.266% (9459/19200)\n",
      "Loss: 1.407 | Acc: 49.289% (9495/19264)\n",
      "Loss: 1.406 | Acc: 49.307% (9530/19328)\n",
      "Loss: 1.407 | Acc: 49.304% (9561/19392)\n",
      "Loss: 1.407 | Acc: 49.291% (9590/19456)\n",
      "Loss: 1.407 | Acc: 49.308% (9625/19520)\n",
      "Loss: 1.407 | Acc: 49.316% (9658/19584)\n",
      "Loss: 1.406 | Acc: 49.333% (9693/19648)\n",
      "Loss: 1.406 | Acc: 49.351% (9728/19712)\n",
      "Loss: 1.406 | Acc: 49.343% (9758/19776)\n",
      "Loss: 1.405 | Acc: 49.350% (9791/19840)\n",
      "Loss: 1.406 | Acc: 49.317% (9816/19904)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.406 | Acc: 49.314% (9847/19968)\n",
      "Loss: 1.406 | Acc: 49.306% (9877/20032)\n",
      "Loss: 1.406 | Acc: 49.293% (9906/20096)\n",
      "Loss: 1.406 | Acc: 49.315% (9942/20160)\n",
      "Loss: 1.406 | Acc: 49.308% (9972/20224)\n",
      "Loss: 1.406 | Acc: 49.300% (10002/20288)\n",
      "Loss: 1.406 | Acc: 49.278% (10029/20352)\n",
      "Loss: 1.406 | Acc: 49.300% (10065/20416)\n",
      "Loss: 1.407 | Acc: 49.253% (10087/20480)\n",
      "Loss: 1.406 | Acc: 49.275% (10123/20544)\n",
      "Loss: 1.406 | Acc: 49.272% (10154/20608)\n",
      "Loss: 1.405 | Acc: 49.284% (10188/20672)\n",
      "Loss: 1.405 | Acc: 49.257% (10214/20736)\n",
      "Loss: 1.406 | Acc: 49.250% (10244/20800)\n",
      "Loss: 1.406 | Acc: 49.243% (10274/20864)\n",
      "Loss: 1.406 | Acc: 49.235% (10304/20928)\n",
      "Loss: 1.406 | Acc: 49.214% (10331/20992)\n",
      "Loss: 1.406 | Acc: 49.221% (10364/21056)\n",
      "Loss: 1.406 | Acc: 49.242% (10400/21120)\n",
      "Loss: 1.406 | Acc: 49.231% (10429/21184)\n",
      "Loss: 1.406 | Acc: 49.242% (10463/21248)\n",
      "Loss: 1.406 | Acc: 49.212% (10488/21312)\n",
      "Loss: 1.406 | Acc: 49.205% (10518/21376)\n",
      "Loss: 1.406 | Acc: 49.216% (10552/21440)\n",
      "Loss: 1.406 | Acc: 49.200% (10580/21504)\n",
      "Loss: 1.406 | Acc: 49.193% (10610/21568)\n",
      "Loss: 1.405 | Acc: 49.214% (10646/21632)\n",
      "Loss: 1.405 | Acc: 49.221% (10679/21696)\n",
      "Loss: 1.405 | Acc: 49.242% (10715/21760)\n",
      "Loss: 1.404 | Acc: 49.239% (10746/21824)\n",
      "Loss: 1.404 | Acc: 49.246% (10779/21888)\n",
      "Loss: 1.405 | Acc: 49.262% (10814/21952)\n",
      "Loss: 1.404 | Acc: 49.251% (10843/22016)\n",
      "Loss: 1.404 | Acc: 49.226% (10869/22080)\n",
      "Loss: 1.403 | Acc: 49.268% (10910/22144)\n",
      "Loss: 1.403 | Acc: 49.280% (10944/22208)\n",
      "Loss: 1.402 | Acc: 49.291% (10978/22272)\n",
      "Loss: 1.402 | Acc: 49.288% (11009/22336)\n",
      "Loss: 1.402 | Acc: 49.308% (11045/22400)\n",
      "Loss: 1.401 | Acc: 49.319% (11079/22464)\n",
      "Loss: 1.402 | Acc: 49.312% (11109/22528)\n",
      "Loss: 1.402 | Acc: 49.345% (11148/22592)\n",
      "Loss: 1.402 | Acc: 49.356% (11182/22656)\n",
      "Loss: 1.402 | Acc: 49.344% (11211/22720)\n",
      "Loss: 1.402 | Acc: 49.364% (11247/22784)\n",
      "Loss: 1.401 | Acc: 49.361% (11278/22848)\n",
      "Loss: 1.402 | Acc: 49.341% (11305/22912)\n",
      "Loss: 1.402 | Acc: 49.343% (11337/22976)\n",
      "Loss: 1.401 | Acc: 49.379% (11377/23040)\n",
      "Loss: 1.401 | Acc: 49.398% (11413/23104)\n",
      "Loss: 1.401 | Acc: 49.409% (11447/23168)\n",
      "Loss: 1.401 | Acc: 49.428% (11483/23232)\n",
      "Loss: 1.401 | Acc: 49.429% (11515/23296)\n",
      "Loss: 1.401 | Acc: 49.409% (11542/23360)\n",
      "Loss: 1.401 | Acc: 49.419% (11576/23424)\n",
      "Loss: 1.401 | Acc: 49.434% (11611/23488)\n",
      "Loss: 1.401 | Acc: 49.414% (11638/23552)\n",
      "Loss: 1.401 | Acc: 49.407% (11668/23616)\n",
      "Loss: 1.401 | Acc: 49.409% (11700/23680)\n",
      "Loss: 1.401 | Acc: 49.410% (11732/23744)\n",
      "Loss: 1.401 | Acc: 49.416% (11765/23808)\n",
      "Loss: 1.400 | Acc: 49.418% (11797/23872)\n",
      "Loss: 1.400 | Acc: 49.428% (11831/23936)\n",
      "Loss: 1.400 | Acc: 49.408% (11858/24000)\n",
      "Loss: 1.401 | Acc: 49.393% (11886/24064)\n",
      "Loss: 1.401 | Acc: 49.407% (11921/24128)\n",
      "Loss: 1.401 | Acc: 49.396% (11950/24192)\n",
      "Loss: 1.401 | Acc: 49.373% (11976/24256)\n",
      "Loss: 1.401 | Acc: 49.363% (12005/24320)\n",
      "Loss: 1.401 | Acc: 49.368% (12038/24384)\n",
      "Loss: 1.401 | Acc: 49.374% (12071/24448)\n",
      "Loss: 1.401 | Acc: 49.380% (12104/24512)\n",
      "Loss: 1.401 | Acc: 49.377% (12135/24576)\n",
      "Loss: 1.401 | Acc: 49.383% (12168/24640)\n",
      "Loss: 1.401 | Acc: 49.373% (12197/24704)\n",
      "Loss: 1.400 | Acc: 49.382% (12231/24768)\n",
      "Loss: 1.400 | Acc: 49.392% (12265/24832)\n",
      "Loss: 1.400 | Acc: 49.402% (12299/24896)\n",
      "Loss: 1.400 | Acc: 49.411% (12333/24960)\n",
      "Loss: 1.399 | Acc: 49.393% (12360/25024)\n",
      "Loss: 1.399 | Acc: 49.398% (12393/25088)\n",
      "Loss: 1.399 | Acc: 49.396% (12424/25152)\n",
      "Loss: 1.399 | Acc: 49.425% (12463/25216)\n",
      "Loss: 1.399 | Acc: 49.426% (12495/25280)\n",
      "Loss: 1.399 | Acc: 49.432% (12528/25344)\n",
      "Loss: 1.398 | Acc: 49.445% (12563/25408)\n",
      "Loss: 1.398 | Acc: 49.462% (12599/25472)\n",
      "Loss: 1.397 | Acc: 49.495% (12639/25536)\n",
      "Loss: 1.397 | Acc: 49.496% (12671/25600)\n",
      "Loss: 1.398 | Acc: 49.486% (12700/25664)\n",
      "Loss: 1.398 | Acc: 49.499% (12735/25728)\n",
      "Loss: 1.399 | Acc: 49.488% (12764/25792)\n",
      "Loss: 1.399 | Acc: 49.466% (12790/25856)\n",
      "Loss: 1.399 | Acc: 49.471% (12823/25920)\n",
      "Loss: 1.399 | Acc: 49.454% (12850/25984)\n",
      "Loss: 1.399 | Acc: 49.482% (12889/26048)\n",
      "Loss: 1.399 | Acc: 49.468% (12917/26112)\n",
      "Loss: 1.399 | Acc: 49.469% (12949/26176)\n",
      "Loss: 1.398 | Acc: 49.466% (12980/26240)\n",
      "Loss: 1.398 | Acc: 49.487% (13017/26304)\n",
      "Loss: 1.398 | Acc: 49.480% (13047/26368)\n",
      "Loss: 1.398 | Acc: 49.478% (13078/26432)\n",
      "Loss: 1.398 | Acc: 49.472% (13108/26496)\n",
      "Loss: 1.398 | Acc: 49.477% (13141/26560)\n",
      "Loss: 1.398 | Acc: 49.482% (13174/26624)\n",
      "Loss: 1.397 | Acc: 49.509% (13213/26688)\n",
      "Loss: 1.397 | Acc: 49.492% (13240/26752)\n",
      "Loss: 1.398 | Acc: 49.456% (13262/26816)\n",
      "Loss: 1.397 | Acc: 49.453% (13293/26880)\n",
      "Loss: 1.397 | Acc: 49.428% (13318/26944)\n",
      "Loss: 1.398 | Acc: 49.404% (13343/27008)\n",
      "Loss: 1.398 | Acc: 49.394% (13372/27072)\n",
      "Loss: 1.397 | Acc: 49.407% (13407/27136)\n",
      "Loss: 1.398 | Acc: 49.382% (13432/27200)\n",
      "Loss: 1.397 | Acc: 49.387% (13465/27264)\n",
      "Loss: 1.398 | Acc: 49.378% (13494/27328)\n",
      "Loss: 1.397 | Acc: 49.376% (13525/27392)\n",
      "Loss: 1.398 | Acc: 49.370% (13555/27456)\n",
      "Loss: 1.397 | Acc: 49.390% (13592/27520)\n",
      "Loss: 1.397 | Acc: 49.398% (13626/27584)\n",
      "Loss: 1.397 | Acc: 49.414% (13662/27648)\n",
      "Loss: 1.397 | Acc: 49.412% (13693/27712)\n",
      "Loss: 1.396 | Acc: 49.428% (13729/27776)\n",
      "Loss: 1.396 | Acc: 49.440% (13764/27840)\n",
      "Loss: 1.396 | Acc: 49.459% (13801/27904)\n",
      "Loss: 1.396 | Acc: 49.431% (13825/27968)\n",
      "Loss: 1.396 | Acc: 49.443% (13860/28032)\n",
      "Loss: 1.396 | Acc: 49.438% (13890/28096)\n",
      "Loss: 1.396 | Acc: 49.432% (13920/28160)\n",
      "Loss: 1.396 | Acc: 49.430% (13951/28224)\n",
      "Loss: 1.396 | Acc: 49.438% (13985/28288)\n",
      "Loss: 1.396 | Acc: 49.439% (14017/28352)\n",
      "Loss: 1.396 | Acc: 49.447% (14051/28416)\n",
      "Loss: 1.396 | Acc: 49.452% (14084/28480)\n",
      "Loss: 1.395 | Acc: 49.450% (14115/28544)\n",
      "Loss: 1.395 | Acc: 49.451% (14147/28608)\n",
      "Loss: 1.395 | Acc: 49.442% (14176/28672)\n",
      "Loss: 1.395 | Acc: 49.436% (14206/28736)\n",
      "Loss: 1.395 | Acc: 49.444% (14240/28800)\n",
      "Loss: 1.395 | Acc: 49.442% (14271/28864)\n",
      "Loss: 1.395 | Acc: 49.443% (14303/28928)\n",
      "Loss: 1.395 | Acc: 49.455% (14338/28992)\n",
      "Loss: 1.395 | Acc: 49.467% (14373/29056)\n",
      "Loss: 1.395 | Acc: 49.475% (14407/29120)\n",
      "Loss: 1.395 | Acc: 49.465% (14436/29184)\n",
      "Loss: 1.395 | Acc: 49.477% (14471/29248)\n",
      "Loss: 1.395 | Acc: 49.454% (14496/29312)\n",
      "Loss: 1.395 | Acc: 49.472% (14533/29376)\n",
      "Loss: 1.395 | Acc: 49.470% (14564/29440)\n",
      "Loss: 1.395 | Acc: 49.454% (14591/29504)\n",
      "Loss: 1.395 | Acc: 49.449% (14621/29568)\n",
      "Loss: 1.395 | Acc: 49.463% (14657/29632)\n",
      "Loss: 1.395 | Acc: 49.458% (14687/29696)\n",
      "Loss: 1.395 | Acc: 49.449% (14716/29760)\n",
      "Loss: 1.395 | Acc: 49.443% (14746/29824)\n",
      "Loss: 1.395 | Acc: 49.425% (14772/29888)\n",
      "Loss: 1.395 | Acc: 49.432% (14806/29952)\n",
      "Loss: 1.395 | Acc: 49.420% (14834/30016)\n",
      "Loss: 1.395 | Acc: 49.428% (14868/30080)\n",
      "Loss: 1.395 | Acc: 49.429% (14900/30144)\n",
      "Loss: 1.395 | Acc: 49.424% (14930/30208)\n",
      "Loss: 1.394 | Acc: 49.435% (14965/30272)\n",
      "Loss: 1.394 | Acc: 49.440% (14998/30336)\n",
      "Loss: 1.394 | Acc: 49.457% (15035/30400)\n",
      "Loss: 1.393 | Acc: 49.468% (15070/30464)\n",
      "Loss: 1.393 | Acc: 49.482% (15106/30528)\n",
      "Loss: 1.393 | Acc: 49.464% (15132/30592)\n",
      "Loss: 1.393 | Acc: 49.462% (15163/30656)\n",
      "Loss: 1.393 | Acc: 49.456% (15193/30720)\n",
      "Loss: 1.393 | Acc: 49.480% (15232/30784)\n",
      "Loss: 1.392 | Acc: 49.494% (15268/30848)\n",
      "Loss: 1.392 | Acc: 49.502% (15302/30912)\n",
      "Loss: 1.393 | Acc: 49.496% (15332/30976)\n",
      "Loss: 1.393 | Acc: 49.491% (15362/31040)\n",
      "Loss: 1.392 | Acc: 49.518% (15402/31104)\n",
      "Loss: 1.392 | Acc: 49.525% (15436/31168)\n",
      "Loss: 1.392 | Acc: 49.526% (15468/31232)\n",
      "Loss: 1.393 | Acc: 49.511% (15495/31296)\n",
      "Loss: 1.392 | Acc: 49.525% (15531/31360)\n",
      "Loss: 1.393 | Acc: 49.519% (15561/31424)\n",
      "Loss: 1.393 | Acc: 49.498% (15586/31488)\n",
      "Loss: 1.393 | Acc: 49.496% (15617/31552)\n",
      "Loss: 1.393 | Acc: 49.491% (15647/31616)\n",
      "Loss: 1.393 | Acc: 49.501% (15682/31680)\n",
      "Loss: 1.393 | Acc: 49.515% (15718/31744)\n",
      "Loss: 1.392 | Acc: 49.525% (15753/31808)\n",
      "Loss: 1.392 | Acc: 49.542% (15790/31872)\n",
      "Loss: 1.392 | Acc: 49.524% (15816/31936)\n",
      "Loss: 1.392 | Acc: 49.525% (15848/32000)\n",
      "Loss: 1.392 | Acc: 49.548% (15887/32064)\n",
      "Loss: 1.392 | Acc: 49.533% (15914/32128)\n",
      "Loss: 1.392 | Acc: 49.528% (15944/32192)\n",
      "Loss: 1.392 | Acc: 49.550% (15983/32256)\n",
      "Loss: 1.391 | Acc: 49.573% (16022/32320)\n",
      "Loss: 1.391 | Acc: 49.583% (16057/32384)\n",
      "Loss: 1.391 | Acc: 49.587% (16090/32448)\n",
      "Loss: 1.390 | Acc: 49.609% (16129/32512)\n",
      "Loss: 1.389 | Acc: 49.644% (16172/32576)\n",
      "Loss: 1.389 | Acc: 49.648% (16205/32640)\n",
      "Loss: 1.389 | Acc: 49.654% (16239/32704)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.388 | Acc: 49.661% (16273/32768)\n",
      "Loss: 1.388 | Acc: 49.662% (16305/32832)\n",
      "Loss: 1.388 | Acc: 49.641% (16330/32896)\n",
      "Loss: 1.388 | Acc: 49.642% (16362/32960)\n",
      "Loss: 1.389 | Acc: 49.640% (16393/33024)\n",
      "Loss: 1.389 | Acc: 49.649% (16428/33088)\n",
      "Loss: 1.389 | Acc: 49.647% (16459/33152)\n",
      "Loss: 1.390 | Acc: 49.606% (16477/33216)\n",
      "Loss: 1.390 | Acc: 49.594% (16505/33280)\n",
      "Loss: 1.390 | Acc: 49.571% (16529/33344)\n",
      "Loss: 1.390 | Acc: 49.557% (16556/33408)\n",
      "Loss: 1.390 | Acc: 49.567% (16591/33472)\n",
      "Loss: 1.390 | Acc: 49.574% (16625/33536)\n",
      "Loss: 1.390 | Acc: 49.568% (16655/33600)\n",
      "Loss: 1.390 | Acc: 49.566% (16686/33664)\n",
      "Loss: 1.390 | Acc: 49.573% (16720/33728)\n",
      "Loss: 1.390 | Acc: 49.580% (16754/33792)\n",
      "Loss: 1.390 | Acc: 49.575% (16784/33856)\n",
      "Loss: 1.390 | Acc: 49.573% (16815/33920)\n",
      "Loss: 1.390 | Acc: 49.570% (16846/33984)\n",
      "Loss: 1.390 | Acc: 49.559% (16874/34048)\n",
      "Loss: 1.390 | Acc: 49.554% (16904/34112)\n",
      "Loss: 1.390 | Acc: 49.567% (16940/34176)\n",
      "Loss: 1.390 | Acc: 49.565% (16971/34240)\n",
      "Loss: 1.390 | Acc: 49.566% (17003/34304)\n",
      "Loss: 1.390 | Acc: 49.575% (17038/34368)\n",
      "Loss: 1.390 | Acc: 49.561% (17065/34432)\n",
      "Loss: 1.390 | Acc: 49.571% (17100/34496)\n",
      "Loss: 1.390 | Acc: 49.598% (17141/34560)\n",
      "Loss: 1.390 | Acc: 49.575% (17165/34624)\n",
      "Loss: 1.390 | Acc: 49.591% (17202/34688)\n",
      "Loss: 1.390 | Acc: 49.594% (17235/34752)\n",
      "Loss: 1.390 | Acc: 49.598% (17268/34816)\n",
      "Loss: 1.390 | Acc: 49.593% (17298/34880)\n",
      "Loss: 1.390 | Acc: 49.602% (17333/34944)\n",
      "Loss: 1.390 | Acc: 49.597% (17363/35008)\n",
      "Loss: 1.390 | Acc: 49.601% (17396/35072)\n",
      "Loss: 1.389 | Acc: 49.630% (17438/35136)\n",
      "Loss: 1.389 | Acc: 49.636% (17472/35200)\n",
      "Loss: 1.389 | Acc: 49.637% (17504/35264)\n",
      "Loss: 1.389 | Acc: 49.643% (17538/35328)\n",
      "Loss: 1.390 | Acc: 49.624% (17563/35392)\n",
      "Loss: 1.390 | Acc: 49.625% (17595/35456)\n",
      "Loss: 1.390 | Acc: 49.642% (17633/35520)\n",
      "Loss: 1.390 | Acc: 49.635% (17662/35584)\n",
      "Loss: 1.390 | Acc: 49.649% (17699/35648)\n",
      "Loss: 1.390 | Acc: 49.639% (17727/35712)\n",
      "Loss: 1.390 | Acc: 49.653% (17764/35776)\n",
      "Loss: 1.390 | Acc: 49.646% (17793/35840)\n",
      "Loss: 1.390 | Acc: 49.643% (17824/35904)\n",
      "Loss: 1.390 | Acc: 49.636% (17853/35968)\n",
      "Loss: 1.390 | Acc: 49.623% (17880/36032)\n",
      "Loss: 1.390 | Acc: 49.626% (17913/36096)\n",
      "Loss: 1.390 | Acc: 49.621% (17943/36160)\n",
      "Loss: 1.390 | Acc: 49.622% (17975/36224)\n",
      "Loss: 1.390 | Acc: 49.614% (18004/36288)\n",
      "Loss: 1.390 | Acc: 49.618% (18037/36352)\n",
      "Loss: 1.390 | Acc: 49.618% (18069/36416)\n",
      "Loss: 1.390 | Acc: 49.616% (18100/36480)\n",
      "Loss: 1.390 | Acc: 49.600% (18126/36544)\n",
      "Loss: 1.390 | Acc: 49.598% (18157/36608)\n",
      "Loss: 1.390 | Acc: 49.607% (18192/36672)\n",
      "Loss: 1.390 | Acc: 49.616% (18227/36736)\n",
      "Loss: 1.391 | Acc: 49.601% (18253/36800)\n",
      "Loss: 1.390 | Acc: 49.626% (18294/36864)\n",
      "Loss: 1.390 | Acc: 49.637% (18330/36928)\n",
      "Loss: 1.390 | Acc: 49.662% (18371/36992)\n",
      "Loss: 1.390 | Acc: 49.660% (18402/37056)\n",
      "Loss: 1.390 | Acc: 49.650% (18430/37120)\n",
      "Loss: 1.390 | Acc: 49.653% (18463/37184)\n",
      "Loss: 1.389 | Acc: 49.654% (18495/37248)\n",
      "Loss: 1.390 | Acc: 49.660% (18529/37312)\n",
      "Loss: 1.390 | Acc: 49.644% (18555/37376)\n",
      "Loss: 1.390 | Acc: 49.647% (18588/37440)\n",
      "Loss: 1.390 | Acc: 49.651% (18621/37504)\n",
      "Loss: 1.390 | Acc: 49.641% (18649/37568)\n",
      "Loss: 1.390 | Acc: 49.633% (18678/37632)\n",
      "Loss: 1.390 | Acc: 49.623% (18706/37696)\n",
      "Loss: 1.390 | Acc: 49.627% (18739/37760)\n",
      "Loss: 1.390 | Acc: 49.611% (18765/37824)\n",
      "Loss: 1.390 | Acc: 49.617% (18799/37888)\n",
      "Loss: 1.390 | Acc: 49.618% (18831/37952)\n",
      "Loss: 1.390 | Acc: 49.626% (18866/38016)\n",
      "Loss: 1.390 | Acc: 49.624% (18897/38080)\n",
      "Loss: 1.390 | Acc: 49.630% (18931/38144)\n",
      "Loss: 1.390 | Acc: 49.620% (18959/38208)\n",
      "Loss: 1.390 | Acc: 49.611% (18987/38272)\n",
      "Loss: 1.391 | Acc: 49.593% (19012/38336)\n",
      "Loss: 1.390 | Acc: 49.602% (19047/38400)\n",
      "Loss: 1.390 | Acc: 49.610% (19082/38464)\n",
      "Loss: 1.391 | Acc: 49.590% (19106/38528)\n",
      "Loss: 1.391 | Acc: 49.598% (19141/38592)\n",
      "Loss: 1.391 | Acc: 49.584% (19167/38656)\n",
      "Loss: 1.391 | Acc: 49.582% (19198/38720)\n",
      "Loss: 1.391 | Acc: 49.593% (19234/38784)\n",
      "Loss: 1.391 | Acc: 49.601% (19269/38848)\n",
      "Loss: 1.390 | Acc: 49.594% (19298/38912)\n",
      "Loss: 1.390 | Acc: 49.587% (19327/38976)\n",
      "Loss: 1.391 | Acc: 49.575% (19354/39040)\n",
      "Loss: 1.390 | Acc: 49.591% (19392/39104)\n",
      "Loss: 1.390 | Acc: 49.617% (19434/39168)\n",
      "Loss: 1.390 | Acc: 49.620% (19467/39232)\n",
      "Loss: 1.390 | Acc: 49.623% (19500/39296)\n",
      "Loss: 1.390 | Acc: 49.627% (19533/39360)\n",
      "Loss: 1.390 | Acc: 49.627% (19565/39424)\n",
      "Loss: 1.390 | Acc: 49.628% (19597/39488)\n",
      "Loss: 1.389 | Acc: 49.654% (19639/39552)\n",
      "Loss: 1.389 | Acc: 49.649% (19669/39616)\n",
      "Loss: 1.389 | Acc: 49.637% (19696/39680)\n",
      "Loss: 1.390 | Acc: 49.618% (19720/39744)\n",
      "Loss: 1.390 | Acc: 49.611% (19749/39808)\n",
      "Loss: 1.390 | Acc: 49.604% (19778/39872)\n",
      "Loss: 1.390 | Acc: 49.599% (19808/39936)\n",
      "Loss: 1.390 | Acc: 49.595% (19838/40000)\n",
      "Loss: 1.389 | Acc: 49.603% (19873/40064)\n",
      "Loss: 1.389 | Acc: 49.609% (19907/40128)\n",
      "Loss: 1.389 | Acc: 49.607% (19938/40192)\n",
      "Loss: 1.389 | Acc: 49.612% (19972/40256)\n",
      "Loss: 1.389 | Acc: 49.618% (20006/40320)\n",
      "Loss: 1.388 | Acc: 49.631% (20043/40384)\n",
      "Loss: 1.388 | Acc: 49.646% (20081/40448)\n",
      "Loss: 1.388 | Acc: 49.652% (20115/40512)\n",
      "Loss: 1.388 | Acc: 49.653% (20147/40576)\n",
      "Loss: 1.388 | Acc: 49.660% (20182/40640)\n",
      "Loss: 1.388 | Acc: 49.656% (20212/40704)\n",
      "Loss: 1.388 | Acc: 49.644% (20239/40768)\n",
      "Loss: 1.388 | Acc: 49.652% (20274/40832)\n",
      "Loss: 1.389 | Acc: 49.641% (20301/40896)\n",
      "Loss: 1.388 | Acc: 49.658% (20340/40960)\n",
      "Loss: 1.389 | Acc: 49.642% (20365/41024)\n",
      "Loss: 1.388 | Acc: 49.640% (20396/41088)\n",
      "Loss: 1.388 | Acc: 49.638% (20427/41152)\n",
      "Loss: 1.388 | Acc: 49.655% (20466/41216)\n",
      "Loss: 1.388 | Acc: 49.654% (20497/41280)\n",
      "Loss: 1.388 | Acc: 49.669% (20535/41344)\n",
      "Loss: 1.388 | Acc: 49.655% (20561/41408)\n",
      "Loss: 1.388 | Acc: 49.660% (20595/41472)\n",
      "Loss: 1.388 | Acc: 49.675% (20633/41536)\n",
      "Loss: 1.388 | Acc: 49.675% (20665/41600)\n",
      "Loss: 1.388 | Acc: 49.671% (20695/41664)\n",
      "Loss: 1.387 | Acc: 49.681% (20731/41728)\n",
      "Loss: 1.387 | Acc: 49.682% (20763/41792)\n",
      "Loss: 1.387 | Acc: 49.680% (20794/41856)\n",
      "Loss: 1.387 | Acc: 49.673% (20823/41920)\n",
      "Loss: 1.386 | Acc: 49.669% (20853/41984)\n",
      "Loss: 1.387 | Acc: 49.662% (20882/42048)\n",
      "Loss: 1.387 | Acc: 49.670% (20917/42112)\n",
      "Loss: 1.386 | Acc: 49.673% (20950/42176)\n",
      "Loss: 1.386 | Acc: 49.680% (20985/42240)\n",
      "Loss: 1.386 | Acc: 49.681% (21017/42304)\n",
      "Loss: 1.386 | Acc: 49.674% (21046/42368)\n",
      "Loss: 1.386 | Acc: 49.672% (21077/42432)\n",
      "Loss: 1.386 | Acc: 49.668% (21107/42496)\n",
      "Loss: 1.386 | Acc: 49.659% (21135/42560)\n",
      "Loss: 1.386 | Acc: 49.660% (21167/42624)\n",
      "Loss: 1.386 | Acc: 49.667% (21202/42688)\n",
      "Loss: 1.386 | Acc: 49.670% (21235/42752)\n",
      "Loss: 1.386 | Acc: 49.673% (21268/42816)\n",
      "Loss: 1.386 | Acc: 49.676% (21301/42880)\n",
      "Loss: 1.386 | Acc: 49.690% (21339/42944)\n",
      "Loss: 1.385 | Acc: 49.688% (21370/43008)\n",
      "Loss: 1.385 | Acc: 49.701% (21407/43072)\n",
      "Loss: 1.385 | Acc: 49.699% (21438/43136)\n",
      "Loss: 1.385 | Acc: 49.699% (21470/43200)\n",
      "Loss: 1.385 | Acc: 49.688% (21497/43264)\n",
      "Loss: 1.385 | Acc: 49.686% (21528/43328)\n",
      "Loss: 1.385 | Acc: 49.691% (21562/43392)\n",
      "Loss: 1.385 | Acc: 49.689% (21593/43456)\n",
      "Loss: 1.385 | Acc: 49.694% (21627/43520)\n",
      "Loss: 1.385 | Acc: 49.704% (21663/43584)\n",
      "Loss: 1.385 | Acc: 49.704% (21695/43648)\n",
      "Loss: 1.385 | Acc: 49.714% (21731/43712)\n",
      "Loss: 1.385 | Acc: 49.735% (21772/43776)\n",
      "Loss: 1.385 | Acc: 49.745% (21808/43840)\n",
      "Loss: 1.385 | Acc: 49.734% (21835/43904)\n",
      "Loss: 1.385 | Acc: 49.748% (21873/43968)\n",
      "Loss: 1.385 | Acc: 49.750% (21906/44032)\n",
      "Loss: 1.384 | Acc: 49.746% (21936/44096)\n",
      "Loss: 1.384 | Acc: 49.758% (21973/44160)\n",
      "Loss: 1.384 | Acc: 49.760% (22006/44224)\n",
      "Loss: 1.384 | Acc: 49.770% (22042/44288)\n",
      "Loss: 1.385 | Acc: 49.750% (22065/44352)\n",
      "Loss: 1.385 | Acc: 49.757% (22100/44416)\n",
      "Loss: 1.385 | Acc: 49.746% (22127/44480)\n",
      "Loss: 1.385 | Acc: 49.751% (22161/44544)\n",
      "Loss: 1.385 | Acc: 49.751% (22193/44608)\n",
      "Loss: 1.384 | Acc: 49.752% (22225/44672)\n",
      "Loss: 1.384 | Acc: 49.750% (22256/44736)\n",
      "Loss: 1.384 | Acc: 49.757% (22291/44800)\n",
      "Loss: 1.384 | Acc: 49.755% (22322/44864)\n",
      "Loss: 1.384 | Acc: 49.764% (22358/44928)\n",
      "Loss: 1.384 | Acc: 49.764% (22390/44992)\n",
      "Loss: 1.384 | Acc: 49.767% (22423/45056)\n",
      "Loss: 1.384 | Acc: 49.772% (22457/45120)\n",
      "Loss: 1.384 | Acc: 49.781% (22493/45184)\n",
      "Loss: 1.383 | Acc: 49.797% (22532/45248)\n",
      "Loss: 1.384 | Acc: 49.790% (22561/45312)\n",
      "Loss: 1.384 | Acc: 49.791% (22593/45376)\n",
      "Loss: 1.384 | Acc: 49.800% (22629/45440)\n",
      "Loss: 1.384 | Acc: 49.789% (22656/45504)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.384 | Acc: 49.776% (22682/45568)\n",
      "Loss: 1.384 | Acc: 49.776% (22714/45632)\n",
      "Loss: 1.384 | Acc: 49.786% (22750/45696)\n",
      "Loss: 1.384 | Acc: 49.792% (22785/45760)\n",
      "Loss: 1.384 | Acc: 49.795% (22818/45824)\n",
      "Loss: 1.384 | Acc: 49.806% (22855/45888)\n",
      "Loss: 1.384 | Acc: 49.800% (22884/45952)\n",
      "Loss: 1.384 | Acc: 49.802% (22917/46016)\n",
      "Loss: 1.384 | Acc: 49.807% (22951/46080)\n",
      "Loss: 1.383 | Acc: 49.814% (22986/46144)\n",
      "Loss: 1.383 | Acc: 49.818% (23020/46208)\n",
      "Loss: 1.383 | Acc: 49.816% (23051/46272)\n",
      "Loss: 1.383 | Acc: 49.810% (23080/46336)\n",
      "Loss: 1.383 | Acc: 49.819% (23116/46400)\n",
      "Loss: 1.383 | Acc: 49.832% (23154/46464)\n",
      "Loss: 1.383 | Acc: 49.835% (23187/46528)\n",
      "Loss: 1.383 | Acc: 49.835% (23219/46592)\n",
      "Loss: 1.383 | Acc: 49.818% (23243/46656)\n",
      "Loss: 1.382 | Acc: 49.839% (23285/46720)\n",
      "Loss: 1.382 | Acc: 49.846% (23320/46784)\n",
      "Loss: 1.382 | Acc: 49.853% (23355/46848)\n",
      "Loss: 1.382 | Acc: 49.844% (23383/46912)\n",
      "Loss: 1.382 | Acc: 49.855% (23420/46976)\n",
      "Loss: 1.382 | Acc: 49.868% (23458/47040)\n",
      "Loss: 1.382 | Acc: 49.862% (23487/47104)\n",
      "Loss: 1.382 | Acc: 49.854% (23515/47168)\n",
      "Loss: 1.382 | Acc: 49.848% (23544/47232)\n",
      "Loss: 1.382 | Acc: 49.854% (23579/47296)\n",
      "Loss: 1.382 | Acc: 49.846% (23607/47360)\n",
      "Loss: 1.382 | Acc: 49.840% (23636/47424)\n",
      "Loss: 1.383 | Acc: 49.832% (23664/47488)\n",
      "Loss: 1.382 | Acc: 49.830% (23695/47552)\n",
      "Loss: 1.383 | Acc: 49.821% (23723/47616)\n",
      "Loss: 1.383 | Acc: 49.824% (23756/47680)\n",
      "Loss: 1.383 | Acc: 49.824% (23788/47744)\n",
      "Loss: 1.382 | Acc: 49.822% (23819/47808)\n",
      "Loss: 1.382 | Acc: 49.829% (23854/47872)\n",
      "Loss: 1.382 | Acc: 49.833% (23888/47936)\n",
      "Loss: 1.382 | Acc: 49.819% (23913/48000)\n",
      "Loss: 1.382 | Acc: 49.809% (23940/48064)\n",
      "Loss: 1.382 | Acc: 49.805% (23970/48128)\n",
      "Loss: 1.382 | Acc: 49.795% (23997/48192)\n",
      "Loss: 1.382 | Acc: 49.807% (24035/48256)\n",
      "Loss: 1.382 | Acc: 49.810% (24068/48320)\n",
      "Loss: 1.382 | Acc: 49.802% (24096/48384)\n",
      "Loss: 1.382 | Acc: 49.796% (24125/48448)\n",
      "Loss: 1.382 | Acc: 49.794% (24156/48512)\n",
      "Loss: 1.382 | Acc: 49.796% (24189/48576)\n",
      "Loss: 1.382 | Acc: 49.794% (24220/48640)\n",
      "Loss: 1.382 | Acc: 49.795% (24252/48704)\n",
      "Loss: 1.382 | Acc: 49.793% (24283/48768)\n",
      "Loss: 1.382 | Acc: 49.793% (24315/48832)\n",
      "Loss: 1.382 | Acc: 49.795% (24348/48896)\n",
      "Loss: 1.382 | Acc: 49.796% (24380/48960)\n",
      "Loss: 1.382 | Acc: 49.802% (24403/49000)\n",
      "Epoch 1 of training is completed, Training accuracy for this epoch is 49.80204081632653\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.356 | Acc: 50.000% (32/64)\n",
      "Loss: 1.258 | Acc: 53.906% (69/128)\n",
      "Loss: 1.339 | Acc: 48.958% (94/192)\n",
      "Loss: 1.419 | Acc: 48.047% (123/256)\n",
      "Loss: 1.404 | Acc: 50.625% (162/320)\n",
      "Loss: 1.437 | Acc: 48.958% (188/384)\n",
      "Loss: 1.449 | Acc: 47.768% (214/448)\n",
      "Loss: 1.452 | Acc: 47.070% (241/512)\n",
      "Loss: 1.432 | Acc: 48.090% (277/576)\n",
      "Loss: 1.411 | Acc: 48.594% (311/640)\n",
      "Loss: 1.418 | Acc: 48.438% (341/704)\n",
      "Loss: 1.416 | Acc: 48.568% (373/768)\n",
      "Loss: 1.411 | Acc: 48.798% (406/832)\n",
      "Loss: 1.413 | Acc: 48.438% (434/896)\n",
      "Loss: 1.394 | Acc: 48.958% (470/960)\n",
      "Loss: 1.379 | Acc: 49.609% (508/1024)\n",
      "Loss: 1.387 | Acc: 49.632% (540/1088)\n",
      "Loss: 1.385 | Acc: 49.566% (571/1152)\n",
      "Loss: 1.387 | Acc: 49.589% (603/1216)\n",
      "Loss: 1.394 | Acc: 49.609% (635/1280)\n",
      "Loss: 1.400 | Acc: 49.405% (664/1344)\n",
      "Loss: 1.400 | Acc: 49.219% (693/1408)\n",
      "Loss: 1.392 | Acc: 49.389% (727/1472)\n",
      "Loss: 1.394 | Acc: 49.609% (762/1536)\n",
      "Loss: 1.397 | Acc: 49.375% (790/1600)\n",
      "Loss: 1.400 | Acc: 49.279% (820/1664)\n",
      "Loss: 1.397 | Acc: 49.479% (855/1728)\n",
      "Loss: 1.400 | Acc: 49.442% (886/1792)\n",
      "Loss: 1.404 | Acc: 49.300% (915/1856)\n",
      "Loss: 1.406 | Acc: 49.323% (947/1920)\n",
      "Loss: 1.405 | Acc: 49.294% (978/1984)\n",
      "Loss: 1.405 | Acc: 49.121% (1006/2048)\n",
      "Loss: 1.399 | Acc: 49.479% (1045/2112)\n",
      "Loss: 1.398 | Acc: 49.449% (1076/2176)\n",
      "Loss: 1.396 | Acc: 49.509% (1109/2240)\n",
      "Loss: 1.398 | Acc: 49.132% (1132/2304)\n",
      "Loss: 1.399 | Acc: 48.986% (1160/2368)\n",
      "Loss: 1.398 | Acc: 48.972% (1191/2432)\n",
      "Loss: 1.396 | Acc: 49.159% (1227/2496)\n",
      "Loss: 1.405 | Acc: 49.062% (1256/2560)\n",
      "Loss: 1.403 | Acc: 49.085% (1288/2624)\n",
      "Loss: 1.403 | Acc: 49.070% (1319/2688)\n",
      "Loss: 1.402 | Acc: 49.092% (1351/2752)\n",
      "Loss: 1.403 | Acc: 49.148% (1384/2816)\n",
      "Loss: 1.401 | Acc: 49.201% (1417/2880)\n",
      "Loss: 1.397 | Acc: 49.423% (1455/2944)\n",
      "Loss: 1.396 | Acc: 49.435% (1487/3008)\n",
      "Loss: 1.395 | Acc: 49.642% (1525/3072)\n",
      "Loss: 1.392 | Acc: 49.585% (1555/3136)\n",
      "Loss: 1.393 | Acc: 49.625% (1588/3200)\n",
      "Loss: 1.392 | Acc: 49.632% (1620/3264)\n",
      "Loss: 1.392 | Acc: 49.609% (1651/3328)\n",
      "Loss: 1.393 | Acc: 49.587% (1682/3392)\n",
      "Loss: 1.393 | Acc: 49.624% (1715/3456)\n",
      "Loss: 1.396 | Acc: 49.403% (1739/3520)\n",
      "Loss: 1.392 | Acc: 49.470% (1773/3584)\n",
      "Loss: 1.392 | Acc: 49.452% (1804/3648)\n",
      "Loss: 1.389 | Acc: 49.407% (1834/3712)\n",
      "Loss: 1.393 | Acc: 49.206% (1858/3776)\n",
      "Loss: 1.392 | Acc: 49.245% (1891/3840)\n",
      "Loss: 1.390 | Acc: 49.206% (1921/3904)\n",
      "Loss: 1.391 | Acc: 49.294% (1956/3968)\n",
      "Loss: 1.391 | Acc: 49.405% (1992/4032)\n",
      "Loss: 1.392 | Acc: 49.512% (2028/4096)\n",
      "Loss: 1.393 | Acc: 49.591% (2063/4160)\n",
      "Loss: 1.391 | Acc: 49.621% (2096/4224)\n",
      "Loss: 1.392 | Acc: 49.580% (2126/4288)\n",
      "Loss: 1.390 | Acc: 49.609% (2159/4352)\n",
      "Loss: 1.386 | Acc: 49.819% (2200/4416)\n",
      "Loss: 1.384 | Acc: 49.866% (2234/4480)\n",
      "Loss: 1.383 | Acc: 49.890% (2267/4544)\n",
      "Loss: 1.384 | Acc: 49.870% (2298/4608)\n",
      "Loss: 1.382 | Acc: 50.000% (2336/4672)\n",
      "Loss: 1.382 | Acc: 49.958% (2366/4736)\n",
      "Loss: 1.383 | Acc: 50.042% (2402/4800)\n",
      "Loss: 1.382 | Acc: 50.041% (2434/4864)\n",
      "Loss: 1.380 | Acc: 50.061% (2467/4928)\n",
      "Loss: 1.380 | Acc: 50.060% (2499/4992)\n",
      "Loss: 1.380 | Acc: 50.099% (2533/5056)\n",
      "Loss: 1.384 | Acc: 50.020% (2561/5120)\n",
      "Loss: 1.385 | Acc: 49.981% (2591/5184)\n",
      "Loss: 1.386 | Acc: 49.867% (2617/5248)\n",
      "Loss: 1.384 | Acc: 49.981% (2655/5312)\n",
      "Loss: 1.386 | Acc: 49.926% (2684/5376)\n",
      "Loss: 1.386 | Acc: 49.982% (2719/5440)\n",
      "Loss: 1.385 | Acc: 49.982% (2751/5504)\n",
      "Loss: 1.388 | Acc: 49.856% (2776/5568)\n",
      "Loss: 1.390 | Acc: 49.822% (2806/5632)\n",
      "Loss: 1.391 | Acc: 49.789% (2836/5696)\n",
      "Loss: 1.389 | Acc: 49.861% (2872/5760)\n",
      "Loss: 1.389 | Acc: 49.931% (2908/5824)\n",
      "Loss: 1.390 | Acc: 49.864% (2936/5888)\n",
      "Loss: 1.392 | Acc: 49.765% (2962/5952)\n",
      "Loss: 1.392 | Acc: 49.734% (2992/6016)\n",
      "Loss: 1.393 | Acc: 49.605% (3016/6080)\n",
      "Loss: 1.393 | Acc: 49.626% (3049/6144)\n",
      "Loss: 1.393 | Acc: 49.646% (3082/6208)\n",
      "Loss: 1.396 | Acc: 49.617% (3112/6272)\n",
      "Loss: 1.396 | Acc: 49.637% (3145/6336)\n",
      "Loss: 1.397 | Acc: 49.625% (3176/6400)\n",
      "Loss: 1.397 | Acc: 49.598% (3206/6464)\n",
      "Loss: 1.397 | Acc: 49.617% (3239/6528)\n",
      "Loss: 1.400 | Acc: 49.439% (3259/6592)\n",
      "Loss: 1.398 | Acc: 49.504% (3295/6656)\n",
      "Loss: 1.398 | Acc: 49.479% (3325/6720)\n",
      "Loss: 1.397 | Acc: 49.528% (3360/6784)\n",
      "Loss: 1.394 | Acc: 49.606% (3397/6848)\n",
      "Loss: 1.398 | Acc: 49.479% (3420/6912)\n",
      "Loss: 1.399 | Acc: 49.412% (3447/6976)\n",
      "Loss: 1.400 | Acc: 49.418% (3479/7040)\n",
      "Loss: 1.402 | Acc: 49.324% (3504/7104)\n",
      "Loss: 1.402 | Acc: 49.372% (3539/7168)\n",
      "Loss: 1.403 | Acc: 49.336% (3568/7232)\n",
      "Loss: 1.402 | Acc: 49.315% (3598/7296)\n",
      "Loss: 1.400 | Acc: 49.389% (3635/7360)\n",
      "Loss: 1.401 | Acc: 49.300% (3660/7424)\n",
      "Loss: 1.399 | Acc: 49.346% (3695/7488)\n",
      "Loss: 1.400 | Acc: 49.338% (3726/7552)\n",
      "Loss: 1.401 | Acc: 49.304% (3755/7616)\n",
      "Loss: 1.400 | Acc: 49.323% (3788/7680)\n",
      "Loss: 1.398 | Acc: 49.380% (3824/7744)\n",
      "Loss: 1.398 | Acc: 49.411% (3858/7808)\n",
      "Loss: 1.400 | Acc: 49.390% (3888/7872)\n",
      "Loss: 1.400 | Acc: 49.370% (3918/7936)\n",
      "Loss: 1.400 | Acc: 49.413% (3953/8000)\n",
      "Loss: 1.400 | Acc: 49.430% (3986/8064)\n",
      "Loss: 1.400 | Acc: 49.459% (4020/8128)\n",
      "Loss: 1.399 | Acc: 49.475% (4053/8192)\n",
      "Loss: 1.399 | Acc: 49.431% (4081/8256)\n",
      "Loss: 1.401 | Acc: 49.327% (4104/8320)\n",
      "Loss: 1.401 | Acc: 49.344% (4137/8384)\n",
      "Loss: 1.401 | Acc: 49.373% (4171/8448)\n",
      "Loss: 1.402 | Acc: 49.330% (4199/8512)\n",
      "Loss: 1.403 | Acc: 49.335% (4231/8576)\n",
      "Loss: 1.402 | Acc: 49.329% (4262/8640)\n",
      "Loss: 1.403 | Acc: 49.357% (4296/8704)\n",
      "Loss: 1.404 | Acc: 49.304% (4323/8768)\n",
      "Loss: 1.403 | Acc: 49.332% (4357/8832)\n",
      "Loss: 1.402 | Acc: 49.326% (4388/8896)\n",
      "Loss: 1.403 | Acc: 49.286% (4416/8960)\n",
      "Loss: 1.402 | Acc: 49.313% (4450/9024)\n",
      "Loss: 1.404 | Acc: 49.230% (4474/9088)\n",
      "Loss: 1.405 | Acc: 49.246% (4507/9152)\n",
      "Loss: 1.403 | Acc: 49.295% (4543/9216)\n",
      "Loss: 1.403 | Acc: 49.256% (4571/9280)\n",
      "Loss: 1.403 | Acc: 49.187% (4596/9344)\n",
      "Loss: 1.403 | Acc: 49.192% (4628/9408)\n",
      "Loss: 1.404 | Acc: 49.177% (4658/9472)\n",
      "Loss: 1.404 | Acc: 49.151% (4687/9536)\n",
      "Loss: 1.403 | Acc: 49.188% (4722/9600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.403 | Acc: 49.172% (4752/9664)\n",
      "Loss: 1.403 | Acc: 49.157% (4782/9728)\n",
      "Loss: 1.404 | Acc: 49.132% (4811/9792)\n",
      "Loss: 1.404 | Acc: 49.087% (4838/9856)\n",
      "Loss: 1.405 | Acc: 49.052% (4866/9920)\n",
      "Loss: 1.406 | Acc: 48.998% (4892/9984)\n",
      "Loss: 1.405 | Acc: 49.000% (4900/10000)\n",
      "Evaluation of Epoch 1 is completed, Test accuracy for this epoch is 49.0\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.491 | Acc: 45.312% (29/64)\n",
      "Loss: 1.357 | Acc: 49.219% (63/128)\n",
      "Loss: 1.278 | Acc: 50.521% (97/192)\n",
      "Loss: 1.265 | Acc: 53.516% (137/256)\n",
      "Loss: 1.266 | Acc: 53.438% (171/320)\n",
      "Loss: 1.228 | Acc: 54.167% (208/384)\n",
      "Loss: 1.282 | Acc: 52.902% (237/448)\n",
      "Loss: 1.324 | Acc: 51.953% (266/512)\n",
      "Loss: 1.340 | Acc: 51.389% (296/576)\n",
      "Loss: 1.335 | Acc: 51.719% (331/640)\n",
      "Loss: 1.342 | Acc: 50.994% (359/704)\n",
      "Loss: 1.334 | Acc: 51.042% (392/768)\n",
      "Loss: 1.327 | Acc: 51.683% (430/832)\n",
      "Loss: 1.326 | Acc: 51.786% (464/896)\n",
      "Loss: 1.336 | Acc: 51.458% (494/960)\n",
      "Loss: 1.330 | Acc: 51.367% (526/1024)\n",
      "Loss: 1.325 | Acc: 51.654% (562/1088)\n",
      "Loss: 1.319 | Acc: 51.823% (597/1152)\n",
      "Loss: 1.320 | Acc: 51.316% (624/1216)\n",
      "Loss: 1.317 | Acc: 51.328% (657/1280)\n",
      "Loss: 1.302 | Acc: 51.711% (695/1344)\n",
      "Loss: 1.303 | Acc: 51.562% (726/1408)\n",
      "Loss: 1.295 | Acc: 52.038% (766/1472)\n",
      "Loss: 1.294 | Acc: 52.018% (799/1536)\n",
      "Loss: 1.287 | Acc: 52.625% (842/1600)\n",
      "Loss: 1.283 | Acc: 52.764% (878/1664)\n",
      "Loss: 1.283 | Acc: 52.778% (912/1728)\n",
      "Loss: 1.282 | Acc: 52.958% (949/1792)\n",
      "Loss: 1.277 | Acc: 53.179% (987/1856)\n",
      "Loss: 1.278 | Acc: 53.177% (1021/1920)\n",
      "Loss: 1.281 | Acc: 52.923% (1050/1984)\n",
      "Loss: 1.280 | Acc: 52.930% (1084/2048)\n",
      "Loss: 1.276 | Acc: 53.220% (1124/2112)\n",
      "Loss: 1.273 | Acc: 53.493% (1164/2176)\n",
      "Loss: 1.272 | Acc: 53.571% (1200/2240)\n",
      "Loss: 1.270 | Acc: 53.602% (1235/2304)\n",
      "Loss: 1.272 | Acc: 53.547% (1268/2368)\n",
      "Loss: 1.273 | Acc: 53.372% (1298/2432)\n",
      "Loss: 1.275 | Acc: 53.285% (1330/2496)\n",
      "Loss: 1.278 | Acc: 53.086% (1359/2560)\n",
      "Loss: 1.277 | Acc: 53.239% (1397/2624)\n",
      "Loss: 1.280 | Acc: 53.125% (1428/2688)\n",
      "Loss: 1.279 | Acc: 53.270% (1466/2752)\n",
      "Loss: 1.280 | Acc: 53.196% (1498/2816)\n",
      "Loss: 1.277 | Acc: 53.264% (1534/2880)\n",
      "Loss: 1.272 | Acc: 53.329% (1570/2944)\n",
      "Loss: 1.270 | Acc: 53.324% (1604/3008)\n",
      "Loss: 1.269 | Acc: 53.288% (1637/3072)\n",
      "Loss: 1.270 | Acc: 53.253% (1670/3136)\n",
      "Loss: 1.269 | Acc: 53.375% (1708/3200)\n",
      "Loss: 1.267 | Acc: 53.401% (1743/3264)\n",
      "Loss: 1.264 | Acc: 53.425% (1778/3328)\n",
      "Loss: 1.266 | Acc: 53.302% (1808/3392)\n",
      "Loss: 1.265 | Acc: 53.501% (1849/3456)\n",
      "Loss: 1.263 | Acc: 53.551% (1885/3520)\n",
      "Loss: 1.263 | Acc: 53.683% (1924/3584)\n",
      "Loss: 1.259 | Acc: 53.920% (1967/3648)\n",
      "Loss: 1.260 | Acc: 53.745% (1995/3712)\n",
      "Loss: 1.261 | Acc: 53.867% (2034/3776)\n",
      "Loss: 1.264 | Acc: 53.854% (2068/3840)\n",
      "Loss: 1.260 | Acc: 54.047% (2110/3904)\n",
      "Loss: 1.260 | Acc: 54.057% (2145/3968)\n",
      "Loss: 1.258 | Acc: 54.092% (2181/4032)\n",
      "Loss: 1.259 | Acc: 54.077% (2215/4096)\n",
      "Loss: 1.262 | Acc: 54.038% (2248/4160)\n",
      "Loss: 1.262 | Acc: 54.072% (2284/4224)\n",
      "Loss: 1.263 | Acc: 54.035% (2317/4288)\n",
      "Loss: 1.261 | Acc: 54.067% (2353/4352)\n",
      "Loss: 1.266 | Acc: 53.918% (2381/4416)\n",
      "Loss: 1.266 | Acc: 53.996% (2419/4480)\n",
      "Loss: 1.268 | Acc: 53.917% (2450/4544)\n",
      "Loss: 1.264 | Acc: 54.102% (2493/4608)\n",
      "Loss: 1.265 | Acc: 54.003% (2523/4672)\n",
      "Loss: 1.266 | Acc: 53.948% (2555/4736)\n",
      "Loss: 1.267 | Acc: 54.000% (2592/4800)\n",
      "Loss: 1.264 | Acc: 53.988% (2626/4864)\n",
      "Loss: 1.267 | Acc: 53.957% (2659/4928)\n",
      "Loss: 1.266 | Acc: 54.067% (2699/4992)\n",
      "Loss: 1.265 | Acc: 54.114% (2736/5056)\n",
      "Loss: 1.265 | Acc: 54.102% (2770/5120)\n",
      "Loss: 1.265 | Acc: 54.070% (2803/5184)\n",
      "Loss: 1.265 | Acc: 54.078% (2838/5248)\n",
      "Loss: 1.265 | Acc: 53.991% (2868/5312)\n",
      "Loss: 1.265 | Acc: 53.943% (2900/5376)\n",
      "Loss: 1.265 | Acc: 53.915% (2933/5440)\n",
      "Loss: 1.265 | Acc: 53.961% (2970/5504)\n",
      "Loss: 1.266 | Acc: 53.987% (3006/5568)\n",
      "Loss: 1.267 | Acc: 53.960% (3039/5632)\n",
      "Loss: 1.266 | Acc: 54.003% (3076/5696)\n",
      "Loss: 1.263 | Acc: 54.219% (3123/5760)\n",
      "Loss: 1.262 | Acc: 54.258% (3160/5824)\n",
      "Loss: 1.263 | Acc: 54.229% (3193/5888)\n",
      "Loss: 1.260 | Acc: 54.318% (3233/5952)\n",
      "Loss: 1.258 | Acc: 54.388% (3272/6016)\n",
      "Loss: 1.259 | Acc: 54.342% (3304/6080)\n",
      "Loss: 1.260 | Acc: 54.313% (3337/6144)\n",
      "Loss: 1.262 | Acc: 54.349% (3374/6208)\n",
      "Loss: 1.262 | Acc: 54.353% (3409/6272)\n",
      "Loss: 1.265 | Acc: 54.293% (3440/6336)\n",
      "Loss: 1.264 | Acc: 54.359% (3479/6400)\n",
      "Loss: 1.264 | Acc: 54.332% (3512/6464)\n",
      "Loss: 1.264 | Acc: 54.305% (3545/6528)\n",
      "Loss: 1.266 | Acc: 54.263% (3577/6592)\n",
      "Loss: 1.265 | Acc: 54.237% (3610/6656)\n",
      "Loss: 1.266 | Acc: 54.226% (3644/6720)\n",
      "Loss: 1.265 | Acc: 54.260% (3681/6784)\n",
      "Loss: 1.266 | Acc: 54.206% (3712/6848)\n",
      "Loss: 1.266 | Acc: 54.282% (3752/6912)\n",
      "Loss: 1.267 | Acc: 54.200% (3781/6976)\n",
      "Loss: 1.267 | Acc: 54.190% (3815/7040)\n",
      "Loss: 1.266 | Acc: 54.265% (3855/7104)\n",
      "Loss: 1.266 | Acc: 54.311% (3893/7168)\n",
      "Loss: 1.268 | Acc: 54.287% (3926/7232)\n",
      "Loss: 1.266 | Acc: 54.345% (3965/7296)\n",
      "Loss: 1.265 | Acc: 54.361% (4001/7360)\n",
      "Loss: 1.266 | Acc: 54.364% (4036/7424)\n",
      "Loss: 1.265 | Acc: 54.354% (4070/7488)\n",
      "Loss: 1.265 | Acc: 54.383% (4107/7552)\n",
      "Loss: 1.264 | Acc: 54.464% (4148/7616)\n",
      "Loss: 1.264 | Acc: 54.479% (4184/7680)\n",
      "Loss: 1.265 | Acc: 54.429% (4215/7744)\n",
      "Loss: 1.265 | Acc: 54.470% (4253/7808)\n",
      "Loss: 1.266 | Acc: 54.433% (4285/7872)\n",
      "Loss: 1.268 | Acc: 54.360% (4314/7936)\n",
      "Loss: 1.267 | Acc: 54.450% (4356/8000)\n",
      "Loss: 1.267 | Acc: 54.415% (4388/8064)\n",
      "Loss: 1.268 | Acc: 54.343% (4417/8128)\n",
      "Loss: 1.268 | Acc: 54.370% (4454/8192)\n",
      "Loss: 1.268 | Acc: 54.385% (4490/8256)\n",
      "Loss: 1.268 | Acc: 54.363% (4523/8320)\n",
      "Loss: 1.269 | Acc: 54.354% (4557/8384)\n",
      "Loss: 1.270 | Acc: 54.250% (4583/8448)\n",
      "Loss: 1.269 | Acc: 54.241% (4617/8512)\n",
      "Loss: 1.270 | Acc: 54.198% (4648/8576)\n",
      "Loss: 1.269 | Acc: 54.271% (4689/8640)\n",
      "Loss: 1.269 | Acc: 54.308% (4727/8704)\n",
      "Loss: 1.269 | Acc: 54.300% (4761/8768)\n",
      "Loss: 1.269 | Acc: 54.235% (4790/8832)\n",
      "Loss: 1.271 | Acc: 54.148% (4817/8896)\n",
      "Loss: 1.270 | Acc: 54.118% (4849/8960)\n",
      "Loss: 1.270 | Acc: 54.145% (4886/9024)\n",
      "Loss: 1.269 | Acc: 54.203% (4926/9088)\n",
      "Loss: 1.270 | Acc: 54.185% (4959/9152)\n",
      "Loss: 1.270 | Acc: 54.188% (4994/9216)\n",
      "Loss: 1.270 | Acc: 54.138% (5024/9280)\n",
      "Loss: 1.272 | Acc: 54.067% (5052/9344)\n",
      "Loss: 1.271 | Acc: 54.082% (5088/9408)\n",
      "Loss: 1.270 | Acc: 54.075% (5122/9472)\n",
      "Loss: 1.270 | Acc: 54.121% (5161/9536)\n",
      "Loss: 1.270 | Acc: 54.115% (5195/9600)\n",
      "Loss: 1.270 | Acc: 54.129% (5231/9664)\n",
      "Loss: 1.267 | Acc: 54.225% (5275/9728)\n",
      "Loss: 1.268 | Acc: 54.177% (5305/9792)\n",
      "Loss: 1.267 | Acc: 54.231% (5345/9856)\n",
      "Loss: 1.267 | Acc: 54.214% (5378/9920)\n",
      "Loss: 1.268 | Acc: 54.227% (5414/9984)\n",
      "Loss: 1.269 | Acc: 54.210% (5447/10048)\n",
      "Loss: 1.270 | Acc: 54.183% (5479/10112)\n",
      "Loss: 1.270 | Acc: 54.157% (5511/10176)\n",
      "Loss: 1.270 | Acc: 54.131% (5543/10240)\n",
      "Loss: 1.268 | Acc: 54.202% (5585/10304)\n",
      "Loss: 1.268 | Acc: 54.186% (5618/10368)\n",
      "Loss: 1.268 | Acc: 54.208% (5655/10432)\n",
      "Loss: 1.267 | Acc: 54.202% (5689/10496)\n",
      "Loss: 1.266 | Acc: 54.233% (5727/10560)\n",
      "Loss: 1.266 | Acc: 54.245% (5763/10624)\n",
      "Loss: 1.266 | Acc: 54.229% (5796/10688)\n",
      "Loss: 1.266 | Acc: 54.260% (5834/10752)\n",
      "Loss: 1.265 | Acc: 54.327% (5876/10816)\n",
      "Loss: 1.265 | Acc: 54.320% (5910/10880)\n",
      "Loss: 1.264 | Acc: 54.349% (5948/10944)\n",
      "Loss: 1.264 | Acc: 54.342% (5982/11008)\n",
      "Loss: 1.264 | Acc: 54.362% (6019/11072)\n",
      "Loss: 1.264 | Acc: 54.355% (6053/11136)\n",
      "Loss: 1.263 | Acc: 54.420% (6095/11200)\n",
      "Loss: 1.262 | Acc: 54.483% (6137/11264)\n",
      "Loss: 1.262 | Acc: 54.493% (6173/11328)\n",
      "Loss: 1.263 | Acc: 54.477% (6206/11392)\n",
      "Loss: 1.263 | Acc: 54.461% (6239/11456)\n",
      "Loss: 1.264 | Acc: 54.427% (6270/11520)\n",
      "Loss: 1.263 | Acc: 54.446% (6307/11584)\n",
      "Loss: 1.263 | Acc: 54.447% (6342/11648)\n",
      "Loss: 1.265 | Acc: 54.389% (6370/11712)\n",
      "Loss: 1.265 | Acc: 54.416% (6408/11776)\n",
      "Loss: 1.264 | Acc: 54.459% (6448/11840)\n",
      "Loss: 1.262 | Acc: 54.545% (6493/11904)\n",
      "Loss: 1.262 | Acc: 54.579% (6532/11968)\n",
      "Loss: 1.261 | Acc: 54.621% (6572/12032)\n",
      "Loss: 1.260 | Acc: 54.638% (6609/12096)\n",
      "Loss: 1.261 | Acc: 54.638% (6644/12160)\n",
      "Loss: 1.263 | Acc: 54.565% (6670/12224)\n",
      "Loss: 1.263 | Acc: 54.565% (6705/12288)\n",
      "Loss: 1.262 | Acc: 54.582% (6742/12352)\n",
      "Loss: 1.264 | Acc: 54.526% (6770/12416)\n",
      "Loss: 1.264 | Acc: 54.495% (6801/12480)\n",
      "Loss: 1.265 | Acc: 54.472% (6833/12544)\n",
      "Loss: 1.265 | Acc: 54.457% (6866/12608)\n",
      "Loss: 1.266 | Acc: 54.388% (6892/12672)\n",
      "Loss: 1.266 | Acc: 54.358% (6923/12736)\n",
      "Loss: 1.268 | Acc: 54.281% (6948/12800)\n",
      "Loss: 1.267 | Acc: 54.314% (6987/12864)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.267 | Acc: 54.308% (7021/12928)\n",
      "Loss: 1.266 | Acc: 54.326% (7058/12992)\n",
      "Loss: 1.266 | Acc: 54.335% (7094/13056)\n",
      "Loss: 1.266 | Acc: 54.345% (7130/13120)\n",
      "Loss: 1.266 | Acc: 54.339% (7164/13184)\n",
      "Loss: 1.266 | Acc: 54.333% (7198/13248)\n",
      "Loss: 1.266 | Acc: 54.312% (7230/13312)\n",
      "Loss: 1.268 | Acc: 54.246% (7256/13376)\n",
      "Loss: 1.267 | Acc: 54.219% (7287/13440)\n",
      "Loss: 1.267 | Acc: 54.221% (7322/13504)\n",
      "Loss: 1.267 | Acc: 54.201% (7354/13568)\n",
      "Loss: 1.266 | Acc: 54.225% (7392/13632)\n",
      "Loss: 1.266 | Acc: 54.228% (7427/13696)\n",
      "Loss: 1.266 | Acc: 54.237% (7463/13760)\n",
      "Loss: 1.265 | Acc: 54.196% (7492/13824)\n",
      "Loss: 1.266 | Acc: 54.169% (7523/13888)\n",
      "Loss: 1.265 | Acc: 54.193% (7561/13952)\n",
      "Loss: 1.264 | Acc: 54.195% (7596/14016)\n",
      "Loss: 1.264 | Acc: 54.190% (7630/14080)\n",
      "Loss: 1.263 | Acc: 54.242% (7672/14144)\n",
      "Loss: 1.263 | Acc: 54.265% (7710/14208)\n",
      "Loss: 1.264 | Acc: 54.253% (7743/14272)\n",
      "Loss: 1.264 | Acc: 54.262% (7779/14336)\n",
      "Loss: 1.265 | Acc: 54.243% (7811/14400)\n",
      "Loss: 1.265 | Acc: 54.204% (7840/14464)\n",
      "Loss: 1.264 | Acc: 54.226% (7878/14528)\n",
      "Loss: 1.264 | Acc: 54.235% (7914/14592)\n",
      "Loss: 1.265 | Acc: 54.183% (7941/14656)\n",
      "Loss: 1.266 | Acc: 54.171% (7974/14720)\n",
      "Loss: 1.266 | Acc: 54.133% (8003/14784)\n",
      "Loss: 1.267 | Acc: 54.095% (8032/14848)\n",
      "Loss: 1.268 | Acc: 54.071% (8063/14912)\n",
      "Loss: 1.269 | Acc: 54.000% (8087/14976)\n",
      "Loss: 1.268 | Acc: 54.003% (8122/15040)\n",
      "Loss: 1.269 | Acc: 53.972% (8152/15104)\n",
      "Loss: 1.269 | Acc: 53.989% (8189/15168)\n",
      "Loss: 1.269 | Acc: 53.985% (8223/15232)\n",
      "Loss: 1.269 | Acc: 53.981% (8257/15296)\n",
      "Loss: 1.269 | Acc: 53.991% (8293/15360)\n",
      "Loss: 1.269 | Acc: 53.981% (8326/15424)\n",
      "Loss: 1.269 | Acc: 54.022% (8367/15488)\n",
      "Loss: 1.269 | Acc: 53.980% (8395/15552)\n",
      "Loss: 1.269 | Acc: 53.983% (8430/15616)\n",
      "Loss: 1.269 | Acc: 53.948% (8459/15680)\n",
      "Loss: 1.269 | Acc: 53.957% (8495/15744)\n",
      "Loss: 1.269 | Acc: 53.935% (8526/15808)\n",
      "Loss: 1.270 | Acc: 53.919% (8558/15872)\n",
      "Loss: 1.270 | Acc: 53.909% (8591/15936)\n",
      "Loss: 1.270 | Acc: 53.950% (8632/16000)\n",
      "Loss: 1.270 | Acc: 53.928% (8663/16064)\n",
      "Loss: 1.269 | Acc: 53.943% (8700/16128)\n",
      "Loss: 1.269 | Acc: 53.983% (8741/16192)\n",
      "Loss: 1.269 | Acc: 53.986% (8776/16256)\n",
      "Loss: 1.268 | Acc: 54.013% (8815/16320)\n",
      "Loss: 1.268 | Acc: 54.004% (8848/16384)\n",
      "Loss: 1.267 | Acc: 54.000% (8882/16448)\n",
      "Loss: 1.268 | Acc: 53.985% (8914/16512)\n",
      "Loss: 1.268 | Acc: 53.994% (8950/16576)\n",
      "Loss: 1.267 | Acc: 54.014% (8988/16640)\n",
      "Loss: 1.268 | Acc: 53.993% (9019/16704)\n",
      "Loss: 1.268 | Acc: 54.014% (9057/16768)\n",
      "Loss: 1.268 | Acc: 54.016% (9092/16832)\n",
      "Loss: 1.268 | Acc: 54.031% (9129/16896)\n",
      "Loss: 1.267 | Acc: 54.057% (9168/16960)\n",
      "Loss: 1.267 | Acc: 54.082% (9207/17024)\n",
      "Loss: 1.267 | Acc: 54.085% (9242/17088)\n",
      "Loss: 1.268 | Acc: 54.040% (9269/17152)\n",
      "Loss: 1.267 | Acc: 54.049% (9305/17216)\n",
      "Loss: 1.267 | Acc: 54.028% (9336/17280)\n",
      "Loss: 1.267 | Acc: 54.036% (9372/17344)\n",
      "Loss: 1.267 | Acc: 54.015% (9403/17408)\n",
      "Loss: 1.266 | Acc: 54.052% (9444/17472)\n",
      "Loss: 1.267 | Acc: 54.020% (9473/17536)\n",
      "Loss: 1.266 | Acc: 54.051% (9513/17600)\n",
      "Loss: 1.266 | Acc: 54.053% (9548/17664)\n",
      "Loss: 1.266 | Acc: 54.039% (9580/17728)\n",
      "Loss: 1.266 | Acc: 54.036% (9614/17792)\n",
      "Loss: 1.267 | Acc: 54.010% (9644/17856)\n",
      "Loss: 1.267 | Acc: 54.023% (9681/17920)\n",
      "Loss: 1.266 | Acc: 54.031% (9717/17984)\n",
      "Loss: 1.267 | Acc: 53.995% (9745/18048)\n",
      "Loss: 1.267 | Acc: 54.019% (9784/18112)\n",
      "Loss: 1.267 | Acc: 53.994% (9814/18176)\n",
      "Loss: 1.266 | Acc: 53.997% (9849/18240)\n",
      "Loss: 1.267 | Acc: 54.021% (9888/18304)\n",
      "Loss: 1.267 | Acc: 54.029% (9924/18368)\n",
      "Loss: 1.266 | Acc: 54.053% (9963/18432)\n",
      "Loss: 1.266 | Acc: 54.077% (10002/18496)\n",
      "Loss: 1.266 | Acc: 54.062% (10034/18560)\n",
      "Loss: 1.266 | Acc: 54.059% (10068/18624)\n",
      "Loss: 1.265 | Acc: 54.088% (10108/18688)\n",
      "Loss: 1.265 | Acc: 54.101% (10145/18752)\n",
      "Loss: 1.265 | Acc: 54.108% (10181/18816)\n",
      "Loss: 1.265 | Acc: 54.100% (10214/18880)\n",
      "Loss: 1.265 | Acc: 54.107% (10250/18944)\n",
      "Loss: 1.265 | Acc: 54.119% (10287/19008)\n",
      "Loss: 1.265 | Acc: 54.121% (10322/19072)\n",
      "Loss: 1.264 | Acc: 54.139% (10360/19136)\n",
      "Loss: 1.264 | Acc: 54.146% (10396/19200)\n",
      "Loss: 1.264 | Acc: 54.153% (10432/19264)\n",
      "Loss: 1.265 | Acc: 54.129% (10462/19328)\n",
      "Loss: 1.265 | Acc: 54.115% (10494/19392)\n",
      "Loss: 1.265 | Acc: 54.117% (10529/19456)\n",
      "Loss: 1.265 | Acc: 54.114% (10563/19520)\n",
      "Loss: 1.265 | Acc: 54.075% (10590/19584)\n",
      "Loss: 1.265 | Acc: 54.082% (10626/19648)\n",
      "Loss: 1.265 | Acc: 54.084% (10661/19712)\n",
      "Loss: 1.265 | Acc: 54.106% (10700/19776)\n",
      "Loss: 1.265 | Acc: 54.068% (10727/19840)\n",
      "Loss: 1.266 | Acc: 54.044% (10757/19904)\n",
      "Loss: 1.265 | Acc: 54.036% (10790/19968)\n",
      "Loss: 1.266 | Acc: 54.009% (10819/20032)\n",
      "Loss: 1.266 | Acc: 54.016% (10855/20096)\n",
      "Loss: 1.265 | Acc: 54.038% (10894/20160)\n",
      "Loss: 1.266 | Acc: 54.005% (10922/20224)\n",
      "Loss: 1.267 | Acc: 54.012% (10958/20288)\n",
      "Loss: 1.267 | Acc: 54.000% (10990/20352)\n",
      "Loss: 1.267 | Acc: 53.997% (11024/20416)\n",
      "Loss: 1.268 | Acc: 54.004% (11060/20480)\n",
      "Loss: 1.268 | Acc: 54.011% (11096/20544)\n",
      "Loss: 1.267 | Acc: 54.023% (11133/20608)\n",
      "Loss: 1.268 | Acc: 54.044% (11172/20672)\n",
      "Loss: 1.267 | Acc: 54.065% (11211/20736)\n",
      "Loss: 1.268 | Acc: 54.062% (11245/20800)\n",
      "Loss: 1.268 | Acc: 54.064% (11280/20864)\n",
      "Loss: 1.268 | Acc: 54.023% (11306/20928)\n",
      "Loss: 1.268 | Acc: 54.025% (11341/20992)\n",
      "Loss: 1.268 | Acc: 54.023% (11375/21056)\n",
      "Loss: 1.268 | Acc: 54.029% (11411/21120)\n",
      "Loss: 1.268 | Acc: 54.022% (11444/21184)\n",
      "Loss: 1.268 | Acc: 53.996% (11473/21248)\n",
      "Loss: 1.269 | Acc: 53.998% (11508/21312)\n",
      "Loss: 1.268 | Acc: 54.028% (11549/21376)\n",
      "Loss: 1.268 | Acc: 54.058% (11590/21440)\n",
      "Loss: 1.268 | Acc: 54.060% (11625/21504)\n",
      "Loss: 1.268 | Acc: 54.071% (11662/21568)\n",
      "Loss: 1.268 | Acc: 54.026% (11687/21632)\n",
      "Loss: 1.269 | Acc: 54.019% (11720/21696)\n",
      "Loss: 1.269 | Acc: 54.026% (11756/21760)\n",
      "Loss: 1.269 | Acc: 54.028% (11791/21824)\n",
      "Loss: 1.269 | Acc: 54.043% (11829/21888)\n",
      "Loss: 1.269 | Acc: 54.041% (11863/21952)\n",
      "Loss: 1.269 | Acc: 54.043% (11898/22016)\n",
      "Loss: 1.268 | Acc: 54.044% (11933/22080)\n",
      "Loss: 1.268 | Acc: 54.042% (11967/22144)\n",
      "Loss: 1.267 | Acc: 54.035% (12000/22208)\n",
      "Loss: 1.267 | Acc: 54.050% (12038/22272)\n",
      "Loss: 1.267 | Acc: 54.047% (12072/22336)\n",
      "Loss: 1.267 | Acc: 54.062% (12110/22400)\n",
      "Loss: 1.267 | Acc: 54.082% (12149/22464)\n",
      "Loss: 1.266 | Acc: 54.097% (12187/22528)\n",
      "Loss: 1.266 | Acc: 54.108% (12224/22592)\n",
      "Loss: 1.266 | Acc: 54.114% (12260/22656)\n",
      "Loss: 1.266 | Acc: 54.120% (12296/22720)\n",
      "Loss: 1.265 | Acc: 54.139% (12335/22784)\n",
      "Loss: 1.265 | Acc: 54.145% (12371/22848)\n",
      "Loss: 1.265 | Acc: 54.133% (12403/22912)\n",
      "Loss: 1.266 | Acc: 54.113% (12433/22976)\n",
      "Loss: 1.266 | Acc: 54.093% (12463/23040)\n",
      "Loss: 1.266 | Acc: 54.086% (12496/23104)\n",
      "Loss: 1.266 | Acc: 54.066% (12526/23168)\n",
      "Loss: 1.266 | Acc: 54.076% (12563/23232)\n",
      "Loss: 1.266 | Acc: 54.065% (12595/23296)\n",
      "Loss: 1.267 | Acc: 54.058% (12628/23360)\n",
      "Loss: 1.267 | Acc: 54.056% (12662/23424)\n",
      "Loss: 1.266 | Acc: 54.100% (12707/23488)\n",
      "Loss: 1.266 | Acc: 54.093% (12740/23552)\n",
      "Loss: 1.266 | Acc: 54.090% (12774/23616)\n",
      "Loss: 1.266 | Acc: 54.113% (12814/23680)\n",
      "Loss: 1.266 | Acc: 54.123% (12851/23744)\n",
      "Loss: 1.266 | Acc: 54.137% (12889/23808)\n",
      "Loss: 1.265 | Acc: 54.139% (12924/23872)\n",
      "Loss: 1.265 | Acc: 54.140% (12959/23936)\n",
      "Loss: 1.265 | Acc: 54.142% (12994/24000)\n",
      "Loss: 1.266 | Acc: 54.126% (13025/24064)\n",
      "Loss: 1.266 | Acc: 54.107% (13055/24128)\n",
      "Loss: 1.266 | Acc: 54.096% (13087/24192)\n",
      "Loss: 1.266 | Acc: 54.094% (13121/24256)\n",
      "Loss: 1.266 | Acc: 54.124% (13163/24320)\n",
      "Loss: 1.266 | Acc: 54.093% (13190/24384)\n",
      "Loss: 1.267 | Acc: 54.094% (13225/24448)\n",
      "Loss: 1.266 | Acc: 54.116% (13265/24512)\n",
      "Loss: 1.266 | Acc: 54.138% (13305/24576)\n",
      "Loss: 1.266 | Acc: 54.131% (13338/24640)\n",
      "Loss: 1.267 | Acc: 54.105% (13366/24704)\n",
      "Loss: 1.267 | Acc: 54.114% (13403/24768)\n",
      "Loss: 1.267 | Acc: 54.112% (13437/24832)\n",
      "Loss: 1.267 | Acc: 54.121% (13474/24896)\n",
      "Loss: 1.267 | Acc: 54.123% (13509/24960)\n",
      "Loss: 1.267 | Acc: 54.124% (13544/25024)\n",
      "Loss: 1.267 | Acc: 54.121% (13578/25088)\n",
      "Loss: 1.267 | Acc: 54.127% (13614/25152)\n",
      "Loss: 1.267 | Acc: 54.128% (13649/25216)\n",
      "Loss: 1.268 | Acc: 54.098% (13676/25280)\n",
      "Loss: 1.268 | Acc: 54.107% (13713/25344)\n",
      "Loss: 1.268 | Acc: 54.109% (13748/25408)\n",
      "Loss: 1.268 | Acc: 54.138% (13790/25472)\n",
      "Loss: 1.268 | Acc: 54.124% (13821/25536)\n",
      "Loss: 1.267 | Acc: 54.121% (13855/25600)\n",
      "Loss: 1.268 | Acc: 54.115% (13888/25664)\n",
      "Loss: 1.268 | Acc: 54.104% (13920/25728)\n",
      "Loss: 1.268 | Acc: 54.106% (13955/25792)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.268 | Acc: 54.107% (13990/25856)\n",
      "Loss: 1.268 | Acc: 54.078% (14017/25920)\n",
      "Loss: 1.268 | Acc: 54.083% (14053/25984)\n",
      "Loss: 1.269 | Acc: 54.077% (14086/26048)\n",
      "Loss: 1.269 | Acc: 54.094% (14125/26112)\n",
      "Loss: 1.269 | Acc: 54.107% (14163/26176)\n",
      "Loss: 1.269 | Acc: 54.108% (14198/26240)\n",
      "Loss: 1.269 | Acc: 54.094% (14229/26304)\n",
      "Loss: 1.269 | Acc: 54.092% (14263/26368)\n",
      "Loss: 1.269 | Acc: 54.086% (14296/26432)\n",
      "Loss: 1.270 | Acc: 54.069% (14326/26496)\n",
      "Loss: 1.269 | Acc: 54.085% (14365/26560)\n",
      "Loss: 1.269 | Acc: 54.098% (14403/26624)\n",
      "Loss: 1.269 | Acc: 54.107% (14440/26688)\n",
      "Loss: 1.268 | Acc: 54.119% (14478/26752)\n",
      "Loss: 1.269 | Acc: 54.113% (14511/26816)\n",
      "Loss: 1.269 | Acc: 54.115% (14546/26880)\n",
      "Loss: 1.269 | Acc: 54.120% (14582/26944)\n",
      "Loss: 1.269 | Acc: 54.102% (14612/27008)\n",
      "Loss: 1.268 | Acc: 54.108% (14648/27072)\n",
      "Loss: 1.268 | Acc: 54.113% (14684/27136)\n",
      "Loss: 1.269 | Acc: 54.099% (14715/27200)\n",
      "Loss: 1.269 | Acc: 54.093% (14748/27264)\n",
      "Loss: 1.269 | Acc: 54.080% (14779/27328)\n",
      "Loss: 1.270 | Acc: 54.085% (14815/27392)\n",
      "Loss: 1.269 | Acc: 54.094% (14852/27456)\n",
      "Loss: 1.270 | Acc: 54.077% (14882/27520)\n",
      "Loss: 1.270 | Acc: 54.060% (14912/27584)\n",
      "Loss: 1.270 | Acc: 54.065% (14948/27648)\n",
      "Loss: 1.270 | Acc: 54.078% (14986/27712)\n",
      "Loss: 1.270 | Acc: 54.068% (15018/27776)\n",
      "Loss: 1.270 | Acc: 54.070% (15053/27840)\n",
      "Loss: 1.270 | Acc: 54.082% (15091/27904)\n",
      "Loss: 1.270 | Acc: 54.065% (15121/27968)\n",
      "Loss: 1.271 | Acc: 54.053% (15152/28032)\n",
      "Loss: 1.272 | Acc: 54.008% (15174/28096)\n",
      "Loss: 1.271 | Acc: 54.020% (15212/28160)\n",
      "Loss: 1.271 | Acc: 54.032% (15250/28224)\n",
      "Loss: 1.272 | Acc: 54.005% (15277/28288)\n",
      "Loss: 1.272 | Acc: 53.972% (15302/28352)\n",
      "Loss: 1.273 | Acc: 53.956% (15332/28416)\n",
      "Loss: 1.273 | Acc: 53.957% (15367/28480)\n",
      "Loss: 1.273 | Acc: 53.948% (15399/28544)\n",
      "Loss: 1.273 | Acc: 53.950% (15434/28608)\n",
      "Loss: 1.273 | Acc: 53.938% (15465/28672)\n",
      "Loss: 1.273 | Acc: 53.953% (15504/28736)\n",
      "Loss: 1.273 | Acc: 53.951% (15538/28800)\n",
      "Loss: 1.273 | Acc: 53.932% (15567/28864)\n",
      "Loss: 1.273 | Acc: 53.944% (15605/28928)\n",
      "Loss: 1.273 | Acc: 53.970% (15647/28992)\n",
      "Loss: 1.273 | Acc: 53.972% (15682/29056)\n",
      "Loss: 1.273 | Acc: 53.970% (15716/29120)\n",
      "Loss: 1.273 | Acc: 53.954% (15746/29184)\n",
      "Loss: 1.273 | Acc: 53.949% (15779/29248)\n",
      "Loss: 1.273 | Acc: 53.951% (15814/29312)\n",
      "Loss: 1.273 | Acc: 53.952% (15849/29376)\n",
      "Loss: 1.273 | Acc: 53.967% (15888/29440)\n",
      "Loss: 1.274 | Acc: 53.962% (15921/29504)\n",
      "Loss: 1.274 | Acc: 53.954% (15953/29568)\n",
      "Loss: 1.274 | Acc: 53.952% (15987/29632)\n",
      "Loss: 1.274 | Acc: 53.940% (16018/29696)\n",
      "Loss: 1.275 | Acc: 53.948% (16055/29760)\n",
      "Loss: 1.274 | Acc: 53.957% (16092/29824)\n",
      "Loss: 1.274 | Acc: 53.941% (16122/29888)\n",
      "Loss: 1.274 | Acc: 53.953% (16160/29952)\n",
      "Loss: 1.274 | Acc: 53.958% (16196/30016)\n",
      "Loss: 1.273 | Acc: 53.966% (16233/30080)\n",
      "Loss: 1.274 | Acc: 53.964% (16267/30144)\n",
      "Loss: 1.275 | Acc: 53.933% (16292/30208)\n",
      "Loss: 1.274 | Acc: 53.948% (16331/30272)\n",
      "Loss: 1.274 | Acc: 53.959% (16369/30336)\n",
      "Loss: 1.275 | Acc: 53.938% (16397/30400)\n",
      "Loss: 1.275 | Acc: 53.946% (16434/30464)\n",
      "Loss: 1.274 | Acc: 53.950% (16470/30528)\n",
      "Loss: 1.274 | Acc: 53.955% (16506/30592)\n",
      "Loss: 1.274 | Acc: 53.950% (16539/30656)\n",
      "Loss: 1.274 | Acc: 53.958% (16576/30720)\n",
      "Loss: 1.274 | Acc: 53.953% (16609/30784)\n",
      "Loss: 1.274 | Acc: 53.948% (16642/30848)\n",
      "Loss: 1.275 | Acc: 53.937% (16673/30912)\n",
      "Loss: 1.275 | Acc: 53.922% (16703/30976)\n",
      "Loss: 1.275 | Acc: 53.930% (16740/31040)\n",
      "Loss: 1.275 | Acc: 53.906% (16767/31104)\n",
      "Loss: 1.275 | Acc: 53.917% (16805/31168)\n",
      "Loss: 1.275 | Acc: 53.916% (16839/31232)\n",
      "Loss: 1.275 | Acc: 53.917% (16874/31296)\n",
      "Loss: 1.275 | Acc: 53.906% (16905/31360)\n",
      "Loss: 1.276 | Acc: 53.914% (16942/31424)\n",
      "Loss: 1.275 | Acc: 53.919% (16978/31488)\n",
      "Loss: 1.275 | Acc: 53.905% (17008/31552)\n",
      "Loss: 1.275 | Acc: 53.922% (17048/31616)\n",
      "Loss: 1.275 | Acc: 53.917% (17081/31680)\n",
      "Loss: 1.275 | Acc: 53.928% (17119/31744)\n",
      "Loss: 1.275 | Acc: 53.936% (17156/31808)\n",
      "Loss: 1.275 | Acc: 53.950% (17195/31872)\n",
      "Loss: 1.274 | Acc: 53.964% (17234/31936)\n",
      "Loss: 1.275 | Acc: 53.938% (17260/32000)\n",
      "Loss: 1.275 | Acc: 53.936% (17294/32064)\n",
      "Loss: 1.275 | Acc: 53.919% (17323/32128)\n",
      "Loss: 1.274 | Acc: 53.948% (17367/32192)\n",
      "Loss: 1.275 | Acc: 53.937% (17398/32256)\n",
      "Loss: 1.275 | Acc: 53.923% (17428/32320)\n",
      "Loss: 1.274 | Acc: 53.928% (17464/32384)\n",
      "Loss: 1.274 | Acc: 53.942% (17503/32448)\n",
      "Loss: 1.274 | Acc: 53.931% (17534/32512)\n",
      "Loss: 1.274 | Acc: 53.935% (17570/32576)\n",
      "Loss: 1.274 | Acc: 53.922% (17600/32640)\n",
      "Loss: 1.275 | Acc: 53.905% (17629/32704)\n",
      "Loss: 1.275 | Acc: 53.897% (17661/32768)\n",
      "Loss: 1.275 | Acc: 53.889% (17693/32832)\n",
      "Loss: 1.275 | Acc: 53.891% (17728/32896)\n",
      "Loss: 1.275 | Acc: 53.883% (17760/32960)\n",
      "Loss: 1.275 | Acc: 53.888% (17796/33024)\n",
      "Loss: 1.275 | Acc: 53.896% (17833/33088)\n",
      "Loss: 1.275 | Acc: 53.897% (17868/33152)\n",
      "Loss: 1.275 | Acc: 53.896% (17902/33216)\n",
      "Loss: 1.276 | Acc: 53.879% (17931/33280)\n",
      "Loss: 1.275 | Acc: 53.881% (17966/33344)\n",
      "Loss: 1.275 | Acc: 53.900% (18007/33408)\n",
      "Loss: 1.275 | Acc: 53.902% (18042/33472)\n",
      "Loss: 1.275 | Acc: 53.900% (18076/33536)\n",
      "Loss: 1.275 | Acc: 53.911% (18114/33600)\n",
      "Loss: 1.275 | Acc: 53.915% (18150/33664)\n",
      "Loss: 1.275 | Acc: 53.902% (18180/33728)\n",
      "Loss: 1.275 | Acc: 53.897% (18213/33792)\n",
      "Loss: 1.275 | Acc: 53.899% (18248/33856)\n",
      "Loss: 1.275 | Acc: 53.894% (18281/33920)\n",
      "Loss: 1.275 | Acc: 53.881% (18311/33984)\n",
      "Loss: 1.276 | Acc: 53.865% (18340/34048)\n",
      "Loss: 1.275 | Acc: 53.873% (18377/34112)\n",
      "Loss: 1.275 | Acc: 53.871% (18411/34176)\n",
      "Loss: 1.274 | Acc: 53.893% (18453/34240)\n",
      "Loss: 1.274 | Acc: 53.895% (18488/34304)\n",
      "Loss: 1.274 | Acc: 53.911% (18528/34368)\n",
      "Loss: 1.274 | Acc: 53.912% (18563/34432)\n",
      "Loss: 1.274 | Acc: 53.908% (18596/34496)\n",
      "Loss: 1.274 | Acc: 53.921% (18635/34560)\n",
      "Loss: 1.274 | Acc: 53.916% (18668/34624)\n",
      "Loss: 1.274 | Acc: 53.932% (18708/34688)\n",
      "Loss: 1.274 | Acc: 53.939% (18745/34752)\n",
      "Loss: 1.274 | Acc: 53.952% (18784/34816)\n",
      "Loss: 1.274 | Acc: 53.939% (18814/34880)\n",
      "Loss: 1.274 | Acc: 53.932% (18846/34944)\n",
      "Loss: 1.274 | Acc: 53.919% (18876/35008)\n",
      "Loss: 1.274 | Acc: 53.929% (18914/35072)\n",
      "Loss: 1.274 | Acc: 53.922% (18946/35136)\n",
      "Loss: 1.274 | Acc: 53.932% (18984/35200)\n",
      "Loss: 1.274 | Acc: 53.925% (19016/35264)\n",
      "Loss: 1.274 | Acc: 53.929% (19052/35328)\n",
      "Loss: 1.274 | Acc: 53.927% (19086/35392)\n",
      "Loss: 1.274 | Acc: 53.926% (19120/35456)\n",
      "Loss: 1.274 | Acc: 53.910% (19149/35520)\n",
      "Loss: 1.274 | Acc: 53.903% (19181/35584)\n",
      "Loss: 1.274 | Acc: 53.899% (19214/35648)\n",
      "Loss: 1.274 | Acc: 53.909% (19252/35712)\n",
      "Loss: 1.274 | Acc: 53.910% (19287/35776)\n",
      "Loss: 1.275 | Acc: 53.895% (19316/35840)\n",
      "Loss: 1.275 | Acc: 53.913% (19357/35904)\n",
      "Loss: 1.275 | Acc: 53.917% (19393/35968)\n",
      "Loss: 1.275 | Acc: 53.933% (19433/36032)\n",
      "Loss: 1.274 | Acc: 53.959% (19477/36096)\n",
      "Loss: 1.274 | Acc: 53.957% (19511/36160)\n",
      "Loss: 1.274 | Acc: 53.973% (19551/36224)\n",
      "Loss: 1.274 | Acc: 53.965% (19583/36288)\n",
      "Loss: 1.274 | Acc: 53.959% (19615/36352)\n",
      "Loss: 1.274 | Acc: 53.960% (19650/36416)\n",
      "Loss: 1.274 | Acc: 53.958% (19684/36480)\n",
      "Loss: 1.274 | Acc: 53.968% (19722/36544)\n",
      "Loss: 1.274 | Acc: 53.966% (19756/36608)\n",
      "Loss: 1.274 | Acc: 53.954% (19786/36672)\n",
      "Loss: 1.274 | Acc: 53.947% (19818/36736)\n",
      "Loss: 1.274 | Acc: 53.959% (19857/36800)\n",
      "Loss: 1.274 | Acc: 53.971% (19896/36864)\n",
      "Loss: 1.274 | Acc: 53.973% (19931/36928)\n",
      "Loss: 1.274 | Acc: 53.977% (19967/36992)\n",
      "Loss: 1.274 | Acc: 53.975% (20001/37056)\n",
      "Loss: 1.274 | Acc: 53.968% (20033/37120)\n",
      "Loss: 1.274 | Acc: 53.967% (20067/37184)\n",
      "Loss: 1.274 | Acc: 53.973% (20104/37248)\n",
      "Loss: 1.274 | Acc: 53.975% (20139/37312)\n",
      "Loss: 1.274 | Acc: 53.970% (20172/37376)\n",
      "Loss: 1.274 | Acc: 53.977% (20209/37440)\n",
      "Loss: 1.274 | Acc: 53.978% (20244/37504)\n",
      "Loss: 1.273 | Acc: 53.998% (20286/37568)\n",
      "Loss: 1.273 | Acc: 54.010% (20325/37632)\n",
      "Loss: 1.274 | Acc: 53.998% (20355/37696)\n",
      "Loss: 1.273 | Acc: 54.010% (20394/37760)\n",
      "Loss: 1.274 | Acc: 53.984% (20419/37824)\n",
      "Loss: 1.275 | Acc: 53.959% (20444/37888)\n",
      "Loss: 1.274 | Acc: 53.966% (20481/37952)\n",
      "Loss: 1.275 | Acc: 53.946% (20508/38016)\n",
      "Loss: 1.275 | Acc: 53.936% (20539/38080)\n",
      "Loss: 1.274 | Acc: 53.967% (20585/38144)\n",
      "Loss: 1.274 | Acc: 53.978% (20624/38208)\n",
      "Loss: 1.275 | Acc: 53.982% (20660/38272)\n",
      "Loss: 1.275 | Acc: 53.988% (20697/38336)\n",
      "Loss: 1.275 | Acc: 53.987% (20731/38400)\n",
      "Loss: 1.274 | Acc: 53.999% (20770/38464)\n",
      "Loss: 1.274 | Acc: 53.994% (20803/38528)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.274 | Acc: 53.998% (20839/38592)\n",
      "Loss: 1.274 | Acc: 54.002% (20875/38656)\n",
      "Loss: 1.274 | Acc: 53.998% (20908/38720)\n",
      "Loss: 1.274 | Acc: 54.007% (20946/38784)\n",
      "Loss: 1.274 | Acc: 54.005% (20980/38848)\n",
      "Loss: 1.274 | Acc: 53.994% (21010/38912)\n",
      "Loss: 1.274 | Acc: 54.000% (21047/38976)\n",
      "Loss: 1.274 | Acc: 53.993% (21079/39040)\n",
      "Loss: 1.274 | Acc: 54.002% (21117/39104)\n",
      "Loss: 1.274 | Acc: 54.011% (21155/39168)\n",
      "Loss: 1.274 | Acc: 54.009% (21189/39232)\n",
      "Loss: 1.274 | Acc: 54.003% (21221/39296)\n",
      "Loss: 1.274 | Acc: 54.014% (21260/39360)\n",
      "Loss: 1.274 | Acc: 54.008% (21292/39424)\n",
      "Loss: 1.274 | Acc: 54.009% (21327/39488)\n",
      "Loss: 1.274 | Acc: 54.020% (21366/39552)\n",
      "Loss: 1.274 | Acc: 54.016% (21399/39616)\n",
      "Loss: 1.274 | Acc: 54.015% (21433/39680)\n",
      "Loss: 1.274 | Acc: 54.031% (21474/39744)\n",
      "Loss: 1.274 | Acc: 54.029% (21508/39808)\n",
      "Loss: 1.275 | Acc: 54.028% (21542/39872)\n",
      "Loss: 1.274 | Acc: 54.046% (21584/39936)\n",
      "Loss: 1.275 | Acc: 54.040% (21616/40000)\n",
      "Loss: 1.275 | Acc: 54.049% (21654/40064)\n",
      "Loss: 1.274 | Acc: 54.050% (21689/40128)\n",
      "Loss: 1.275 | Acc: 54.026% (21714/40192)\n",
      "Loss: 1.275 | Acc: 54.032% (21751/40256)\n",
      "Loss: 1.275 | Acc: 54.030% (21785/40320)\n",
      "Loss: 1.275 | Acc: 54.036% (21822/40384)\n",
      "Loss: 1.275 | Acc: 54.027% (21853/40448)\n",
      "Loss: 1.275 | Acc: 54.023% (21886/40512)\n",
      "Loss: 1.275 | Acc: 54.020% (21919/40576)\n",
      "Loss: 1.275 | Acc: 54.018% (21953/40640)\n",
      "Loss: 1.275 | Acc: 54.027% (21991/40704)\n",
      "Loss: 1.275 | Acc: 54.023% (22024/40768)\n",
      "Loss: 1.275 | Acc: 54.014% (22055/40832)\n",
      "Loss: 1.275 | Acc: 54.018% (22091/40896)\n",
      "Loss: 1.274 | Acc: 54.033% (22132/40960)\n",
      "Loss: 1.274 | Acc: 54.044% (22171/41024)\n",
      "Loss: 1.274 | Acc: 54.040% (22204/41088)\n",
      "Loss: 1.274 | Acc: 54.044% (22240/41152)\n",
      "Loss: 1.275 | Acc: 54.018% (22264/41216)\n",
      "Loss: 1.275 | Acc: 54.016% (22298/41280)\n",
      "Loss: 1.275 | Acc: 54.013% (22331/41344)\n",
      "Loss: 1.275 | Acc: 54.004% (22362/41408)\n",
      "Loss: 1.275 | Acc: 54.029% (22407/41472)\n",
      "Loss: 1.275 | Acc: 54.035% (22444/41536)\n",
      "Loss: 1.274 | Acc: 54.048% (22484/41600)\n",
      "Loss: 1.274 | Acc: 54.049% (22519/41664)\n",
      "Loss: 1.274 | Acc: 54.038% (22549/41728)\n",
      "Loss: 1.275 | Acc: 54.029% (22580/41792)\n",
      "Loss: 1.275 | Acc: 54.026% (22613/41856)\n",
      "Loss: 1.275 | Acc: 54.022% (22646/41920)\n",
      "Loss: 1.275 | Acc: 54.040% (22688/41984)\n",
      "Loss: 1.275 | Acc: 54.050% (22727/42048)\n",
      "Loss: 1.274 | Acc: 54.068% (22769/42112)\n",
      "Loss: 1.274 | Acc: 54.085% (22811/42176)\n",
      "Loss: 1.274 | Acc: 54.077% (22842/42240)\n",
      "Loss: 1.274 | Acc: 54.089% (22882/42304)\n",
      "Loss: 1.274 | Acc: 54.071% (22909/42368)\n",
      "Loss: 1.274 | Acc: 54.087% (22950/42432)\n",
      "Loss: 1.274 | Acc: 54.090% (22986/42496)\n",
      "Loss: 1.274 | Acc: 54.095% (23023/42560)\n",
      "Loss: 1.274 | Acc: 54.099% (23059/42624)\n",
      "Loss: 1.273 | Acc: 54.109% (23098/42688)\n",
      "Loss: 1.273 | Acc: 54.119% (23137/42752)\n",
      "Loss: 1.272 | Acc: 54.136% (23179/42816)\n",
      "Loss: 1.272 | Acc: 54.139% (23215/42880)\n",
      "Loss: 1.272 | Acc: 54.143% (23251/42944)\n",
      "Loss: 1.272 | Acc: 54.160% (23293/43008)\n",
      "Loss: 1.272 | Acc: 54.151% (23324/43072)\n",
      "Loss: 1.272 | Acc: 54.154% (23360/43136)\n",
      "Loss: 1.272 | Acc: 54.162% (23398/43200)\n",
      "Loss: 1.272 | Acc: 54.167% (23435/43264)\n",
      "Loss: 1.272 | Acc: 54.164% (23468/43328)\n",
      "Loss: 1.272 | Acc: 54.164% (23503/43392)\n",
      "Loss: 1.271 | Acc: 54.167% (23539/43456)\n",
      "Loss: 1.271 | Acc: 54.159% (23570/43520)\n",
      "Loss: 1.271 | Acc: 54.157% (23604/43584)\n",
      "Loss: 1.271 | Acc: 54.145% (23633/43648)\n",
      "Loss: 1.271 | Acc: 54.150% (23670/43712)\n",
      "Loss: 1.271 | Acc: 54.153% (23706/43776)\n",
      "Loss: 1.271 | Acc: 54.163% (23745/43840)\n",
      "Loss: 1.271 | Acc: 54.150% (23774/43904)\n",
      "Loss: 1.271 | Acc: 54.153% (23810/43968)\n",
      "Loss: 1.271 | Acc: 54.172% (23853/44032)\n",
      "Loss: 1.271 | Acc: 54.170% (23887/44096)\n",
      "Loss: 1.271 | Acc: 54.180% (23926/44160)\n",
      "Loss: 1.271 | Acc: 54.154% (23949/44224)\n",
      "Loss: 1.271 | Acc: 54.150% (23982/44288)\n",
      "Loss: 1.271 | Acc: 54.151% (24017/44352)\n",
      "Loss: 1.271 | Acc: 54.143% (24048/44416)\n",
      "Loss: 1.272 | Acc: 54.143% (24083/44480)\n",
      "Loss: 1.272 | Acc: 54.142% (24117/44544)\n",
      "Loss: 1.272 | Acc: 54.138% (24150/44608)\n",
      "Loss: 1.272 | Acc: 54.141% (24186/44672)\n",
      "Loss: 1.272 | Acc: 54.151% (24225/44736)\n",
      "Loss: 1.271 | Acc: 54.158% (24263/44800)\n",
      "Loss: 1.272 | Acc: 54.150% (24294/44864)\n",
      "Loss: 1.272 | Acc: 54.158% (24332/44928)\n",
      "Loss: 1.271 | Acc: 54.167% (24371/44992)\n",
      "Loss: 1.272 | Acc: 54.161% (24403/45056)\n",
      "Loss: 1.271 | Acc: 54.169% (24441/45120)\n",
      "Loss: 1.271 | Acc: 54.172% (24477/45184)\n",
      "Loss: 1.271 | Acc: 54.170% (24511/45248)\n",
      "Loss: 1.271 | Acc: 54.175% (24548/45312)\n",
      "Loss: 1.271 | Acc: 54.185% (24587/45376)\n",
      "Loss: 1.271 | Acc: 54.175% (24617/45440)\n",
      "Loss: 1.271 | Acc: 54.182% (24655/45504)\n",
      "Loss: 1.271 | Acc: 54.185% (24691/45568)\n",
      "Loss: 1.271 | Acc: 54.183% (24725/45632)\n",
      "Loss: 1.271 | Acc: 54.173% (24755/45696)\n",
      "Loss: 1.271 | Acc: 54.181% (24793/45760)\n",
      "Loss: 1.271 | Acc: 54.183% (24829/45824)\n",
      "Loss: 1.270 | Acc: 54.197% (24870/45888)\n",
      "Loss: 1.270 | Acc: 54.187% (24900/45952)\n",
      "Loss: 1.270 | Acc: 54.190% (24936/46016)\n",
      "Loss: 1.270 | Acc: 54.188% (24970/46080)\n",
      "Loss: 1.270 | Acc: 54.196% (25008/46144)\n",
      "Loss: 1.270 | Acc: 54.196% (25043/46208)\n",
      "Loss: 1.270 | Acc: 54.197% (25078/46272)\n",
      "Loss: 1.270 | Acc: 54.202% (25115/46336)\n",
      "Loss: 1.270 | Acc: 54.209% (25153/46400)\n",
      "Loss: 1.270 | Acc: 54.208% (25187/46464)\n",
      "Loss: 1.270 | Acc: 54.206% (25221/46528)\n",
      "Loss: 1.270 | Acc: 54.205% (25255/46592)\n",
      "Loss: 1.271 | Acc: 54.207% (25291/46656)\n",
      "Loss: 1.271 | Acc: 54.208% (25326/46720)\n",
      "Loss: 1.270 | Acc: 54.209% (25361/46784)\n",
      "Loss: 1.270 | Acc: 54.216% (25399/46848)\n",
      "Loss: 1.270 | Acc: 54.229% (25440/46912)\n",
      "Loss: 1.270 | Acc: 54.232% (25476/46976)\n",
      "Loss: 1.269 | Acc: 54.241% (25515/47040)\n",
      "Loss: 1.269 | Acc: 54.263% (25560/47104)\n",
      "Loss: 1.269 | Acc: 54.263% (25595/47168)\n",
      "Loss: 1.269 | Acc: 54.279% (25637/47232)\n",
      "Loss: 1.269 | Acc: 54.282% (25673/47296)\n",
      "Loss: 1.269 | Acc: 54.265% (25700/47360)\n",
      "Loss: 1.269 | Acc: 54.259% (25732/47424)\n",
      "Loss: 1.269 | Acc: 54.254% (25764/47488)\n",
      "Loss: 1.269 | Acc: 54.246% (25795/47552)\n",
      "Loss: 1.269 | Acc: 54.240% (25827/47616)\n",
      "Loss: 1.269 | Acc: 54.232% (25858/47680)\n",
      "Loss: 1.270 | Acc: 54.233% (25893/47744)\n",
      "Loss: 1.269 | Acc: 54.236% (25929/47808)\n",
      "Loss: 1.269 | Acc: 54.253% (25972/47872)\n",
      "Loss: 1.269 | Acc: 54.245% (26003/47936)\n",
      "Loss: 1.269 | Acc: 54.252% (26041/48000)\n",
      "Loss: 1.269 | Acc: 54.263% (26081/48064)\n",
      "Loss: 1.269 | Acc: 54.264% (26116/48128)\n",
      "Loss: 1.269 | Acc: 54.260% (26149/48192)\n",
      "Loss: 1.269 | Acc: 54.250% (26179/48256)\n",
      "Loss: 1.269 | Acc: 54.253% (26215/48320)\n",
      "Loss: 1.269 | Acc: 54.243% (26245/48384)\n",
      "Loss: 1.269 | Acc: 54.256% (26286/48448)\n",
      "Loss: 1.269 | Acc: 54.255% (26320/48512)\n",
      "Loss: 1.269 | Acc: 54.257% (26356/48576)\n",
      "Loss: 1.269 | Acc: 54.243% (26384/48640)\n",
      "Loss: 1.269 | Acc: 54.242% (26418/48704)\n",
      "Loss: 1.269 | Acc: 54.240% (26452/48768)\n",
      "Loss: 1.269 | Acc: 54.251% (26492/48832)\n",
      "Loss: 1.269 | Acc: 54.240% (26521/48896)\n",
      "Loss: 1.269 | Acc: 54.250% (26561/48960)\n",
      "Loss: 1.268 | Acc: 54.269% (26592/49000)\n",
      "Epoch 2 of training is completed, Training accuracy for this epoch is 54.26938775510204\n",
      "\n",
      "---- Evaluation in process ----\n",
      "Loss: 1.131 | Acc: 59.375% (38/64)\n",
      "Loss: 1.129 | Acc: 62.500% (80/128)\n",
      "Loss: 1.235 | Acc: 58.854% (113/192)\n",
      "Loss: 1.298 | Acc: 56.250% (144/256)\n",
      "Loss: 1.277 | Acc: 56.250% (180/320)\n",
      "Loss: 1.289 | Acc: 55.469% (213/384)\n",
      "Loss: 1.303 | Acc: 56.027% (251/448)\n",
      "Loss: 1.298 | Acc: 55.859% (286/512)\n",
      "Loss: 1.284 | Acc: 56.076% (323/576)\n",
      "Loss: 1.260 | Acc: 56.719% (363/640)\n",
      "Loss: 1.277 | Acc: 56.534% (398/704)\n",
      "Loss: 1.275 | Acc: 55.990% (430/768)\n",
      "Loss: 1.273 | Acc: 56.370% (469/832)\n",
      "Loss: 1.268 | Acc: 56.585% (507/896)\n",
      "Loss: 1.263 | Acc: 56.771% (545/960)\n",
      "Loss: 1.254 | Acc: 57.031% (584/1024)\n",
      "Loss: 1.260 | Acc: 57.261% (623/1088)\n",
      "Loss: 1.259 | Acc: 56.858% (655/1152)\n",
      "Loss: 1.254 | Acc: 56.990% (693/1216)\n",
      "Loss: 1.262 | Acc: 57.031% (730/1280)\n",
      "Loss: 1.260 | Acc: 56.548% (760/1344)\n",
      "Loss: 1.255 | Acc: 56.676% (798/1408)\n",
      "Loss: 1.248 | Acc: 56.522% (832/1472)\n",
      "Loss: 1.252 | Acc: 56.120% (862/1536)\n",
      "Loss: 1.252 | Acc: 56.250% (900/1600)\n",
      "Loss: 1.254 | Acc: 56.250% (936/1664)\n",
      "Loss: 1.254 | Acc: 56.308% (973/1728)\n",
      "Loss: 1.257 | Acc: 55.971% (1003/1792)\n",
      "Loss: 1.256 | Acc: 55.819% (1036/1856)\n",
      "Loss: 1.258 | Acc: 55.938% (1074/1920)\n",
      "Loss: 1.261 | Acc: 55.847% (1108/1984)\n",
      "Loss: 1.261 | Acc: 55.664% (1140/2048)\n",
      "Loss: 1.253 | Acc: 55.919% (1181/2112)\n",
      "Loss: 1.255 | Acc: 55.653% (1211/2176)\n",
      "Loss: 1.252 | Acc: 55.670% (1247/2240)\n",
      "Loss: 1.250 | Acc: 55.816% (1286/2304)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.254 | Acc: 55.659% (1318/2368)\n",
      "Loss: 1.252 | Acc: 55.798% (1357/2432)\n",
      "Loss: 1.248 | Acc: 56.050% (1399/2496)\n",
      "Loss: 1.259 | Acc: 55.742% (1427/2560)\n",
      "Loss: 1.260 | Acc: 55.640% (1460/2624)\n",
      "Loss: 1.262 | Acc: 55.543% (1493/2688)\n",
      "Loss: 1.259 | Acc: 55.596% (1530/2752)\n",
      "Loss: 1.256 | Acc: 55.717% (1569/2816)\n",
      "Loss: 1.254 | Acc: 55.694% (1604/2880)\n",
      "Loss: 1.253 | Acc: 55.774% (1642/2944)\n",
      "Loss: 1.251 | Acc: 55.818% (1679/3008)\n",
      "Loss: 1.251 | Acc: 55.729% (1712/3072)\n",
      "Loss: 1.252 | Acc: 55.676% (1746/3136)\n",
      "Loss: 1.252 | Acc: 55.594% (1779/3200)\n",
      "Loss: 1.254 | Acc: 55.453% (1810/3264)\n",
      "Loss: 1.254 | Acc: 55.529% (1848/3328)\n",
      "Loss: 1.253 | Acc: 55.542% (1884/3392)\n",
      "Loss: 1.254 | Acc: 55.498% (1918/3456)\n",
      "Loss: 1.256 | Acc: 55.398% (1950/3520)\n",
      "Loss: 1.256 | Acc: 55.413% (1986/3584)\n",
      "Loss: 1.258 | Acc: 55.400% (2021/3648)\n",
      "Loss: 1.254 | Acc: 55.550% (2062/3712)\n",
      "Loss: 1.256 | Acc: 55.376% (2091/3776)\n",
      "Loss: 1.252 | Acc: 55.547% (2133/3840)\n",
      "Loss: 1.253 | Acc: 55.430% (2164/3904)\n",
      "Loss: 1.254 | Acc: 55.368% (2197/3968)\n",
      "Loss: 1.255 | Acc: 55.432% (2235/4032)\n",
      "Loss: 1.255 | Acc: 55.566% (2276/4096)\n",
      "Loss: 1.259 | Acc: 55.577% (2312/4160)\n",
      "Loss: 1.258 | Acc: 55.634% (2350/4224)\n",
      "Loss: 1.255 | Acc: 55.620% (2385/4288)\n",
      "Loss: 1.255 | Acc: 55.676% (2423/4352)\n",
      "Loss: 1.253 | Acc: 55.842% (2466/4416)\n",
      "Loss: 1.251 | Acc: 55.826% (2501/4480)\n",
      "Loss: 1.249 | Acc: 55.898% (2540/4544)\n",
      "Loss: 1.252 | Acc: 55.707% (2567/4608)\n",
      "Loss: 1.250 | Acc: 55.865% (2610/4672)\n",
      "Loss: 1.249 | Acc: 55.954% (2650/4736)\n",
      "Loss: 1.250 | Acc: 56.083% (2692/4800)\n",
      "Loss: 1.247 | Acc: 56.127% (2730/4864)\n",
      "Loss: 1.246 | Acc: 56.108% (2765/4928)\n",
      "Loss: 1.247 | Acc: 55.990% (2795/4992)\n",
      "Loss: 1.248 | Acc: 56.052% (2834/5056)\n",
      "Loss: 1.250 | Acc: 55.938% (2864/5120)\n",
      "Loss: 1.249 | Acc: 55.980% (2902/5184)\n",
      "Loss: 1.249 | Acc: 55.926% (2935/5248)\n",
      "Loss: 1.247 | Acc: 55.949% (2972/5312)\n",
      "Loss: 1.248 | Acc: 55.915% (3006/5376)\n",
      "Loss: 1.248 | Acc: 55.901% (3041/5440)\n",
      "Loss: 1.248 | Acc: 55.941% (3079/5504)\n",
      "Loss: 1.250 | Acc: 55.873% (3111/5568)\n",
      "Loss: 1.253 | Acc: 55.753% (3140/5632)\n",
      "Loss: 1.254 | Acc: 55.618% (3168/5696)\n",
      "Loss: 1.253 | Acc: 55.573% (3201/5760)\n",
      "Loss: 1.252 | Acc: 55.615% (3239/5824)\n",
      "Loss: 1.253 | Acc: 55.486% (3267/5888)\n",
      "Loss: 1.254 | Acc: 55.427% (3299/5952)\n",
      "Loss: 1.254 | Acc: 55.452% (3336/6016)\n",
      "Loss: 1.254 | Acc: 55.378% (3367/6080)\n",
      "Loss: 1.254 | Acc: 55.404% (3404/6144)\n",
      "Loss: 1.254 | Acc: 55.396% (3439/6208)\n",
      "Loss: 1.256 | Acc: 55.277% (3467/6272)\n",
      "Loss: 1.255 | Acc: 55.287% (3503/6336)\n",
      "Loss: 1.256 | Acc: 55.250% (3536/6400)\n",
      "Loss: 1.257 | Acc: 55.136% (3564/6464)\n",
      "Loss: 1.256 | Acc: 55.224% (3605/6528)\n",
      "Loss: 1.258 | Acc: 55.234% (3641/6592)\n",
      "Loss: 1.256 | Acc: 55.243% (3677/6656)\n",
      "Loss: 1.257 | Acc: 55.283% (3715/6720)\n",
      "Loss: 1.256 | Acc: 55.307% (3752/6784)\n",
      "Loss: 1.254 | Acc: 55.330% (3789/6848)\n",
      "Loss: 1.257 | Acc: 55.281% (3821/6912)\n",
      "Loss: 1.260 | Acc: 55.189% (3850/6976)\n",
      "Loss: 1.261 | Acc: 55.185% (3885/7040)\n",
      "Loss: 1.261 | Acc: 55.096% (3914/7104)\n",
      "Loss: 1.261 | Acc: 55.106% (3950/7168)\n",
      "Loss: 1.261 | Acc: 55.102% (3985/7232)\n",
      "Loss: 1.259 | Acc: 55.167% (4025/7296)\n",
      "Loss: 1.258 | Acc: 55.231% (4065/7360)\n",
      "Loss: 1.258 | Acc: 55.172% (4096/7424)\n",
      "Loss: 1.258 | Acc: 55.128% (4128/7488)\n",
      "Loss: 1.256 | Acc: 55.164% (4166/7552)\n",
      "Loss: 1.256 | Acc: 55.147% (4200/7616)\n",
      "Loss: 1.256 | Acc: 55.182% (4238/7680)\n",
      "Loss: 1.253 | Acc: 55.307% (4283/7744)\n",
      "Loss: 1.252 | Acc: 55.341% (4321/7808)\n",
      "Loss: 1.252 | Acc: 55.399% (4361/7872)\n",
      "Loss: 1.251 | Acc: 55.418% (4398/7936)\n",
      "Loss: 1.252 | Acc: 55.375% (4430/8000)\n",
      "Loss: 1.251 | Acc: 55.357% (4464/8064)\n",
      "Loss: 1.252 | Acc: 55.389% (4502/8128)\n",
      "Loss: 1.251 | Acc: 55.334% (4533/8192)\n",
      "Loss: 1.252 | Acc: 55.269% (4563/8256)\n",
      "Loss: 1.254 | Acc: 55.180% (4591/8320)\n",
      "Loss: 1.254 | Acc: 55.129% (4622/8384)\n",
      "Loss: 1.254 | Acc: 55.137% (4658/8448)\n",
      "Loss: 1.254 | Acc: 55.122% (4692/8512)\n",
      "Loss: 1.255 | Acc: 55.061% (4722/8576)\n",
      "Loss: 1.256 | Acc: 55.023% (4754/8640)\n",
      "Loss: 1.256 | Acc: 55.009% (4788/8704)\n",
      "Loss: 1.256 | Acc: 54.984% (4821/8768)\n",
      "Loss: 1.256 | Acc: 54.993% (4857/8832)\n",
      "Loss: 1.255 | Acc: 55.081% (4900/8896)\n",
      "Loss: 1.255 | Acc: 55.067% (4934/8960)\n",
      "Loss: 1.254 | Acc: 55.142% (4976/9024)\n",
      "Loss: 1.256 | Acc: 55.084% (5006/9088)\n",
      "Loss: 1.256 | Acc: 55.081% (5041/9152)\n",
      "Loss: 1.254 | Acc: 55.154% (5083/9216)\n",
      "Loss: 1.254 | Acc: 55.183% (5121/9280)\n",
      "Loss: 1.253 | Acc: 55.158% (5154/9344)\n",
      "Loss: 1.255 | Acc: 55.091% (5183/9408)\n",
      "Loss: 1.257 | Acc: 55.068% (5216/9472)\n",
      "Loss: 1.256 | Acc: 55.117% (5256/9536)\n",
      "Loss: 1.256 | Acc: 55.146% (5294/9600)\n",
      "Loss: 1.256 | Acc: 55.163% (5331/9664)\n",
      "Loss: 1.256 | Acc: 55.160% (5366/9728)\n",
      "Loss: 1.257 | Acc: 55.198% (5405/9792)\n",
      "Loss: 1.257 | Acc: 55.134% (5434/9856)\n",
      "Loss: 1.259 | Acc: 55.060% (5462/9920)\n",
      "Loss: 1.259 | Acc: 55.058% (5497/9984)\n",
      "Loss: 1.259 | Acc: 55.040% (5504/10000)\n",
      "Evaluation of Epoch 2 is completed, Test accuracy for this epoch is 55.04\n",
      "\n",
      "Final train set accuracy is 54.26938775510204\n",
      "Final test set accuracy is 55.04\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.002\n",
    "regularization_val = 1e-6\n",
    "input_dims = 3\n",
    "hidden_dims = 128\n",
    "output_dims=10\n",
    "num_trans_layers = 4\n",
    "num_heads=4\n",
    "image_k=32\n",
    "patch_k=4\n",
    "\n",
    "network = None\n",
    "optimizer = None\n",
    "\n",
    "################################################################################\n",
    "# TODO: Instantiate your ViT model and a corresponding optimizer #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "network = ViT(hidden_dims, \n",
    "            input_dims, \n",
    "            output_dims, \n",
    "            num_trans_layers, \n",
    "            num_heads, \n",
    "            image_k, \n",
    "            patch_k)\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr = learning_rate)\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             \n",
    "################################################################################\n",
    "\n",
    "tr_accs=[]\n",
    "test_accs=[]\n",
    "for epoch in range(3):\n",
    "    tr_acc = train(network, optimizer, loader_train)\n",
    "    print('Epoch {} of training is completed, Training accuracy for this epoch is {}'\\\n",
    "              .format(epoch, tr_acc))  \n",
    "    \n",
    "    test_acc = evaluate(network, loader_test)\n",
    "    print('Evaluation of Epoch {} is completed, Test accuracy for this epoch is {}'\\\n",
    "              .format(epoch, test_acc))  \n",
    "    \n",
    "    tr_accs.append(tr_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "print(\"\\nFinal train set accuracy is {}\".format(tr_accs[-1]))\n",
    "print(\"Final test set accuracy is {}\".format(test_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
